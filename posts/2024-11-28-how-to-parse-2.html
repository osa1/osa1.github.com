<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <title>osa1 - Exploring parsing APIs: adding a lexer</title>
        <link rel="icon" href="data:,">
        <link rel="stylesheet" type="text/css" href="../css/default.css" />
        <link rel="stylesheet" type="text/css" href="../css/syntax.css" />

        <link rel="alternate" type="application/rss+xml" title="osa1.net blog" href="../rss.xml" />
    </head>
    <body>
        <div id="column">
            <div id="header-inner">
    <span id="blog-title"><a href="../">osa1</a></span>
    <span class="menu-item"><a href="https://github.com/osa1">github</a></span>
    <span class="menu-item"><a href="../cv.html">about</a></span>
    <span class="menu-item"><a href="../rss.xml">atom</a></span>
</div>

            <div class="inner">
                <h1 id="post-title">Exploring parsing APIs: adding a lexer</h1>

<p><strong>November 28, 2024</strong> - Tagged as: <a title="All pages tagged 'en'." href="../tags/en.html">en</a>, <a title="All pages tagged 'parsing'." href="../tags/parsing.html">parsing</a>, <a title="All pages tagged 'rust'." href="../tags/rust.html">rust</a>.</p>

<p>In the <a href="https://osa1.net/posts/2024-11-22-how-to-parse-1.html">previous post</a> we looked at three different parsing APIs, and compared them for runtime and the use cases they support.</p>
<p>In this post we’ll add a lexer (or “tokenizer”), with two APIs, and for each lexer API see how the parsers from the previous post perform when combined with a lexer.</p>
<p><strong>What is a lexer?</strong> A lexer is very similar to the event parsers we saw in the previous post, but it doesn’t try to maintain any structure. It generates “tokens”, which are parts of the program that cannot be split into smaller parts. A lexer doesn’t care about parentheses or other delimiters being balanced, or that values in an array are separated by commas, or anything else. It simply splits the input into tokens.</p>
<p><strong>Why is a lexer useful?</strong> If you already have an event parser, adding a lexer may not allow a lot of new use cases. The main use cases that I’m aware of are:</p>
<ul>
<li><p>Syntax highlighting: when higlighting syntax we don’t care about the tree structure, we care about keywords, punctuation (list separators, dots in paths etc.), delimiters (commas, bracets, brackets), and literals. A lexer gives us exactly these and nothing else.</p></li>
<li><p>Supporting incremental parsing: one way of incrementally update an AST is by starting re-lexing a few (often just one) tokens before the edited token, re-lexing until after the edit location, until generating a token identical to an existing token again. AST nodes of modified tokens are then marked as “modified” and re-parsed.</p>
<p>The details are complicated, I recommend chapter 2 of <a href="https://diekmann.uk/diekmann_phd.pdf">this PhD thesis</a> for an introduction to incremental parsing.</p>
<p>If you need to re-parse the same text as it gets edited, even if you don’t need or want incremental parsing, incremental lexing is easy, so it makes sense to re-lex incrementally and then parse from scratch using the incrementally updated token list.</p></li>
<li><p>For separating complex parsing code into smaller parts: modern languages can have complicated literal syntax, with multiple string literals with varying delimiters (like <code>r#"..."#</code> syntax in Rust, or <code>[=[...]=]</code> in Lua), multiple variants of comments (single line and multi-line, documentation and normal), multiple number syntaxes (with different suffixes like <code>123u32</code> in Rust, underscores to separate digits for readability) and so on.</p>
<p>A lexer separates handling of these from the part of the parser that deals with the program structure.</p></li>
</ul>
<h2 id="the-apis">The APIs</h2>
<p>Similar to the previous post, we will look at three different APIs for lexing:</p>
<ul>
<li>A lexer that generates a list of tokens directly: <code>tokenize_list</code>.</li>
<li>An iterator that generates one token at a time: <code>tokenize_iter</code>.</li>
<li>A “push” API that calls “listener” methods for the tokens: <code>tokenize_push</code>.</li>
</ul>
<p>For our simplified (and enhanced, with comments) JSON, our token type is:</p>
<pre><code>pub enum Token {
    Int(u64),
    Str { size_in_bytes: usize },
    True,
    False,
    Null,
    LBracket,
    RBracket,
    LBrace,
    RBrace,
    Colon,
    Comma,
    Comment { size_in_bytes: usize },
}</code></pre>
<p>Similar to our event type from the previous post, this type needs to be cheap to generate (ideally stack allocated).</p>
<p>The tokens are generated along with byte offsets in the input, also similar to events.</p>
<p>For the push API, the listener interface also directly follows the token type:</p>
<pre><code>pub trait LexerEventListener {
    fn handle_int(&amp;mut self, byte_offset: usize, i: u64);

    fn handle_str(&amp;mut self, byte_offset: usize, size_in_bytes: usize);

    // Similar for other token types.
    ...

    fn handle_error(&amp;mut self, byte_offset: usize);
}</code></pre>
<p>To keep things simple, the event handlers don’t return a <code>bool</code> to stop parsing. It can be added in a few lines of code and it doesn’t affect performance.</p>
<p>Unlike the different types of event parsers from the previous post, implementations of these lexer APIs are almost identical. This is because the lexer has only one state, which is the current position in the input. A <code>next</code> call in the iterator implementation simply continues from the current location in the input, and updates the current location as it reads characters from the input.</p>
<p>So the code is almost identical in all of these implementations.</p>
<p>The entry points are:</p>
<pre><code>pub fn tokenize_iter&lt;'a&gt;(input: &amp;'a str) -&gt; Lexer&lt;'a&gt; { ... }
  // Lexer implements `Iterator`

pub fn tokenize_push&lt;L: LexerEventListener&gt;(input: &amp;str, listener: &amp;mut L) { ... }

pub fn tokenize_list(input: &amp;str) -&gt; Result&lt;Vec&lt;(usize, Token)&gt; usize&gt; { ... }</code></pre>
<h2 id="combining-with-the-event-parsers">Combining with the event parsers</h2>
<p>We have 3 lexers and 2 event parsers:</p>
<ol type="1">
<li>tokenize_list + parse_events_iter</li>
<li>tokenize_list + parse_events_push</li>
<li>tokenize_iter + parse_events_iter</li>
<li>tokenize_iter + parse_events_push</li>
<li>tokenize_push + parse_events_iter</li>
<li>tokenize_push + parse_events_push</li>
</ol>
<p>However (5) is not easily possible in Rust. The problem is that a push API cannot be converted into an iterator, as it will scan the entire input without ever returning and keep calling the listener methods. To convert a push API into an iterator, we need a language feature that allows us to stop the current thread (or maybe a “fiber”, green thread etc.) and resume it later. In Rust, this is possible with <code>async</code> or threads. Threads are expensive, and <code>async</code> requires a lot of refactoring, and all the call sites to be made <code>async</code> as well.</p>
<p>So in this post we won’t consider this combination.</p>
<h2 id="notes-on-implementations">Notes on implementations</h2>
<p>Implementing these combinations is mostly straightforward. Full code is linked below as usual. The takeaways are:</p>
<ul>
<li>The push API cannot converted into an iterator API, without language features.</li>
<li>The push API requires state management in the consumer: the consumer will have to save the state that needs to be maintained between the calls to the listener methods.</li>
<li>The iterator API is more flexible, it can be used to implement a push API and also an iterator.</li>
<li>The iterator API is also easier to use: the consumer can iterate through the elements in nested loops, and needs less state management. The state can also be function locals, instead of fields of a struct.</li>
<li>The list API (generates an entire vector of tokens) only makes sense when you need to collect all of the tokens in memory. The only use case for this that I’m aware of is incremental parsing.</li>
</ul>
<h2 id="references-and-benchmarks">References and benchmarks</h2>
<p>The code (including benchmarks) is here: <a href="https://github.com/osa1/how-to-parse-2">github.com/osa1/how-to-parse-2</a>.</p>
<p><strong>Token generation benchmarks:</strong> Collect all of the tokens in a <code>Vec</code>.</p>
<ul>
<li>tokenize_list: 305 MB/s</li>
<li>tokenize_push: 303 MB/s</li>
<li>tokenize_iter: 329 MB/s</li>
</ul>
<p>In the event generation benchmarks in the last post, the push implementation is about 10% faster than the iterator. But in the lexer, the iterator is faster when collecting the tokens in a vector. It looks like when the state that the parser manages between the <code>next</code> calls gets simpler, the compiler is able to optimize the code better, and iterator implementations beats the push implementation.</p>
<p>The vector generator and push API adding the elements to a vector via the listener perform the same, which shows that when monomorphised, the push API implementation optimizes quite well for simple cases (but also in complex cases, as we will see below). In languages without monomorphisation, the push API should be slower.</p>
<p><strong>Tokens to events:</strong> Convert tokens to events.</p>
<ul>
<li>events_iter: 282 MB/s</li>
<li>events_push: 315 MB/s</li>
<li>tokenize_list + events_iter: 181 MB/s</li>
<li>tokenize_list + events_push: 187 MB/s</li>
<li>tokenize_iter + events_iter: 269 MB/s</li>
<li>tokenize_iter + events_push: 275 MB/s</li>
<li>tokenize_push + events_push: 351 MB/s</li>
</ul>
<p>The first two benchmarks are the ones from the previous post that don’t use a lexer, generate events directly. The numbers are slightly different than the numbers from the previous post as I rerun them again.</p>
<p>If you need some kind of incremental implementation, scanning the entire input and collecting the events or tokens in a vector performs bad. There’s no point in combining the list API with push or iterator APIs.</p>
<p>What’s surprising is that the push lexer implementation combined with the push event generator implementation performs better than the event generator implementation that parses the input directly without a lexer. I don’t have an explanation to how this works yet.</p>
<p>Lexer iterator implementations combined with any of the event generation implementations perform slower than the event push implementation that parses the input directly, but about as fast as the event iterator implementation that parses the input directly.</p>
<p><strong>Tokens to AST:</strong> Converts tokens to events, builds AST from the events.</p>
<ul>
<li>Recursive descent: 127 MB/s</li>
<li>events_iter to AST: 140 MB/s</li>
<li>events_push to AST: 145 MB/s</li>
<li>tokenize_list + events_iter to AST: 108 MB/s</li>
<li>tokenize_list + events_push to AST: 108 MB/s</li>
<li>tokenize_iter + events_iter to AST: 138 MB/s</li>
<li>tokenize_iter + events_push to AST: 139 MB/s</li>
<li>tokenize_push + events_push to AST: 151 MB/s</li>
</ul>
<p>The first three benchmarks below are from the last post. Rerun and included here for comparison.</p>
<p>When we add an AST building step, which is more complicated compared to the rest of steps, the performance difference between the most convenient implementation (tokenize_iter + events_iter to AST) and the most performant one (tokenize_push + events_push to AST) diminishes. In the event generation benchmark, the fast one is 30% faster, but when building an AST, it’s only 9% faster.</p>
<p>The push implementation is still faster than the recursive descent parser, even with the extra lexing step. I’m planning to investigate this further in a future post.</p>

<hr />

<div id="disqus_thread">
    <a href="#" onclick="loadDisqus(); return false;">
        (Show comments)
    </a>
</div>

<script>
    function loadDisqus() {
        var dsq = document.createElement('script');
        dsq.type = 'text/javascript';
        dsq.async = true;
        dsq.src = 'https://osa1.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] ||
            document.getElementsByTagName('body')[0]).appendChild(dsq);
    };
</script>

            </div>
        </div>
    </body>
</html>
