<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>osa1.net - Posts tagged multi-stage programming</title>
    <link href="http://osa1.net/tags/multi-stage%20programming.xml" rel="self" />
    <link href="http://osa1.net" />
    <id>http://osa1.net/tags/multi-stage%20programming.xml</id>
    <author>
        <name>Ömer Sinan Ağacan</name>
        <email>omeragaca@gmail.com</email>
    </author>
    <updated>2015-08-16T00:00:00Z</updated>
    <entry>
    <title>A learning system that is as fast as a static one</title>
    <link href="http://osa1.net/posts/2015-08-16-a-learning-system-that-is-fast.html" />
    <id>http://osa1.net/posts/2015-08-16-a-learning-system-that-is-fast.html</id>
    <published>2015-08-16T00:00:00Z</published>
    <updated>2015-08-16T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>I was reading the 1986 paper <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.128.6414">The Concept of a Supercompiler</a> one more time, and I found this amazing part that I either missed or didn’t find interesting when I read it for the first time:</p>
<p>(emphasis mine)</p>
<blockquote>
<p>Supercompilation can also be performed throughout all levels of the system. Fix k = K and supercompile E(K, q) with a variable q. You have an expert system that cannot learn, but answers questions very quickly. You can restore the ability of the system to learn by endowing it with two memories: short-term and long-term. To answer a question, both memories must be scanned. The long-term memory has those procedures that are obtained by using a supercompiler with the knowledge present at the moment of the last supercompilation session. The short-term memory includes universal interpretive procedures operating on the incremental knowledge received after the last supercompilation session. Such a system can be both fast and able to learn. <em>When it has no questions to answer(the periods of “sleep”), it will execute supercompilation procedures, converting its short-range memory into the long-range one. This would be a step towards a computer individual.</em></p>
</blockquote>
<p>This is interesting for two reasons. First, we discussed similar ideas with my advisor couple of weeks ago, but in a different context. I’m hoping to write more about this in future posts. Second, I think the idea is easily implementable with a multi-stage language. I think this is one of the ideas that are very good yet even after years still waiting to be implemented and explored further.</p>]]></summary>
</entry>
<entry>
    <title>On sufficiently smart compilers</title>
    <link href="http://osa1.net/posts/2015-08-09-sufficiently-smart-compiler.html" />
    <id>http://osa1.net/posts/2015-08-09-sufficiently-smart-compiler.html</id>
    <published>2015-08-09T00:00:00Z</published>
    <updated>2015-08-09T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>I’ve been thinking about optimizing functional programs recently, for a project that I’m hoping to make my research topic in the near future. You probably already know about <a href="http://c2.com/cgi/wiki?SufficientlySmartCompiler">The Myth of the Sufficiently Smart Compiler</a>. It basically says that the advanced compiler that optimizes your high-level, highly-abstracted programs to efficient low-level code, is basically a myth.</p>
<p>This post is a brain dump on sufficiently smart compilation of functional programs and some compilation techniques. I’ll first make some seemingly-unrelated points, and then hopefully use them to argue that the sufficiently smart compiler is not a myth, it just needs some hard work to be realized.</p>
<h2 id="unreliable-optimizations-and-performance-critical-software">Unreliable optimizations and performance-critical software</h2>
<p>Every once in a while I see some blog posts about optimizing a JIT-compiled program by inspecting JIT trace dumps and generated code carefully, and I find this horrible, for the following reasons:</p>
<ul>
<li><p>It couples your program design with the JIT compiler’s internals. From a software engineering point of view, I think this is really one of the worst things that can happen to a software. You end up structuring your code with the compiler’s convenience in mind. But compilers can’t make sense of high-level, abstracted code (remember the myth?). So you end up with a code that’s low-level, hard to read, understand and maintain. And what happens when a new version of the compiler is released?</p></li>
<li><p>JIT compilers are highly complex, and as a result they’re very hard to reason about and this complex design makes them unpredictable. A seemingly-unrelated change in your program can make the traces go significantly bad, and result in less optimized code, because maybe the change somehow made it to the trace and now you need to refactor your code.</p></li>
<li><p>If you need performance that bad, and you’re willing to read traces and generated assembly output for that, you could probably just write in a language that makes low-level optimizations easy/possible<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. Or at least write performance-critical parts in a low-level language. Both of these cases eliminate the need for a JIT compiler.</p></li>
</ul>
<p>I think the last point is worth discussing further. Most JIT compilers we use nowadays are for compiling dynamic languages<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>. By their nature dynamic languages are hard to optimize in compile time, so they rely on runtime knowledge for optimizations. But does that make JIT compilers useless for statically-typed languages that are more amenable to compile-time optimizations? I don’t have a good answer to this, probably because I’m not a JIT expert. I think the fact that <a href="http://openjdk.java.net/groups/hotspot/">HotSpot</a> is doing good job is not an answer to this, because in JVM there’s bytecode interpretation going on, and this is adding some room for runtime optimizations. Namely, you have one level of indirection that you can eliminate using JIT compilation.</p>
<p>In other statically-typed, compiled languages like C++, Haskell, OCaml etc. there’s less room for that kind of optimizations. I think applicability of JIT compilation techniques to these type of languages would make an interesting topic for a research project.</p>
<h2 id="compilers-that-can-learn-your-domain-and-manipulate-your-programs">Compilers that can learn your domain and manipulate your programs</h2>
<p>High-level languages and abstractions make efficient execution of programs harder, but there are a couple of things that they can do to help with the compilation. Namely, you can guide the compiler to optimize your domain-specific code.</p>
<p>One nice and simple example is <a href="https://downloads.haskell.org/~ghc/7.0.1/docs/html/users_guide/rewrite-rules.html">rewrite rules of GHC</a>. They’re used quite heavily in <a href="http://hackage.haskell.org/package/base">base</a> (GHC’s standard library) to eliminate intermediate lists. Other libraries use the same mechanism to tell the compiler how to optimize the code that uses their abstractions<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>.</p>
<p>But for a compiler to support this kind of program transformations the language has to have some properties. In our case, we should be able to reason about the code in compile time, and locally, i.e. without thinking about runtime execution environment (heap, stack, variables in scope etc.) and the interaction of our code with the rest of the code. This is possible in purely functional languages because they make <a href="http://www.haskellforall.com/2013/12/equational-reasoning.html">equational reasoning</a> possible.</p>
<p>This is a very powerful property. This makes it possible to see programs as terms in an algebra, and we can freely manipulate these terms according to our rules. In the most basic sense, these rules can be the rules that define our language’s operational semantics, because by its very definition these rules are guaranteed to preserve semantics of programs. But we can go even further by adding rewrite rules to these rules. Rewrite rules are a way to say, “trust me, this transformation preserves semantics” and at that point a compiler is free to use these rules.</p>
<p>Furthermore, some properties of the language can give us <a href="http://ttic.uchicago.edu/~dreyer/course/papers/wadler.pdf">free theorems</a>, which in turn can help us with <a href="http://research.microsoft.com/en-us/um/people/simonpj/Papers/deforestation-short-cut.pdf">some optimizations</a><a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>.</p>
<p>This type of “algebraic manipulation of programs” is a very powerful concept, and it can do great things. A very good example is this <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.48.7721">1997 paper about optimizing Haskell</a>. Most (maybe all?) of the transformations described in that paper are still in use.</p>
<h2 id="compilers-that-preserve-the-semantics">Compilers that preserve the semantics</h2>
<p>You probably wouldn’t want a compiler that compiles your programs to programs that do different things. We expect it to preserve the semantics. But that rule is sometimes too strict, and prevents some optimizations.</p>
<p>For example, if floating points and operations on floating points in your language are defined as they’re defined in IEEE-754, then the compiler can’t assume associativity of floating point operations and you lose some optimization opportunities. GCC’s <code>-ffast-math</code> is for relaxing this restriction by letting the compiler assume this associativity.</p>
<p>Another example is termination properties of programs. For example, would you be OK with this transformation in a purely functional language:</p>
<pre><code>(λx . 1) loop ~&gt; 1</code></pre>
<p>In a call-by-name (or call-by-need, which is an efficient implementation of call-by-name) language, this is a valid transformation. But in call-by-value language this would change the semantics. Previously this program was looping, but now it returns 1.</p>
<p>This example is actually a good demonstration of a problem that we have even in purely functional languages. Namely, there are some programs that don’t map to any values in the domain you use to model your language<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>. The way these programs are modeled are generally by defining a special value, ⊥ (read “bottom”). Non-terminating and exception/error throwing programs are said to be “bottom” and denoted with this value. Bottom values are said to be “less defined” than non-bottom values.</p>
<p>Using this definition, we can say that the transformation shown above transforms a program to a more defined one. You might want this restriction of preserving definedness of programs for different reasons, and here’s an example reason: Without this restriction, your program may terminate or loop depending on how the compiler performed. A seemingly-unrelated change in your program may cause a different termination behavior.</p>
<p>Now this is a hard problem. There are papers about transforming call-by-value functional languages while preserving termination properties (see <a href="https://www.sics.se/~pj/papers/scp/popl09-scp.pdf">this</a> as an example). In general, we can’t decide if a program is bottom or not. First of all, that would be solving the halting problem. But more specifically, we can’t do this transformation if <code>y</code> depends on a dynamic input here:</p>
<pre><code>(λx . 1) (1 / y) ~&gt; 1</code></pre>
<p>In most cases though, the compiler is simply not able to propagate enough information to this stage to see if <code>y</code> can be <code>0</code> or not<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>, even if all the necessary information is available in compile time.</p>
<h2 id="making-the-most-out-of-available-input">Making the most out of available input</h2>
<p>There’s an old yet IMHO under-appreciated technique for taking statically known inputs into account while compiling programs. It’s called “partial evaluation” and described in details in this awesome book <a href="http://www.itu.dk/~sestoft/pebook/jonesgomardsestoft-a4.pdf">“Partial Evaluation and Automatic Program Generation”</a> by Neil D. Jones, Carsten K. Gomard and Peter Sestoft. One very interesting but somewhat esoteric application of this idea is <a href="https://cs.au.dk/~hosc/local/HOSC-12-4-pp381-391.pdf">Futamura projections</a><a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>, but to give a easier to understand example, a C partial evaluator could read your Vim config in compile time and compile Vim to an executable that doesn’t read any Vim files on startup because it’s already specialized to the Vim config it read in compile time<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>. General tools may depend of lots of dynamic input, but in your special case you may fix some of these variables and this is where a partial evaluation comes into the play. See <a href="http://blog.regehr.org/archives/1197">this blog post</a> for another example.</p>
<p>How much further could it propagate this statically known input and specialize rest the code using it? That’s completely different story and comes with some very hard to solve problems. I’ll again come to this later.</p>
<p>The whole point is to generate specialized code for known input. We can shift the stage a little bit and apply this idea in runtime, and that gives us <a href="http://www.cs.rice.edu/~taha/MSP/">multi-stage programming</a>.</p>
<p>MSP allows us to generate code in runtime, link it to the program in a way that the generated code runs in the current execution environment (i.e. the generated code can refer to names in enclosing scope, pretty much like how closures would do).</p>
<p>Traditionally, MSP doesn’t allow code generation in compile-time, and the techniques used for code generation are completely different<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>. But we can generate code specialized to input that is only available in runtime. For example, you can write a game that runs code specialized to the player’s options. Or run a web server that does some optimizations on request dispatch code depending on some analysis on recent requests.</p>
<p>This is again a very powerful concept, and only recently I started to appreciate its potential<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a>. IMO, MSP is missing a “killer language”<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> (and also a “killer application” but I think that follows the language) and I’m hoping to make some progress on this front in the future.</p>
<h2 id="finally-a-sufficiently-smart-compiler">Finally, a sufficiently smart compiler</h2>
<p>This post may seem to be going nowhere, so let’s back up a bit and come to the point.</p>
<p>I define a sufficiently smart compiler not as a completely automated program, but as a toolchain. This toolchain has a completely automated compiler, but it also gives programmers tools for runtime code generation, and for teaching the compiler domain-specific optimizations. The compiler knows about the language’s semantics, and when possible it does reductions in compile time to remove abstractions and leave less work to runtime.</p>
<p>While doing reductions in compile time, it takes programmers’ rules into account, and optimizes abstractions accordingly. This allows it to optimize domain-specific abstractions that normally a compiler would have no way of knowing.</p>
<p>By now it should be clear that such a compiler is only possible with a language that allows these optimizations. For example, without a purely functional language, rewrite rules are not easy, if not impossible.</p>
<p>The compiler gradually compiles the language into languages that are more and more close to the machine language that it has to generate in the end. Reductions and user rules are applied in a level where programs are still expressed in a purely functional language. This language should be sufficient for most optimizations that eliminate programmers’ abstractions in compile time.</p>
<p>This way, programmers don’t need to look at ridiculous bytecode traces or instructions written in a highly-complex assembly language to figure out how things are optimized, and rather they stay in the same level of abstraction that their programs are written in. When they want to know about memory allocations, for example, they should be able to look at the next level in the compilation, which should have explicit memory allocation operations and pointers etc. The main point is that they stay in a level where they can observe some particular behavior (e.g. memory allocation) of a program and they don’t have to read assembly, for example, to see if their higher-order <code>map</code> application that uses an increment function to increment integers in a list is compiled to a loop without any function calls.</p>
<p>In this compiler there’s no room for abstraction-breaking, unreliable optimizations or optimizations that cause coupling with the compiler’s internals, like in the case of JIT compilers.</p>
<p>In the beginning I said that I don’t see this as a myth. So how I think this is possible to implement? This is already a long-enough post, and I’ll stop for now. Let me just say that almost all of these things are implemented in different projects:</p>
<ul>
<li><p>MSP does runtime code generation and <a href="http://okmij.org/ftp/ML/MetaOCaml.html">MetaOCaml</a> gives us a nice way to do that in a safe way. Another alternative is <a href="http://terralang.org/">Terra</a>, but in Terra generated code is in a different language, so that’s quite different (also, it’s a dynamically typed language that gives no guarantees about generated code).</p></li>
<li><p>Domain-specific optimizations are possible in Haskell thanks to GHC’s rewrite rules, as mentioned in the related section above.</p></li>
<li><p>GHC’s internal languages Core, STG and Cmm allow programmers to gradually go low level and see the details they’re looking for. Most of the time Core is enough to see if your abstractions are eliminated in compile time and if your rules worked as expected.</p></li>
<li><p>Compile time reduction of programs are done by supercompilers. It was a lesser known technique until recently a couple of papers (<a href="http://dl.acm.org/citation.cfm?id=1863588">1</a>, <a href="http://research.microsoft.com/en-us/um/people/simonpj/papers/supercompilation/supercomp-by-eval.pdf">2</a>) and a <a href="http://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-835.html">PhD thesis</a> explored it in the context of Haskell.</p></li>
</ul>
<p>Some of these features are orthogonal to each other, like MSP and compile-time reductions. But some others are not, for example, we expect a supercompiler to take rewrite rules into account, otherwise it may be impossible to do some optimizations.</p>
<p>The hardest part seems to be compile-time reductions of programs according to operational semantics of the language, which involves some very hard problems, and one of the reasons has to do with preserving semantics. In the next couple of posts I’m hoping to talk about that, and in the meantime you can refer to chapter 9 of the PhD thesis I linked above.</p>
<hr />
<p>This post has made it to <a href="https://www.reddit.com/r/haskell/comments/3wdrv6/on_sufficiently_smart_compilers/">/r/haskell</a>, <a href="https://www.reddit.com/r/compsci/comments/3wdqro/on_sufficiently_smart_compilers/">/r/compsci</a> and <a href="https://news.ycombinator.com/item?id=10733201">Hacker News</a>. Thanks for sharing this!</p>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p><a href="https://github.com/SnabbCo/snabbswitch">Snabb Switch</a> project comes to mind here. It’s a Lua project and they rely on LuaJIT to optimize their code. See this series of blog posts: <a href="https://github.com/lukego/blog/issues/5">1</a>, <a href="https://github.com/lukego/blog/issues/6">2</a>, <a href="https://github.com/lukego/blog/issues/8">3</a>.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p><a href="https://developers.google.com/v8/">V8</a> and <a href="https://wiki.mozilla.org/JavaScript:TraceMonkey">TraceMonkey</a> for JavaScript, <a href="http://luajit.org/">LuaJIT</a> for Lua, <a href="http://pypy.org/">PyPy</a> for Python. There are also research-level JIT compilers, like <a href="https://github.com/higgsjs/Higgs">Higgs</a> for JavaScript and <a href="https://github.com/samth/pycket">Pycket</a> (<a href="https://rpython.readthedocs.org/en/latest/">RPython</a> based, created by colleagues from IU) for Racket.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>One example that I like very much is the <a href="http://hackage.haskell.org/package/pipes">pipes library</a>. You can see some of its rewrite rules <a href="https://github.com/Gabriel439/Haskell-Pipes-Library/blob/d7b1430b1b35abfde98b32cbc4aae02a4e027dd0/src/Pipes/Core.hs#L869">here</a>.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p>One very good question to ask here is, what exactly gives us free theorems? I don’t have an answer to that question yet.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" role="doc-endnote"><p>This type of giving semantics to languages is called “denotational semantics”. I don’t have very good reading material about this but you may want to have a look at <a href="https://en.wikibooks.org/wiki/Haskell/Denotational_semantics">this</a>.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6" role="doc-endnote"><p>We’re assuming that it somehow knows that divide-by-zero leads to bottom.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7" role="doc-endnote"><p>I wrote about this <a href="http://osa1.net/posts/2015-01-11-understanding-futamura-projections.html">previously</a> and I also have <a href="http://osa1.net/posts/2015-05-13-comp-through-interp.html">this related project</a>.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8" role="doc-endnote"><p>In practice this is probably hard to achieve, and it certainly needs some refactoring in current Vim codebase.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9" role="doc-endnote"><p>See <a href="http://osa1.net/posts/2015-05-13-comp-through-interp.html">my blog post</a> for a comparison.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10" role="doc-endnote"><p>Even though I’ve been working on MSP languages for a while know. See my previous work on this: <a href="/posts/2013-04-15-internship.report.html">1</a>, <a href="/posts/2014-03-06-proving-simply-typed-multi-staged-lc.html">2</a>, <a href="/posts/2015-05-13-comp-through-interp.html">3</a>, and here’s a <a href="/posts/2015-05-17-staging-is-not-just-codegen.html">ranty post</a>.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11" role="doc-endnote"><p><a href="http://terralang.org/">Terra</a> comes quite close, but I have some confusions about it and I’m hoping to write about those in the future.<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></summary>
</entry>
<entry>
    <title>Staging is not just code generation</title>
    <link href="http://osa1.net/posts/2015-05-17-staging-is-not-just-codegen.html" />
    <id>http://osa1.net/posts/2015-05-17-staging-is-not-just-codegen.html</id>
    <published>2015-05-17T00:00:00Z</published>
    <updated>2015-05-17T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>It feels weird to see that even <a href="http://okmij.org/ftp/">Oleg</a> seems to <a href="http://lambda-the-ultimate.org/node/5146#comment-85570">think about it that way</a>.</p>
<p>We don’t have a definition of the term that’s supposed to be accepted by everyone – one can use it in different meanings. My minimal definition for the term is “a technique for runtime code generation and linking”. If it’s missing “linking” part, then to me it’s just another AST definition + printer library (sometimes it’s embedded into the language to add some convenience syntactic sugar and/or quasiquotation to the language).</p>
<p>To me the whole point is “runtime specialization”. For that you should be able to use the data available at code-generation time in the generated code<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. This is called “cross-stage persistence”. In a simple multi-stage language, this may be supported simply by serializing the data as code, but this is not as flexible as one might need for runtime optimized code generation. For example, you can’t serialize a socket or file handle this way, but it’s safe and possible to use a socket or file handle available when generating the code in the generated code. You can’t easily do that if the staging library/language doesn’t provide this as a feature.</p>
<p>In the case of <a href="http://okmij.org/ftp/ML/MetaOCaml.html">BER-MetaOCaml</a>, I think this is one of the major limitations: It only supports OCaml bytecode, and cross-stage persistence is available via the serialization of runtime data. If a runtime data is not serializable, then you can’t easily use it in future stages.</p>
<p>One more thing about printing the code: In my opinion, a multi-stage language should provide a way to print generated code <em>only for debugging purposes</em>. (e.g. to check if the generated code is really the code I want)<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<p>To make it clear: I think it’s fine to use it for code generation, but if all it can do is code generation then I think it’s missing the point.</p>
<p>As an example, I used staging for code generation <a href="http://osa1.net/posts/2015-05-13-comp-through-interp.html">in my last project</a>, and it seems like <a href="http://scala-lms.github.io/">Scala LMS</a> people do this a lot too<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>.</p>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>In a sense this is like a closure, generated code should be able to refer to names in enclosing environment of code generator.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>I’m wondering if <a href="http://terralang.org/">Terra</a> has a way to print generated code. Any ideas?<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>I didn’t read the paper very carefully, but I think one example is <a href="http://dl.acm.org/citation.cfm?id=2429128">Optimizing Data Structures in High-Level Programs</a> paper which is published in POPL’13.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></summary>
</entry>
<entry>
    <title>Compilation through interpretation, a small experiment</title>
    <link href="http://osa1.net/posts/2015-05-13-comp-through-interp.html" />
    <id>http://osa1.net/posts/2015-05-13-comp-through-interp.html</id>
    <published>2015-05-13T00:00:00Z</published>
    <updated>2015-05-13T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>I’ve been studying different program transformation techniques recently, and to me <a href="http://osa1.net/posts/2015-01-11-understanding-futamura-projections.html">Futamura projections</a> are one of the most interesting applications of program transformations. Couple of days ago I finished a small project in which I implemented first Futamura projection(aka. interpreter specialization) using <a href="http://www.madore.org/~david/programs/unlambda/">Unlambda</a> as object language. You can see the project <a href="https://github.com/osa1/int-proj">here</a>. I tried to write some comments to the source when I get stuck because of a problem or realized something interesting, so I suggest reading the source if you’re interested.</p>
<p>I did two implementations and used a different meta language for each one. There are multiple ways to achieve first Futamura projections: We can use a partial evaluator, a supercompiler(which may actually subsume partial evaluation, depending on how sophisticated it is), or just a “sufficiently smart” compiler. The problem though, we don’t have a lot of(read: any) usable implementations of partial evaluators or supercompilers, so I had to use the only language with a partial evaluator that I could find: <a href="https://github.com/idris-lang/Idris-dev">Idris</a><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>.</p>
<p>There’s one another technique that we can use. The techniques I listed above are all completely automated. If things don’t go as expected we’re on our own to figure out why is that and hack around to make the tool transform the program the way we want. Indeed this is happened even in this project, which is deliberately kept simple and small<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>.</p>
<p>At the other end of the spectrum is multi-stage programming. In multi-stage programming the programmer specifies, using some annotations<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>, what code to generate and how to generate it. It clearly separates code that runs in code generation time and generated code. When generated code is printed to be compiled later, multi-stage programming feels like a compiler or a partial evaluator that the programmer can guide to generate the code he/she wants.</p>
<p>My second meta language is <a href="http://okmij.org/ftp/ML/MetaOCaml.html">MetaOCaml</a>, which is basically OCaml with multi-stage programming constructs. Using these two languages as representatives of two different program generation techniques, I implemented first Futamura projections for Unlambda.</p>
<p>There’s a report file in the repository, and I refer interesting readers to that document. README file contains compilation directives and some interesting executions. One interesting thing is that I later added a simple partial evaluator to MetaOCaml implementation, and in the <code>programs/</code> directory there’s an Unlambda interpreter, written in Unlambda. Using these two programs, you can do things like partially applying(specializing) Unlambda interpreter to other programs or even itself. Before every experiment, I suggest thinking about what is the generated code you’re expecting(what does it do). What would a “sufficiently smart” partial evaluator generate? What would a simple partial evaluator generate? Similarly, try these while generating first projections.</p>
<p>Finally, if you’re interested in program transformations, stay tuned for more blog posts.</p>
<p><a href="https://github.com/osa1/int-proj">Link to the project.</a></p>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>There is actually an <a href="https://github.com/annenkov/unmix">implementation of well-known partial evaluator unmix</a>. I knew about the project, but didn’t remember by the time I started this project. Still, I think I’d choose Idris even if I remembered.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>Although some of those problems were implementation related, e.g. Idris was buggy. See the source code for comments and Github issues.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p>In MetaOCaml case those annotations are term-level, but there are other cases where annotations happen in type level only. See <a href="http://scala-lms.github.io/">LMS</a> as an example. (I think it’s the only example for now)<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></summary>
</entry>
<entry>
    <title>Proving soundness of simply typed multi-staged lambda-calculus</title>
    <link href="http://osa1.net/posts/2014-03-06-proving-simply-typed-multi-staged-lc.html" />
    <id>http://osa1.net/posts/2014-03-06-proving-simply-typed-multi-staged-lc.html</id>
    <published>2014-03-06T00:00:00Z</published>
    <updated>2014-03-06T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>First part of my first non-trivial(e.g. something other than a Software Foundations exercise) Coq program is finally done. I learned Coq from Software Foundations, but I didn’t finish to book. I still know only very basic tactics, which I think is one of the reasons why my proofs are so long. You can see the source <a href="https://github.com/osa1/StagedLambda/blob/master/Lc.v">here</a>.</p>
<p>Some random notes about implementation:</p>
<ul>
<li>I’m looking to improve proofs. I have a lot more to implement for the rest of the program until deadline, so I may not be able to refactor current proofs very much, but at one point I want to simplify the proofs and use more advanced tactics.</li>
<li>Currently only <code>inversion</code>, <code>destruct</code>, <code>induction</code>, <code>rewrite</code>, <code>assumption</code>, <code>assert</code>, <code>constructor</code>, <code>auro</code>, <code>simpl</code>, <code>intro</code>/<code>intros</code>, <code>apply</code>, <code>right</code>/<code>left</code>, <code>exists</code>, <code>unfold</code>, <code>subst</code>, <code>reflexivity</code>, <code>remember</code> and <code>generalize</code> tactics are used.</li>
<li>Language definition is almost the same as in papers. There is one difference, we implemented substitutions as a function. In reality, substitution in multi-staged lambda-calculus is not a function. I believe this doesn’t effect correctness of theorems. At one point I’ll refactor the code and define substitution as a relation.</li>
<li>I used <code>Case</code>, <code>SCase</code>, <code>SSCase</code> … constructs from SFlib extensively.</li>
</ul>
<p>Now I’m going to implement lambda-calculus with row-polymorphic records. I expect this to be at least 2x harder, since polymorphism is involved. Let’s see how it goes …</p>]]></summary>
</entry>
<entry>
    <title>Internship report - type inference, row polymorphism, and multi-stage programming</title>
    <link href="http://osa1.net/posts/2013-04-15-internship.report.html" />
    <id>http://osa1.net/posts/2013-04-15-internship.report.html</id>
    <published>2013-04-15T00:00:00Z</published>
    <updated>2013-04-15T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>This is the report of my 3.5-month internship fulfilled at <a href="http://ozyegin.edu.tr/">Ozyegin University</a> under <a href="http://faculty.ozyegin.edu.tr/aktemur/">Prof. Baris Aktemur</a>’s supervision.</p>
<p>I wrote this report as a short and informal introduction to the topics I worked on in my internship, and since I know nobody will ever read this internship report, I decided publishing it in my blog with hoping someone benefit from it.</p>
<p>A note before reading: Things went in an unexpected way shortly after I started writing the report: I got bored. So each chapter got shorter and shorter, leaving tons of interesting and important stuff unmentioned. Sorry for that.</p>
<h2 id="contents">Contents:</h2>
<ul>
<li>Introduction to type systems and polymorphism</li>
<li>Hindley-Damas-Milner type system and type inference</li>
<li>Row polymorphism</li>
<li>Multi-stage porgramming</li>
<li>Our work - extending multi-stage language with subtyping and Wallce: a module type inference system</li>
</ul>
<h1 id="introduction-to-type-systems-and-polymorphism">Introduction to type systems and polymorphism</h1>
<p>As far as software engineering concerned, a type system is a static analysis method that aims to prove a program ‘won’t go wrong’<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>, meaning that the program will not crash at runtime. Static type checking can also be used for some compiler optimisations. Also, some languages have mechanisms for dispatching on types.</p>
<p>Type systems (in this text, this term is used as ‘static type systems’) do that by inspecting program terms and proving some properties and relations among them. This work is done before run-time, and most type information can be deleted so that it won’t cause any runtime costs.</p>
<p>For most type systems, process of type checking is ‘syntactic’. This means two things; first, the type checker only needs program text and it doesn’t need any information that can only be obtained at run-time. And second, type checking a program term only depends on type checking subterms of that program. These two properties also give us a somewhat simple way to express type checking on terms in a symbolic and formal way.</p>
<p>Type systems are ‘conservative’, meaning they will reject some ‘correct’ programs. A correct program means a program that won’t crash at runtime. For instance, this program(in OCaml syntax):</p>
<pre><code>if true then 1 else 42(5)</code></pre>
<p>is actually correct, and it can be given the type <code>int</code> (since we know it always evaluates to <code>1</code>, which has type <code>int</code>), but almost all type systems reject this program because of the ‘else’ branch expression <code>42(5)</code> that is ill-typed.</p>
<p>Research on type systems aims to have more “powerful” type systems. Being more powerful means accepting <em>more</em> correct programs and also being able to encode more <em>invariants</em> in the type system, so that more <em>incorrect</em> programs will be rejected.</p>
<p>Being wrong and being incorrect is used with different meanings in this text: a wrong program will crash at run-time, but incorrect program will result with a wrong answer. For instance, a wrong implemented algorithm may not be <em>wrong</em>, but it’s <em>incorrect</em>. So it will work fine, but return a wrong result.</p>
<p>As an instance of accepting more correct programs, this code is not typeable under simply-typed lambda calculus:</p>
<pre><code>let id a = a in
(id 1, id true)</code></pre>
<p>But it’s well-typed in polymorphic lambda calculus. As a second example, this is not typeable under polymorphic lambda calculus:</p>
<pre><code>add5 : float -&gt; float
...
add5 10</code></pre>
<p>Here <code>10</code> has type <code>int</code>, so we can’t apply it to a function that expects a float value. Type systems with subtyping overcome this problem by defining a common supertype of types. In our case, <code>float</code> is already a supertype of <code>int</code>, so this example is well-typed in the presence of subtyping.</p>
<p>As an example of being able to encode more <em>invariants</em> in the type system, let’s look to the type of a sort function from a dependently-typed programming language:<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<pre><code>sort : List Int -&gt; List Int</code></pre>
<p>This type can be inhabited with some wrong functions. For instance, a function that takes a list and returns an empty list will have this type, just like <em>reverse</em> and <em>shuffle</em> functions.</p>
<pre><code>sort : Vect Int n -&gt; Vect Int n</code></pre>
<p>This type has a invariant encoded in it: length of it’s input and output should be equal. This means our first function that takes a list and returns an empty list now can’t be inhabited by this type. We have a more precise type.</p>
<pre><code>sort : (xs : Vect Int n) -&gt;
       (ys : Vect Int n ** Permutation xs ys)</code></pre>
<p>This type is even more precise, a function that takes a list and returns a list with some random elements can’t get this type.</p>
<p>One problem with more expressive type systems is that there may be some types that can’t be inferred by type system, but the type can be given by programmer. In some cases the type system may not be able to prove that an expresion has the denoted type. In case of dependently typed languages, programmer is required to give some proofs to the type checker. But even in much simpler type systems programmer may have to give some types manually.</p>
<p>–</p>
<p>A word about the terms ‘polymorphism’ and ‘subtyping’: Polymorphic type systems can give multiple types to a single piece of code, thus providing reuse. This is done by giving the code a general type and then creating instances of type depending on the use. As an example, let us start with a simple OCaml function, <code>fst</code>.</p>
<p><code>fst</code> returns first element of any pair, it has the type <code>('a * 'b) -&gt; 'a</code> (the language of types is described in next section). This means for any pair with type of <code>('a * 'b)</code>, <code>fst</code> will return value with type of <code>'a'</code>. When applied on a value typed <code>(bool * int)</code> it will return <code>bool</code>, and when applied on a value typed <code>( (string * bool) * string )</code> it will return <code>(string * bool)</code> etc.</p>
<p>Subtyping defines a relationship that if a type <code>t1</code> is subtype of a type <code>t2</code>, this means that <code>t1</code> can be given to context where <code>t2</code> is expected. This usually means <code>t1</code> already has all properties of <code>t2</code>.</p>
<p>For instance, a record with fields <code>a: int, b: bool</code> can be passed to a function that expects a parameter typed <code>{a: int}</code>. The same effect can also be obtained by row polymorphism, which we’ll see in it’s own chapter.</p>
<p>Subtyping is generally divided into two branches; nominal and structural subtyping. Structural subtyping relation is syntactic, ie. type system can decide if one type is subtype of another type just by inspecting their forms (like in the example above). Nominal subtyping requires definition from the programmer or a pre-defined type lattice from the language designer, ie. there is no way for a type system to infer subtyping relation between int and float types if this relation is not built-in; similarly a Square is not a subtype of Shape unless the programmer manually defines this subtyping relation.</p>
<h1 id="hindley-damas-milner-type-system-and-type-inference">Hindley-Damas-Milner type system and type inference</h1>
<p>Manually giving types to program terms may be impractical, since programs can contain hundreds of definitions. Type inference is the process of deriving types for program terms. As an example of language with type inference, OCaml programs with no type annotations can be type-inferred and checked. This results in more concise programs. Type annotations can still be given for documentation or error reporting purposes.</p>
<p>Hindley-Damas-Milner(abbreviated as HM henceforth) type system is likely to be the most widely-used type system. It has a great property that if a term is typeable under HM type system, HM inference algorithm will result with a most general type (also called ‘principal type’).</p>
<p>HM is the system lies behind statically typed functional languages like the ML family of languages and Haskell.</p>
<p>In HM, a polymorphic type is specified as a ‘type scheme’. A type scheme is a type with universally quantified type variables. Type system’s job then is to give terms of a program type schemes so that for each instantiation of the scheme, the result is well-typed.</p>
<p>HM type system produces polymorphic types (type schemes) only for some expressions, in the case of ML family of languages, it’s the <code>let</code> expression. A <code>let</code> expression binds a name to an expression and introduces a scope that the name is bound to the expression. HM type system gives a most general type to that name, and so every use of this name in the scope is valid if a valid instantiation of the required type can be obtained from the type scheme it’s given.</p>
<pre><code>let id a = a in
(id 1, id true)</code></pre>
<p>Here the <code>id</code> function is given the type scheme of <code>forall a. a -&gt; a</code>. The variable <code>a</code> is called to be ‘universally quantified’. In <code>id 1</code>, <code>id</code>s type is instantiated from type scheme as <code>int -&gt; int</code> by substituting <code>a</code> with <code>int</code>. Same operation is done in <code>id true</code> and <code>id</code> is given the type <code>bool -&gt; bool</code>. So this term is well-typed under HM.</p>
<p>Inferring a type scheme is done by assigning fresh type variables to terms with unknown types and unifying this variables with concrete types while type checking rest of the term. At the end, types that are not unified with concrete types will remain polymorphic.</p>
<p>This is also called let-polymorphism because polymorphic type (type scheme) generation is only done in let expressions.</p>
<p>Note that not all names defined in a let expression can be generalized. As an example, see this classic example:</p>
<pre><code>let c = ref (fun x -&gt; x) in
c := (fun x -&gt; x+1);
!c true</code></pre>
<p>Giving <code>c</code> the type <code>forall a. ('a -&gt; 'a) ref</code> (in OCaml syntax) makes this example well-typed, which is wrong. Instead, type system should have given the type <code>( '_a -&gt; '_a) ref</code>, then during the type checking <code>c := (fun x -&gt; x + 1)</code>, <code>_a</code> should be unified with <code>int</code>. Note that since <code>('_a -&gt; '_a) ref</code> doesn’t quantify the variable <code>a</code>, after this point, type of <code>c</code> will be <code>(int -&gt; int_ ref</code>, so applying it a bool value will fail.</p>
<p>Let-polymorphism has some interesting properties. It’s simple, but it still accepts realistic programs. And even though its worst-case efficiency is exponential on the size of input program, most realistic programs don’t hit the worst case and have linear time complexity. For this reason, let-polymorphism is named as the ‘sweet-spot’ of type systems.</p>
<p>Another great property of HM type system is that it can be reduced to constraint generation and solving steps. These separate steps lead to a more modular algorithm, with changeable constraint generators and solvers. HM(X)<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> is a formalization of this observation.</p>
<h1 id="row-polymorphism">Row polymorphism</h1>
<p>Rows are a way to encode ‘labeled products’. A product type is a type that contains some other types; for example, a pair is a product type of two other types (<code>(int * bool)</code> is a product type with <code>int</code> and <code>bool</code> parts).</p>
<p>A ‘labeled sum’ is a product type, but with labels. For instance:</p>
<pre><code>type error = { code : int; reason : string }</code></pre>
<p>is just like an <code>(int * string)</code> type, but it has labels for subparts. There are severals ways to encode labeled products as rows. Here’s one way:</p>
<pre><code>type error = { code : abs int; reason : abs int; rho[code,int] }</code></pre>
<p>This syntax is similar to Rémy’s, which was also used in our project. Last <code>rho</code> is the polymorphic part, it means that an error type is a row type with <code>code</code> and <code>reason</code> fields with gives types, but with a polymorphic part that can be unified with any record except the fields labeled with <code>code</code> or <code>int</code> (some systems accept field repetition, like in Daan Leijen’s “Extensible records with scoped labels”).</p>
<p>Row polymorphism gives us a subtyping-like effect. For example, this function:</p>
<pre><code>fun x -&gt; x.a + 1</code></pre>
<p>Now has the type <code>{ a : int; rho[a] } -&gt; int</code> and we can apply it to the record <code>{ x : some_type; y : some_other_type; a : int; ... }</code>, like in structural subtyping.</p>
<p>The way row polymorphism and structural subtyping accept this term are completely different though. In row polymorphism, parameter type <code>{a : int; rho[a] }</code> is being unified with <code>{ x : some_type; y : some_other_type; a : int; ... }</code>, so parameter type of function is actually getting more specialized. On the other hand, in subtyping, extra fields of actual parameter is being forgotten, like casting a type to it’s super type.</p>
<p>More words will be said about row polymorphism in later sections.</p>
<h1 id="multi-stage-programming">Multi-stage programming</h1>
<p>Multi-stage programming languages makes distinguished evaluation stages in run-time. Some part of the program can be generated depending on some user input.</p>
<p>Evaluation is done at stage 0. A program generated in staged 0 has stage 1, program generated in stage 1 has stage 2 etc. This notion of separate stages has some semantic effects. Each stage has a scope, but most systems offer a mechanism to lift a value from a lower stage.</p>
<p>Being able to generate code in run-time gives us a way to specialize algorithms depending on user input. Let’s look at a classic example:</p>
<pre><code>let rec pow_body_gen n =
  if n = 0 then &lt;1&gt;
  else &lt;a * ~(pow_body_gen (n-1))&gt;;;</code></pre>
<p><code>pow_body_gen</code> takes an integer <code>n</code> and returns the code <code>a * a * a * ... * 1</code>, a to the power of n. With the help of this function, we can generate a specialized function <code>power_five</code> which raises an integer to the power 5, but without running a loop or recursively calling a function:</p>
<pre><code>let rec power_five a = &lt;let a = ~lift(a) in ~(pow_body_gen 5)&gt;;;</code></pre>
<p>For example, output of <code>power_five 12</code> in our interpreter is <code>&lt;let a = 12 in (a * (a * (a * (a * (a * 1)))))&gt;</code>. This is a code value. Notice the <code>a</code> value, which is multiplied by itself, 5 times.</p>
<p>To actually run this code, we can call <code>run</code>(stage primitives will be explained shortly):</p>
<pre><code>&gt; run(power_five 12);;
248832</code></pre>
<p>Our multi-stage language has three primitives for staging operations: angle brackets indicate a staged expression, contents of staged expression will be run in stage <code>n+1</code>, where n is the current stage level. Tilde (~) ‘unboxes’ a staged expression, expressions indicated with a unboxing operator will be run in stage <code>n-1</code>. Unboxing operator can only be used in stages n &gt; 0. Finally, <code>run</code> primitive is used to run a closed code value at stage 0.</p>
<p>Note that code values can be open, but only closed code values can be run. Our <code>pow_body_gen</code> function returns a code value with an unbound variable <code>a</code>, then in <code>power_five</code>, we define <code>a</code> in stage 1, and by unboxing <code>pow_body_gen 5</code>, we actually generate a bigger code value, with <code>a</code> defined. Now our code value is closed, so it can be run.</p>
<p>There is also a relation between multi-stage programming and partial evaluation, but this is out of this text’s scope.</p>
<h1 id="our-work">Our work</h1>
<p>Wallace is a type inference library, supporting subtyping. “Its goal is to serve as a plug-in component in the design of a constraint-based type-checker, regardless of the programming language being analyzed.”<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<p>We used Wallace to get subtyping in our language. Wallace comes with an example language, called ‘toy’, which has extensible records built-in. For simplicity, instead of generating constraints for Wallace, we first translated our multi-stage language to record calculus. Translation to record calculus preserves semantics, but eliminates staged computations<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>. Our record calculus is then translated to ‘toy’. ‘toy’ infers types for us, by generating and passing constraints to Wallace.</p>
<p>We then compared our type system with<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>. This expression:</p>
<pre><code>run((fun c -&gt; &lt;(let x = 1 in ~c, let y = 1 in ~c)&gt;) &lt;1&gt;);;</code></pre>
<p>is not typeable under polymorphic type system, but is well-typed in our system with subtyping. By mapping the calculated type back to our multi-stage language, we got subtyping in the multi-stage language for free.</p>
<p>Wallace is used as is, without any modifications. But we changed ‘toy’ language for our purposes. We first added some numerical types with subtyping relations. ‘toy’ had a problem that it was generalizing every let bindings, without value restriction. To overcome this problem, we added an extra lambda wrapper in the translator when value restriction is needed. Since our type system already had value restriction, it was trivial to apply the same test to check if an expression is expansive. Expansive expressions are those that may allocate new memory cells. Expansive expressions are not generalized.</p>
<p>Another problem with ‘toy’ language is that it doesn’t support toplevel declarations, and adding that wasn’t easy. To overcome this without any serious modifications in ‘toy’s source, we collected all well-typed toplevel expressions and declarations, and then generated a <code>let .. in ..</code> chain of toplevel declarations to give ’toy’ type checker. This means for every new toplevel phrase, ‘toy’ now type checks all old expressions too. This may not be efficient but works fine with our purposes, and it was easy to implement.</p>
<p>Implementing type inference: Implementing a type inference system is hard to do elegantly. One problem is already discussed <a href="/posts/2013-02-15-rowlar-kindlar.html">in my blog (in Turkish)</a>. Every non-trivial type system requires types to be separated into kinds. After that, every type variable will also have kind information and it’s a type error to unify a type variable with wrong kinds. Type applications also checked with the help of kinds.</p>
<p>Kinds are generally checked at run-time. Type systems with kinds usually lead to a simpler type language. For instance, almost all types mentioned in this text can be represented by this simply type language:</p>
<pre><code>type kind =
  | KStar                 (* kind of term types *)
  | KRow                  (* kind of row types *)
  | KArr of (kind * kind) (* kind of type constructors *)

type ty =
  | TCon of tycon     (* constant *)
  | TVar of tyvar     (* type variable *)
  | TApp of (ty * ty)
      (* type application, to be well-typed
         kind of first ty should be KArr (k2, k)
         and second ty should be k2 *)
and tyvar = (tyvarlink ref * kind)
and tyvarlink =
  | NoLink of id (* just a type variable *)
  | LinkTo of ty (* equated to a ty *)
and tycon = (id * kind) (* kind should be always KStar *)</code></pre>
<p>We decided to take a different path, instead of keeping kind information for every type, we created new type constructors for every type and then separating differently kinded type variables with different types. This lead us to this type language:</p>
<pre><code>type ty =
 | TInt
 | TBool
 | TUnit

 | TPair of ty * ty

 | TList of ty
 | TRef  of ty

 | TFun of ty * ty
 | TRec of tyrec
 | TVar of typevar

 | TBox of tyrec * ty

...

and typevar = (tyvarlink * int) ref
and tyvarlink = ty link

and fieldvar = (fieldvarlink * int) ref
and fieldvarlink = field link

and recvar = (recvarlink * int * IdSet.t) ref
and recvarlink = tyrec link</code></pre>
<p>We also needed a sum type for handling differently kinded type variables:</p>
<pre><code>and linkvar =
 | TV of typevar
 | FV of fieldvar
 | RV of recvar</code></pre>
<p>This also made our algorithms more complex, since we had more cases to handle.</p>
<p>This representation is not without any advantages though. For instance, writing a pretty-printer was very easy, because we could easily print a pair and a function or a list and a ref differently, even though they share similar structure, with simple pattern matching.</p>
<p>A note on pretty-printing: OCaml has a great pretty-printer library in stdlib. It’s simple but powerful at the same time, allowing printing most complex structures easily. It interprets some characters in input strings as boxes/spaces indicators etc. and formats the text. For instance, this code:</p>
<pre><code>printf &quot;@[&lt;hov 2&gt;~(&quot;; print_exp exp; printf &quot;)@]&quot;</code></pre>
<p>Here <code>@[&lt;hov 2&gt;</code> part creates a new “horizontal or vertical” box. This box has a property that when a line is split to two lines, every other newline pointers will also be split into separate lines. So contents of this box is either displayed as a single line, or separated from every newline pointers. Later, <code>print_exp exp;</code> part prints the expression inside the box. And lastly, `printf “)@]” closes the paren, then closes the box, so that the text comes later will not be bound with this line split rule.</p>
<p>This gives an easy and concise way to print even most complex structures.</p>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>A Theory of Type Polymorphism in Programming – Robin Milner<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>Code examples taken from <a href="http://vimeo.com/61576198">Dependently Typed Functional Programming with Idris</a> course slides.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p><a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.73.3971">Notes on HM(X)</a><a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p><a href="http://gallium.inria.fr/~fpottier/wallace/">Wallace</a><a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" role="doc-endnote"><p>Choi, Aktemur, Yi, Tatsuta: Static Analysis of Multi-Staged Programs via Unstaging Translation<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6" role="doc-endnote"><p>Kim, Yi, Calcagno: A Polymorphic Modal Type System for List-Like Multi-Staged Languages<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></summary>
</entry>

</feed>
