<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>osa1.net - Posts tagged plt</title>
    <link href="http://osa1.net/tags/plt.xml" rel="self" />
    <link href="http://osa1.net" />
    <id>http://osa1.net/tags/plt.xml</id>
    <author>
        <name>Ömer Sinan Ağacan</name>
        <email>omeragacan@gmail.com</email>
    </author>
    <updated>2025-01-18T00:00:00Z</updated>
    <entry>
    <title>Error handling in Fir</title>
    <link href="http://osa1.net/posts/2025-01-18-fir-error-handling.html" />
    <id>http://osa1.net/posts/2025-01-18-fir-error-handling.html</id>
    <published>2025-01-18T00:00:00Z</published>
    <updated>2025-01-18T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>A while ago I came up with an <a href="https://gist.github.com/osa1/38fd51abe5247462eddb7d014f320cd2">“error handling expressiveness benchmark”</a>, some common error handling cases that I want to support in <a href="https://github.com/fir-lang/fir">Fir</a>.</p>
<p>After 7 months of pondering and hacking, I think I designed a system that meets all of the requirements. Error handling in Fir is safe, expressive, and convenient to use.</p>
<p>Here are some examples of what we can do in Fir today:</p>
<p>(Don’t pay too much attention to type syntax for now. Fir is still a prototype, the syntax will be improved.)</p>
<p>When we have multiple ways to fail, we don’t have to introduce a sum type with all the possible ways that we can fail, we can use variants:</p>
<pre><code>parseU32(s: Str): Result[[InvalidDigit, Overflow, EmptyInput, ..r], U32]
    if s.len() == 0:
        return Result.Err(~EmptyInput)

    let result: U32 = 0

    for c in s.chars():
        if c &lt; &#39;0&#39; || c &gt; &#39;9&#39;:
            return Result.Err(~InvalidDigit)

        let digit = c.asU32() - &#39;0&#39;.asU32()

        result = match checkedMul(result, 10):
            Option.None: return Result.Err(~Overflow)
            Option.Some(newResult): newResult

        result = match checkedAdd(result, digit):
            Option.None: return Result.Err(~Overflow)
            Option.Some(newResult): newResult

    Result.Ok(result)</code></pre>
<p>An advantage of variants is, in pattern matching, we “refine” types of binders to drop handled variants from the type. This allows handling some of the errors and returning the rest to the caller:</p>
<pre><code>defaultEmptyInput(res: Result[[EmptyInput, ..r], U32]): Result[[..r], U32]
    match res:
        Result.Err(~EmptyInput): Result.Ok(0u32)
        Result.Err(other): Result.Err(other)
        Result.Ok(val): Result.Ok(val)</code></pre>
<p>Here <code>EmptyInput</code> is removed from the error value type in the return type. The caller does not need to handle <code>EmptyInput</code>.</p>
<p>(We don’t refine types of variants nested in other types for now, so the last two branches cannot be replaced with <code>other: other</code> for now.)</p>
<p>Another advantage is that they allow composing error returning functions that return different error types:</p>
<p>(Fir supports variant constructors with fields, but to keep things simple we don’t use them in this post.)</p>
<pre><code>readFile(s: Str): Result[[IoError, ..r], Str]
    # We don&#39;t have the standard library support for file IO yet, just return
    # an error for now.
    Result.Err(~IoError)

parseU32FromFile(filePath: Str): Result[[InvalidDigit, Overflow, EmptyInput, IoError, ..r], U32]
    let fileContents = match readFile(filePath):
        Result.Err(err): return Result.Err(err)
        Result.Ok(contents): contents

    parseU32(fileContents)</code></pre>
<p>In the early return I don’t have to manually convert <code>readFile</code>s error value to <code>parseU32</code>s error value to make the types align.</p>
<p>Variants work nicely with higher-order functions as well. Here’s a function that parses a vector of strings, returning any errors to the caller:</p>
<pre><code>parseWith(vec: Vec[Str], parseFn: Fn(Str): Result[errs, a]): Result[errs, Vec[a]]
    let ret = Vec.withCapacity(vec.len())

    for s in vec.iter():
        match parseFn(s):
            Result.Err(err): return Result.Err(err)
            Result.Ok(val): ret.push(val)

    Result.Ok(ret)</code></pre>
<p>If I have a function argument that returns more errors than my callback, I can still call it without any adjustments:</p>
<pre><code>parseWith2(vec: Vec[Str], parseFn: Fn(Str): Result[[OtherError, ..r], a]): Result[[..r], Vec[a]]
    let ret = Vec.withCapacity(vec.len())

    for s in vec.iter():
        match parseFn(s):
            Result.Err(~OtherError): continue
            Result.Err(err): return Result.Err(err)
            Result.Ok(val): ret.push(val)

    Result.Ok(ret)</code></pre>
<p><code>parseWith2(vec, parseU32)</code> type checks even though <code>parseU32</code> doesn’t return <code>OtherError</code>.</p>
<p>Similarly, if I have a function that handles more cases, I can pass it as a function that handles less:</p>
<pre><code>handleSomeErrs(error: [Overflow, OtherError]): U32 = 0

parseWithErrorHandler(
        input: Str,
        handler: Fn([Overflow, ..r1]): U3
    ): Result[[InvalidDigit, EmptyInput, ..r2], U32]
    match parseU32(input):
        Result.Err(~Overflow): Result.Ok(handler(~Overflow))
        Result.Err(other): Result.Err(other)
        Result.Ok(val): Result.Ok(val)</code></pre>
<p>Here I’m able to pass <code>handleSomeErrs</code> to <code>parseWithErrorHandler</code>, even though it handles more errors than what <code>parseWithErrorHandler</code> argument needs.</p>
<h1 id="variants-as-exceptions">Variants as exceptions</h1>
<p>When we use variants as exception values, we end up with a system that is</p>
<ul>
<li>Safe: All exceptions need to be handled before <code>main</code> returns.</li>
<li>Flexible: All of the flexibility of variants shown above apply to exceptions as well.</li>
<li>Convenient:
<ul>
<li>Error values are implicitly propagated to the caller when not handled.</li>
<li>When a library uses one way of error reporting (error values or exceptions) and you need the other, conversion is just a matter of calling one function.</li>
</ul></li>
</ul>
<p>At the core of exceptions in Fir are these three functions:</p>
<ul>
<li><p><code>throw</code>, which converts a variant into an exception:</p>
<pre><code>throw(exn: [..r]): {..r} a</code></pre></li>
<li><p><code>try</code>, which converts exceptions into <code>Result.Err</code> values:</p>
<pre><code>try(cb: Fn(): {..exn} a): {..r} Result[[..exn], a]</code></pre></li>
<li><p><code>untry</code>, which converts a <code>Result.Err</code> value into an exception:</p>
<pre><code>untry(res: Result[[..errs], a]): {..errs} a</code></pre></li>
</ul>
<p>(The part in <code>{...}</code> is the exception variant type. The reason we use <code>{...}</code> instead of <code>[...]</code> is to be able to distinguish exception types from return types in function type signatures when one of the types are omitted. This part of the syntax may change in the future.)</p>
<p>Here are some of the code above, using exceptions instead of error values:</p>
<pre><code>parseU32Exn(s: Str): {InvalidDigit, Overflow, EmptyInput, ..r} U32
    if s.len() == 0:
        return throw(~EmptyInput)

    let result: U32 = 0

    for c in s.chars():
        if c &lt; &#39;0&#39; || c &gt; &#39;9&#39;:
            return throw(~InvalidDigit)

        let digit = c.asU32() - &#39;0&#39;.asU32()

        result = match checkedMul(result, 10):
            Option.None: throw(~Overflow)
            Option.Some(newResult): newResult

        result = match checkedAdd(result, digit):
            Option.None: throw(~Overflow)
            Option.Some(newResult): newResult

    result

readFileExn(s: Str): {IoError, ..r} Str
    # We don&#39;t have the standard library support for file IO yet, just throw
    # an error for now.
    throw(~IoError)

parseU32FromFileExn(filePath: Str): {InvalidDigit, Overflow, EmptyInput, IoError, ..r} U32
    parseU32Exn(readFileExn(filePath))

parseWithExn(vec: Vec[Str], parseFn: Fn(Str): {..errs} a): {..errs} Vec[a]
    let ret = Vec.withCapacity(vec.len())
    for s in vec.iter():
        ret.push(parseFn(s))
    ret</code></pre>
<p>When a library provides one of these, it’s trivial to convert to the other:</p>
<pre><code>parseU32UsingExnVersion(s: Str): Result[[InvalidDigit, Overflow, EmptyInput, ..r], U32]
    try({ parseU32Exn(s) })

parseU32UsingResultVersion(s: Str): {InvalidDigit, Overflow, EmptyInput, ..r} U32
    untry(parseU32(s))</code></pre>
<p>Nice!</p>
<hr />
<p>I’m quite excited about these results. There’s still so much to do, but I think it’s clear that this way of error handling has a lot of potential.</p>
<p>I’ll be working on some of the improvements I mentioned above (and I have others planned as well), and the usual stuff that every language needs (standard library, tools etc.). Depending on interest, I may also write more about variants, error handling, or anything else related to Fir.</p>
<p>You can try Fir online <a href="https://fir-lang.github.io/?errors">here</a>.</p>]]></summary>
</entry>
<entry>
    <title>When is inlining useful?</title>
    <link href="http://osa1.net/posts/2024-12-07-inlining.html" />
    <id>http://osa1.net/posts/2024-12-07-inlining.html</id>
    <published>2024-12-07T00:00:00Z</published>
    <updated>2024-12-07T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>Especially in high-level languages, inlining is most useful when it leads to:</p>
<ul>
<li>Optimizing the callee’s body based on the arguments passed.</li>
<li>Optimizing the call site based on the callee’s return value.</li>
</ul>
<p>Let’s look at some examples.</p>
<h1 id="example-avoiding-redundant-bounds-checks">Example: avoiding redundant bounds checks</h1>
<p>Suppose we have a library for decoding some format of binary files with length-prefixed vectors of 32-bit integers, with all integers encoded as little-endian.</p>
<p>A simple implementation would be:</p>
<pre><code>/// Decode a length-prefixed 32-bit unsigned integer vector.
///
/// # Panics
///
/// Panics if the buffer does not have enough bytes.
pub fn decode_u32_vec(buffer: &amp;[u8]) -&gt; Vec&lt;u32&gt; {
    let len = decode_32_le(buffer, 0) as usize;
    let mut vec = vec![0; len];
    for i in 0..len {
        vec[i] = decode_32_le(buffer, (i + 1) * 4);
    }
    vec
}

/// Decode a single 32-bit unsigned integer, encoded as little-endian.
///
/// # Panics
///
/// Panics if the buffer does not have 4 bytes starting at `offset`.
#[inline(never)]  // this version isn&#39;t inlined
pub fn decode_32_le(buffer: &amp;[u8], offset: usize) -&gt; u32 {
    let b1 = buffer[offset];
    let b2 = buffer[offset + 1];
    let b3 = buffer[offset + 2];
    let b4 = buffer[offset + 3];
    u32::from_le_bytes([b1, b2, b3, b4])
}</code></pre>
<p>This version is not ideal, because <code>decode_32_le</code> checks bounds on each byte access. (<a href="https://godbolt.org/z/eTMa38nqT">compiler explorer</a>)</p>
<p>We can improve it by checking the bounds for all of the reads once:</p>
<pre><code>#[inline(never)]  // this version isn&#39;t inlined
pub fn decode_32_le(buffer: &amp;[u8], offset: usize) -&gt; u32 {
    if buffer.len() &lt; 4 || buffer.len() - 4 &lt; offset {
        panic!();
    }
    let b1 = buffer[offset];
    let b2 = buffer[offset + 1];
    let b3 = buffer[offset + 2];
    let b4 = buffer[offset + 3];
    u32::from_le_bytes([b1, b2, b3, b4])
}</code></pre>
<p>The conditional makes sure that in the rest of the function the slice indices are all within bounds, so the compiler doesn’t generate bounds checks for the accesses. The compiler is also able to optimize further now to load just one 32-bit word from the memory, instead of reading one byte at a time. (<a href="https://godbolt.org/z/MG9EaE5G3">compiler explorer</a>)</p>
<p><code>decode_32_le</code> is quite good now, but it still has to do one bounds check, and since <code>decode_u32_vec</code> calls it in each iteration, it does a bounds check in each iteration.</p>
<p>Ideally we want to be able to do one bounds check before the loop, just like we did in <code>decode_32_le</code>, and then omit bounds checks within the loop:</p>
<pre><code>pub fn decode_u32_vec(buffer: &amp;[u8]) -&gt; Vec&lt;u32&gt; {
    let len = decode_32_le(buffer, 0) as usize;
    if buffer.len() &lt; (len + 1) * 4 {
        panic!();
    }
    let mut vec = vec![0; len];
    for i in 0..len {
        vec[i] = decode_32_le(buffer, (i + 1) * 4);
    }
    vec
}</code></pre>
<p>But this cannot make the bounds checks in <code>decode_32_le</code> disappear, as it may have other call sites that may not always check the bounds before calling it.</p>
<p>Inlining <code>decode_32_le</code> in the use effectively lets us propagate the information in the call site to the callee’s code, and optimize it based on this information. If we change the <code>inline(never)</code> to <code>inline</code> in <code>decode_32_le</code>, with the extra bounds check in <code>decode_u32_vec</code>, we now check bounds once before the loop and don’t check it again in the loop. (<a href="https://godbolt.org/z/EeGbYrfP7">compiler explorer</a>)</p>
<h1 id="example-avoiding-redundant-error-checks">Example: avoiding redundant error checks</h1>
<p>Dart doesn’t have unsigned integers, and many standard library functions throw an <code>ArgumentError</code> when they are passed negative numbers.</p>
<p>One example of these functions is the <a href="https://api.dart.dev/dart-core/int/operator_triple_shift.html">unsigned right shift operator</a>. In many call sites, the shift amount is a <a href="https://github.com/dart-lang/sdk/blob/abb17bc59d8163273451b3ffb2aba784d20001b4/sdk/lib/_internal/wasm/lib/internal_patch.dart#L88-L97">constant positive number</a>. If we call the standard library function in these cases, the library function will check the sign of the arguments that we already know in the call site to be positive.</p>
<p>When we inline <a href="https://github.com/dart-lang/sdk/blob/abb17bc59d8163273451b3ffb2aba784d20001b4/sdk/lib/_internal/wasm/lib/boxed_int.dart#L94-L107">the operator</a> in a call site where the shift argument is constant, the conditional becomes <code>if (constant &lt; 0) throw ...</code>. The compiler can then simplify the condition as <code>true</code> or <code>false</code>, and then simplify this code to just <code>throw ...</code> or eliminate the conditional.</p>
<p>The <code>mix64</code> function linked above when compiled to Wasm, without inlining the right shift operator:</p>
<pre><code>(func $mix64 (param $var0 i64) (result i64)
  ...
  local.tee $var0
  i64.const 24
  call $BoxedInt.&gt;&gt;&gt;
  ...)

(func $BoxedInt.&gt;&gt;&gt; (param $var0 i64) (param $var1 i64) (result i64)
  local.get $var1
  i64.const 64
  i64.lt_u
  if
    local.get $var0
    local.get $var1
    i64.shr_u
    return
  end
  local.get $var1
  i64.const 0
  i64.lt_s
  if
    i32.const 64
    local.get $var1
    struct.new $BoxedInt
    call $ArgumentError
    call $Error._throwWithCurrentStackTrace
    unreachable
  end
  i64.const 0)</code></pre>
<p>With the shift operator inlined:</p>
<pre><code>(func $mix64 (param $var0 i64) (result i64)
  ...
  local.get $var0
  i64.const 24
  i64.shr_u
  ...)</code></pre>
<p>The call to <code>BoxedInt.&gt;&gt;&gt;</code> with error checking is optimized to a single <code>shr_u</code> (shift right, unsigned) instruction.</p>
<h1 id="example-avoiding-boxing">Example: avoiding boxing</h1>
<p>In languages where most values are passed around as boxed, inlining can eliminate boxing.</p>
<p>A common use case where this happens is FFI: pointers/references obtained from FFI calls need to be wrapped in a struct/class/etc. to make them the same “shape” as the language’s native values.</p>
<p>When you have a function that gets a reference from an FFI call, and pass it around to more FFI calls, inlining these FFI calls can avoid boxing the pointer/reference value.</p>
<p>Somewhat silly example, in Dart:</p>
<pre><code>import &#39;dart:ffi&#39;;

@Native&lt;Pointer&lt;Int64&gt; Function()&gt;()
external Pointer&lt;Int64&gt; intPtr();

@Native&lt;Int64 Function(Pointer&lt;Int64&gt;)&gt;()
external int derefIntPtr(Pointer&lt;Int64&gt; ptr);

void main() {
  Pointer&lt;Int64&gt; ptr = intPtr();
  ptr += 1;
  int i = derefIntPtr(ptr);
  print(i);
}</code></pre>
<p>Relevant parts of the generated code when compiled to Wasm:</p>
<pre><code>(func $main
  ...
  call $intPtr
  struct.get $Pointer $field2
  i32.const 8
  i32.add
  call $ffi.derefIntPtr (import)
  ...)

(func $intPtr (result (ref $Pointer))
  i32.const 71
  i32.const 0
  call $ffi.intPtr (import)
  struct.new $Pointer)</code></pre>
<p><code>intPtr</code> allocates a struct, which the call site directly unpacks (reads the field). Inlining <code>intPtr</code> eliminates this allocation:</p>
<pre><code>(func $main
  ...
  call $ffi.intPtr (import)
  i32.const 8
  i32.add
  call $ffi.derefIntPtr (import)
  ...)</code></pre>
<h1 id="example-avoiding-polymorphism">Example: avoiding polymorphism</h1>
<p>When a monomorphic type is passed to a polymorphic function, the polymorphic function can often be inlined to avoid polymorphic access to the monomorphic type.</p>
<p>An example, again in Dart, is <code>Int64List</code>, which is a monomorphic <code>List&lt;int&gt;</code>. It stores the integers unboxed, and when used directly, the integer arguments and return values do not need to be boxed.</p>
<p>When used in a polymorphic site though, the integer elements need to be boxed. Example:</p>
<pre><code>import &#39;dart:typed_data&#39;;

int sum(List&lt;int&gt; list) {
  int ret = 0;
  for (int i = 0; i &lt; list.length; i += 1) {
    ret += list[i];
  }
  return ret;
}

void main() {
  Int64List intList = Int64List.fromList([1, 2, 3, 4]);
  sum(intList);
  sum([1, 2, 3, 4]);
}</code></pre>
<p>Relevant parts of the output when compiled to Wasm:</p>
<pre><code>(func $main
  ;; Allocate `Int64List`, call `sum`:
  ...
  call $sum

  ;; Allocate the other `List&lt;int&gt;`, call `sum`:
  ...
  call $sum)

(func $sum (param $var0 (ref $Object))
  (local $var1 i64)
  (local $var2 i64)
  loop $label0
    ...
    if
      ...
      ;; Virtual call to `operator []`:
      struct.get $Object $field0
      i32.const 747
      i32.add
      call_indirect (param (ref $Object) i64) (result (ref null $#Top))

      ;; The virtual call returns a boxed integer, which we directly unbox:
      ref.cast $BoxedInt
      struct.get $BoxedInt $field1
      i64.add
      ...
    end
  end $label0)</code></pre>
<p>If we inline <code>sum</code>, the loop that iterates the <code>Int64List</code> accesses the unboxed integers directly:</p>
<pre><code>(func $main
  ...
  loop $label1
      ...
      local.get $var3
      local.get $var4
      local.get $var1
      i32.wrap_i64
      ;; Array access is now direct, no boxing.
      array.get $Array&lt;WasmI64&gt;
      i64.add
      local.set $var3
      local.get $var1
      i64.const 1
      i64.add
      local.set $var1
      br $label1
    end
  end $label1)</code></pre>
<p>A similar case is when a monomorphic type is used directly, but via a polymorphic interface. In the <code>Int64List</code> type above, <code>List64List.[]</code> is an override of <a href="https://api.dart.dev/dart-core/List/operator_get.html"><code>List&lt;E&gt;.[]</code></a>, and all overrides of a method need to have the same type.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>So when not inlined, it needs to return a boxed integer:</p>
<pre><code>(func $Int64List.[] (param $var0 (ref $Object)) (param $var1 i64) (result (ref null $#Top))
  ...
  array.get $Array&lt;WasmI64&gt;
  struct.new $BoxedInt)</code></pre>
<p>Similar to the previous example, inlining it eliminates the boxing in monomorphic use sites, as the allocated struct is immediately unboxed.</p>
<h1 id="example-eliminating-closures-and-indirect-calls">Example: eliminating closures and indirect calls</h1>
<p>In languages without monomorphisation, higher-order function arguments need to be allocated as closures.</p>
<p>When calling a closure, the caller needs to get the function pointer from the closure and call the function via the pointer.</p>
<p>These closure allocations and indirect function calls can be too slow in hot loops.</p>
<p>An example of this is in <a href="https://github.com/google/protobuf.dart">Dart protobuf library</a>. In protobufs, packed fields are encoded as a length prefix followed by the elements. In the Dart library, we decode these fields using <a href="https://github.com/google/protobuf.dart/blob/610943a3bed70c1c2079af5fca02462df10d223f/protobuf/lib/src/protobuf/coded_buffer.dart#L271-L277">this helper</a>:</p>
<pre><code>void _readPacked(CodedBufferReader input, void Function() readFunc) {
  input._withLimit(input.readInt32(), () {
    while (!input.isAtEnd()) {
      readFunc();
    }
  });
}</code></pre>
<p>Here the caller of <code>_readPacked</code> passes <code>readFunc</code> as a closure (allocation). <code>_readPacked</code> then allocates another closure, to be passed to <code>_withLimit</code>.</p>
<p>That’s two closure allocations for every packed field in the input. Calling these closures are slow too, because they’re indirect.</p>
<p>Inlining <code>_withLimit</code> in <code>_readPacked</code>, and <code>_readPacked</code> in its call sites (in <code>_mergeFromCodedBufferReader</code>) eliminates closure allocations, and calls to the closures become direct function calls. This improves packed field decoding significantly.</p>
<p>(This change isn’t merged yet, the PR is <a href="https://github.com/google/protobuf.dart/pull/959">here</a>.)</p>
<h1 id="a-trick-for-effective-inlining-outlining">A trick for effective inlining: outlining</h1>
<p>Consider <code>Int64List.[]</code> again. The implementation needs to check that the index is in bounds of the array, and throw an exception if not: (slightly simplified)</p>
<pre><code>class Int64List ... {
  ...

  @override
  int operator [](int index) {
    if (length.leU(index)) { // shorthand for `index &lt; 0 || index &gt;= length`
      ... // throw exception
    }
    return _data.read(index);
  }
}</code></pre>
<p>To avoid boxing when calling this function we want to always inline it, but if we’re not careful with the error throwing code path (the <code>if</code> body above), the function can get quite large, and when inlined the binary can bloat up significantly.</p>
<p>Ideally we want to inline the happy path that can lead to improvements when inlined, and leave the error path separate in a function, so that we can have the benefits of inlining without adding to the binary size too much.</p>
<p>This can be done by moving the error handling code to a separate function, and making sure that separate function is never inlined (ideally with an annotation to the compiler). In the example above, this may look like:</p>
<pre><code>class Int64List ... {
  @override
  @pragma(&#39;inline&#39;)
  int operator [](int index) {
    if (length.leU(index)) { // shorthand for `index &lt; 0 || index &gt;= length`
      fail(index, length);
    }
    return _data.read(index);
  }
}

@pragma(&#39;never-inline&#39;)
void fail(int index, int length) { ... }</code></pre>
<p>With this it doesn’t matter how large the error handling code is, because we never inline it.</p>
<p>This way of separating inlineable parts of a function from the parts we don’t want to inline is sometimes called “outlining” or “partial inlining”. We can do it manually (as in the example above), but it can also be done by a compiler based on heuristics.</p>
<p>An example transformation to this is <a href="https://www.cambridge.org/core/services/aop-cambridge-core/content/view/75629BBEDB11D8463553A09BF5DEA235/S0956796809007175a.pdf/div-class-title-the-worker-wrapper-transformation-div.pdf">GHC’s worker/wrapper transformation</a>, which splits a function into parts that (1) handle unboxing and boxing before calling the function’s body, and (2) the function’s body that work on the unboxed representations of the arguments. (1) is then inlined to avoid redundant boxing of the arguments and return values.</p>
<p>Another example is <a href="https://github.com/WebAssembly/binaryen">wasm-opt</a> which does <a href="https://github.com/WebAssembly/binaryen/blob/6fe5103ab58a4eb751998d13768a0f25795a0de6/src/passes/Inlining.cpp#L684-L754">partial inlining</a>.</p>
<h1 id="a-tip-for-effective-inlining-avoid-inlining-in-slow-paths">A tip for effective inlining: avoid inlining in slow paths</h1>
<p>In the <a href="https://github.com/google/protobuf.dart/blob/610943a3bed70c1c2079af5fca02462df10d223f/protobuf/lib/src/protobuf/coded_buffer.dart#L54-L267">protobuf decoding loop</a> that we mentioned above, we have a big type switch to determine how to decode a field. It looks like this:</p>
<pre><code>switch (fieldType) {
  case PbFieldType._OPTIONAL_BOOL:
    fs._setFieldUnchecked(meta, fi, input.readBool());
    break;
  ...
  case PbFieldType._REPEATED_UINT64:
    final list = fs._ensureRepeatedField(meta, fi);
    if (wireType == WIRETYPE_LENGTH_DELIMITED) {
      _readPacked(input, () =&gt; list.add(input.readUint64()));
    } else {
      list.add(input.readUint64());
    }
    break;
  ...
  // 37 cases in total. 
}</code></pre>
<p>Assume that there’s a function that throws an exception (e.g. <code>_ensureRepeatedField</code> above), and it’s used in only once, in one of the <code>case</code>s of the <code>switch</code>.</p>
<p>Because the function is only used once, it may look like inlining it makes sense, as that would eliminate function call overhead, shave at least a few bytes off the binary, and potentially allow optimizations in the call site.</p>
<p>However since this switch is in a hot loop, and the inlined code branch is taken very rarely (unless the application is receiving a lot of malformed input), inlining this slow path can make the instruction cache usage much worse and slow down the decoder.</p>
<p>This kind of inlining can commonly happen when optimizing for size, e.g. with <code>wasm-opt -Os</code>. Because inlining single-use functions reduce binary sizes, optimization modes that aim to make the final binaries smaller can inline slow-path error throwing functions.</p>
<p>If we really want to inline a slow-path function, a way to avoid making instruction cache usage worse is to move the slow-path code to the end of the function, away from the common code paths.</p>
<p>This is often done with branch prediction hints, such as clang’s <a href="https://llvm.org/docs/BranchWeightMetadata.html#builtin-expect"><code>__builtin_expect</code></a>. When a branch is annotated as “not likely to be taken”, the compiler can move the branch target (the basic blocks) to the end of the current function, away from the hot code. This gives us the binary size benefits of inlining single-use functions, without filling the instruction cache with instructions that will never be executed.</p>
<h1 id="remarks">Remarks</h1>
<p>The main reason why the examples above are mostly in Dart is because I’ve been spending most of my time this year optimizing Dart’s Wasm backend’s standard library implementation, so the examples are still fresh in my memory.</p>
<p>The principles apply to most languages: inlining a function makes any information about the arguments available to the function’s body, and any information on its return value to the call site.</p>
<p>A big part of optimizing high-level statically-typed languages is about avoiding polymorphism, boxing, and redundant error checks. I’m not aware of any cases where inlining a function in a high-level language, when it doesn’t result in improving one of these, is worthwhile.</p>
<p>In a lower-level language with monomorphisation and control over allocations (e.g. Rust, C++), monomorphisation eliminates polymorphism in compile time, boxing is explicit, and redundant checks can be avoided by using unchecked (unsafe) functions.</p>
<p>In these cases where programs are often much faster by default, inlining to avoid stack/register shuffling and simplify control flow can make a difference.</p>
<p>One example of simplified control flow making a big difference can be seen in <a href="https://osa1.net/posts/2024-11-29-how-to-parse-3.html">the previous post</a>, where implementing a recursive parsing function in a non-recursive way improved performance by 22%.</p>
<h1 id="updates">Updates</h1>
<ul>
<li><strong>2024-12-07:</strong> Added link to wasm-opt in partial inlining section.</li>
<li><strong>2025-02-12:</strong> Added a higher-order function example.</li>
<li><strong>2025-02-14:</strong> Added a section about inlining in slow paths.</li>
</ul>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>More accurately, a method that can be called in a polymorphic call site needs to have a type that is a subtype of the least-upper-bound of the types of all functions that can be called in the polymorphic call sites.</p>
<p>Or more briefly, all methods in an override group need to have the same type.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></summary>
</entry>
<entry>
    <title>Resumable exceptions</title>
    <link href="http://osa1.net/posts/2024-11-04-resumable-exceptions.html" />
    <id>http://osa1.net/posts/2024-11-04-resumable-exceptions.html</id>
    <published>2024-11-04T00:00:00Z</published>
    <updated>2024-11-04T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>The main use case of resumable exceptions would be collecting a bunch of errors (instead of bailing out after the first one) to log or show to the user, or actually recovering and continuing from the point of error detection, rather than in a call site.</p>
<p><em>Why not design the code to allow error recovery, instead of using a language feature?</em> There are a few problems with this:</p>
<ul>
<li><p>Without a language feature to do this, libraries will have to implement their own ways to recover from errors, causing inconsistencies and fragmented ecosystem of error handling libraries.</p></li>
<li><p>With resumable exceptions, any code can be trivially made to transfer control to a exception handler, and back. Manually refactoring code to do the same can be a big task. This may even be infeasible.</p></li>
<li><p>With resumable exceptions as a part of the language, libraries will be designed with resumption in mind. Libraries that would normally not allow error recovery will allow error recovery, as it will be easy to do, and it will be a common thing to resume from errors.</p></li>
</ul>
<h1 id="example-use-case-parser-shared-by-a-compiler-and-a-language-server">Example use case: parser shared by a compiler and a language server</h1>
<p>Modern programming languages have complex syntax. Parsers for these languages are often thousands of lines of code.</p>
<p>Ideally, all tooling for a language would share the parser, as it’s a significant amount of work to implement, debug, maintain parsers for such large languages.</p>
<p>However not all of these tools will have the same error handling behavior. A compiler <em>cannot</em> continue in the presence of a parse error, but a language server <em>has to</em> continue.</p>
<p>With resumable exceptions, the compiler can abort after a parse error, the language server can provide placeholder AST nodes for failing parse operations and resume. This flexibility does not make the parser API any more complicated than a parser that throws an exception in any other language. A one-off refactoring script that uses the parser library doesn’t have to deal with error recovery just because the language server, which uses the same parser, needs to recover from parse errors and continue parsing.</p>
<h1 id="types-of-resumable-exceptions">Types of resumable exceptions</h1>
<p>With resumable exceptions <code>throw</code> expressions generate a value. The value depends on the exception type thrown. For example, a <code>FooDecodingException</code> can be resumed with a value of <code>Foo</code> provided by the handler.</p>
<p>This can be implemented with an abstract base class or typeclass/trait with a type parameter:</p>
<pre><code>// in a system with classes:
abstract class ResumableException&lt;Resume&gt; { 
    prim Resume throw();
    prim Never resume(Resume resumptionValue);
}

// or in a system with typeclasses/traits:
trait ResumableException&lt;Resume&gt; {
    prim fn throw(self) -&gt; Resume;
    prim fn resume(resumptionValue: Resume) -&gt; !;
}</code></pre>
<p>Here the <code>prim</code> keyword indicates that the <code>throw</code> and <code>resume</code> methods are provided by the compiler.</p>
<p><code>throw e</code> can then be type checked as <code>e.throw()</code>, and <code>resume e with value</code> can be type checked as <code>e.resume(value)</code>. Or we can use the function call syntax instead of special syntax for throwing and resuming.</p>
<p>Whether to make exceptions thrown by a function a part of its type signature or not is an orthogonal concern.</p>
<h1 id="exception-type-design-considerations">Exception type design considerations</h1>
<p>The same considerations when designing non-resumable exceptions apply to resumable exceptions:</p>
<ul>
<li>The more general an exception type gets, the less you can do with it.</li>
<li>We still want to distinguish “log and stop” kind of exceptions from recoverable ones.</li>
</ul>
<p>For example, it doesn’t make sense to resume from an <a href="https://api.dart.dev/stable/3.5.4/dart-core/ArgumentError-class.html"><code>ArgumentError</code></a>, so we don’t implement <code>ResumableException</code> for it.</p>
<p>To be able to meaningfully resume from an exception, the exception type should document when exactly it is thrown, or have a resumption value type that is specific enough to give an idea on when it is thrown.</p>
<p>For example, an exception that can be resumed with an <code>int</code> cannot be resumed without knowing what that <code>int</code> is going to be used for, so this should be documented. But an exception <code>FooDecodingError implements ResumableException&lt;Foo&gt;</code> makes it clear that it’s thrown when there’s an error during decoding a <code>Foo</code>, and the resumption value is the value to be used as the <code>Foo</code> being decoded.</p>]]></summary>
</entry>
<entry>
    <title>Subtyping and subsumption</title>
    <link href="http://osa1.net/posts/2024-10-21-subtyping-subsumption.html" />
    <id>http://osa1.net/posts/2024-10-21-subtyping-subsumption.html</id>
    <published>2024-10-21T00:00:00Z</published>
    <updated>2024-10-21T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>Subtyping is a relation between two types. It often comes with a typing rule called “subsumption”, which says that if type B is a subtype of type A (usually shown as <code>B &lt;: A</code>), then a value of type B can be assumed to have type A.</p>
<p>The crucial part is that subsumption is <em>implicit</em>, the programmer doesn’t explicitly cast the value with type <code>B</code> to type <code>A</code>.</p>
<p>When we make an operation implicit in a language, we need to make sure that it is (1) safe (2) performant. Users will be doing it without realizing, and we don’t want to accidentally break things or make them slow.</p>
<p>Let’s consider how we can make subsumption safe and performant.</p>
<h1 id="safety-of-subsumption">Safety of subsumption</h1>
<p>Different languages give different safety guarantees. High-level languages often guarantee:</p>
<ol type="1">
<li><p>Memory safety: a memory read or write shouldn’t cause undefined behavior.</p>
<p>Examples: out-of-bounds array accesses should be caught, dangling pointers shouldn’t be allowed or dereferencing them should be caught in runtime.</p></li>
<li><p>Type safety: static guarantees of the language’s type system should be uphold.</p>
<p>Example: if I have a function <code>f : A -&gt; B</code> and a value <code>x : A</code> after subsumption, <code>f(x)</code> shouldn’t fail in compile time or runtime.</p></li>
</ol>
<p>There could be different safeties that the language guarantees. Some of those safeties may also be checked in runtime instead of compile time.</p>
<p>Whatever safeties the language guarantees, they must be preserved with subsumption.</p>
<p>From a programmer’s perspective however, these are not enough to make sure that the program will work as before when subsumption is used. If I can pass a value of type <code>B</code> where <code>A</code> is expected, I need to make sure <code>B</code>, when used as <code>A</code>, acts like <code>A</code>.</p>
<p>This is called “behavioral subtyping” (or “substitutability”), and it depends on not the types of <code>A</code>’s operations but the observable behaviors of <code>A</code> and its subtypes.</p>
<p>I don’t have a good real-world example of this, but you can imagine two types with the same public APIs that work differently. Since the public APIs are the same one can be made subtype of the other and (1) and (2) would still be satisfied, but doing that would cause bugs when one is accidentally passed as the other.</p>
<h1 id="performance-of-subsumption">Performance of subsumption</h1>
<p>Definition of “fast” or “performant” also depends on the language. A C++ programmer’s fast and Python programmer’s fast are often not the same.</p>
<p>However in general, heap allocation should be avoided.</p>
<p>Object-oriented languages (as defined in my <a href="https://osa1.net/posts/2024-10-09-oop-good.html">previous post</a>) without multiple inheritance can often implement subsumption of reference values as no-op, i.e. values of type <code>B</code> work as <code>A</code> in runtime without any changes or copying.</p>
<p>Multiple inheritance makes things more complicated, but a reference to an object can still be converted to a reference of one of its supertypes by just <a href="https://people.montefiore.uliege.be/declercq/INFO0004/documents/vtable.html">adjusting the pointer value</a>.</p>
<p>With unboxed/value types, conceptually, the value needs to be copied as its supertype, but that operation is often no-op. Consider an unboxed record <code>(x: Int, y: Int, z: Int)</code> that we store in a variable <code>a</code>. In runtime, <code>a</code> actually holds multiple stack locations or registers. When we copy it as <code>let b: (x: Int, y: Int) = a</code>, we don’t have to allocate new stack locations for <code>b.x</code> and <code>b.y</code>, we just map those locations to the same locations as <code>a.x</code> and <code>a.y</code>. When we pass <code>b</code> to a function, we pass <code>a.x</code> and <code>a.y</code>.</p>
<p>Where copying becomes a requirement and prohibitive is when you have something like <code>ReadOnlyList&lt;(x: Int, y: Int, z: Int)&gt;</code> and want to upcast it to <code>ReadOnlyList&lt;(x: Int, y: Int)&gt;</code> (the records are unboxed). From the safety perspective this operation is fine, but you have to allocate a new list and copy all the values.</p>
<p>I think this is rarely a problem in practice though, because most generic types, like <code>List&lt;T&gt;</code>, end up being invariant in <code>T</code> anyway, because their API often uses <code>T</code> in both covariant and contravariant positions. So <code>List&lt;(x: Int, y: Int)&gt;</code> is not a supertype of <code>List&lt;(x: Int, y: Int, z: Int)&gt;</code> and subsumption does not apply.</p>
<h1 id="no-conclusions-this-time">No conclusions this time</h1>
<p>In this short post I just wanted to give some definitions that I’m hoping to refer to in future posts.</p>]]></summary>
</entry>
<entry>
    <title>OOP is not that bad, actually</title>
    <link href="http://osa1.net/posts/2024-10-09-oop-good.html" />
    <id>http://osa1.net/posts/2024-10-09-oop-good.html</id>
    <published>2024-10-09T00:00:00Z</published>
    <updated>2024-10-09T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>OOP is certainly not my favorite paradigm, but I think mainstream statically-typed OOP does a few things right that are very important for programming with many people, over long periods of time.</p>
<p>In this post I want to explain what I think is the most important one of these things that the mainstream statically-typed OOP languages do well.</p>
<p>I will then compare the OOP code with Haskell, to try to make the point that OOP is not as bad in everything as some functional programmers seem to think.</p>
<h1 id="what-even-is-oop">What even is OOP?</h1>
<p>In this post I use the word “OOP” to mean programming in statically-typed language with:</p>
<ol type="1">
<li>Classes, that combine state and methods that can modify the state.</li>
<li>Inheritance, which allows classes to reuse state and methods of other classes.</li>
<li>Subtyping, where if a type <code>B</code> implements the public interface of type <code>A</code>, values of type <code>B</code> can be passed as <code>A</code>.</li>
<li>Virtual calls, where receiver class of a method call is not determined by the static type of the receiver but its runtime type.</li>
</ol>
<p>Examples of OO languages according to this definition: C++, Java, C#, Dart.</p>
<h1 id="an-example-of-what-this-allows">An example of what this allows</h1>
<p>This set of features allows a simple and convenient way of developing composable libraries, and extending the libraries with new functionality in a backwards compatible way.</p>
<p>It’s probably best explained with an example. Suppose we have a simple logger library:</p>
<pre><code>class Logger {
  // Private constructor: initializes state, returns an instance of `Logger`.
  Logger._();

  // Public factory: can return `Logger` or any of the subtypes.
  factory Logger() =&gt; Logger._();

  void log(String message, Severity severity) { /* ... */ }
}

enum Severity {
  Info,
  Error,
  Fatal,
}</code></pre>
<p>and another library that does some database stuff:</p>
<pre><code>class DatabaseHandle {
  /* ... */
}</code></pre>
<p>and an application that uses both:</p>
<pre><code>class MyApp {
  final Logger _logger;
  final DatabaseHandle _dbHandle;

  MyApp()
      : _logger = Logger(),
        _dbHandle = DatabaseHandle(...);
}</code></pre>
<p>As is usually the case, things that make network connections, change shared state etc. need to be mocked, faked, or stubbed to be able to test applications. We may also want to extend the libraries with new functionality. With the features that we have, we don’t have to see this coming and prepare the types based on this.</p>
<p>In the first iteration we might just add a concrete class that is just the copy of the current class, and make the current class abstract:</p>
<pre><code>// The class is now abstract.
abstract class Logger {
  // Public factory now returns an instance of a concrete subtype.
  factory Logger() =&gt; _SimpleLogger();

  Logger._();

  // `log` is now abstract.
  void log(String message, Severity severity);
}

class _SimpleLogger extends Logger {
  factory _SimpleLogger() =&gt; _SimpleLogger._();

  _SimpleLogger._() : super._() {/* ... */}

  @override
  void log(String message, Severity severity) {/* ... */}
}</code></pre>
<p>This change is backwards compatible, requires no changes in user code.</p>
<p>Now we might add more implementations, e.g. for ignoring log messages:</p>
<pre><code>abstract class Logger {
  factory Logger() =&gt; _SimpleLogger();

  // New.
  factory Logger.ignoring() =&gt; _IgnoringLogger();

  Logger._();

  void log(String message, Severity severity);
}

class _IgnoringLogger extends Logger {
  factory _IgnoringLogger() =&gt; _IgnoringLogger._();

  _IgnoringLogger._() : super._() {}

  @override
  void log(String message, Severity severity) {}
}</code></pre>
<p>Similarly we can add a logger that logs to a file, to a DB, etc.</p>
<p>We can do the same for the database handle class, but for mocking, faking, or stubbing, in tests.</p>
<p>To be able to use these new subtypes in our app, we implement a factory, or add a constructor to allow passing a logger and a db handle:</p>
<pre><code>class MyApp {
  final Logger _logger;
  final DatabaseHandle _dbHandle;

  MyApp()
      : _logger = Logger(),
        _dbHandle = DatabaseHandle();

  MyApp.withLoggerAndDb(this._logger, this._dbHandle);
}</code></pre>
<p>Note that we did not have to change any types, or add type parameters. Any methods of <code>MyApp</code> that use the <code>_logger</code> and <code>_dbHandle</code> fields do not have to know about the changes.</p>
<p>Now suppose one of the <code>DatabaseHandle</code> implementations also start using the logger library:</p>
<pre><code>abstract class DatabaseHandle {
  factory DatabaseHandle.withLogger(Logger logger) =&gt;
      _LoggingDatabaseHandle._(logger);

  factory DatabaseHandle() =&gt; _LoggingDatabaseHandle._(Logger.ignoring());

  DatabaseHandle._();

  /* ... */
}

class _LoggingDatabaseHandle extends DatabaseHandle {
  final Logger _logger;

  _LoggingDatabaseHandle._(this._logger) : super._();

  /* ... */
}</code></pre>
<p>In our app, we might test by disabling logging in the db library, but start logging db operations in production:</p>
<pre><code>class MyApp {
  // New
  MyApp.testingSetup()
      : _logger = Logger(),
        _dbHandle = DatabaseHandle.withLogger(Logger.ignoring());

  // Updated to start using the logging feature of the DB library.
  MyApp()
      : _logger = Logger(),
        _dbHandle = DatabaseHandle.withLogger(Logger.toFile(...));

  /* ... */
}</code></pre>
<p>As an example that adds more state to the types, we can add a logger implementation that only logs messages above certain severity:</p>
<pre><code>class _LogAboveSeverity extends _SimpleLogger {
  // Only logs messages with this severity or more severe.
  final Severity _severity;

  _LogAboveSeverity(this._severity) : super._();

  @override
  void log(String message, Severity severity) { /* ... */ }
}</code></pre>
<p>We can add another factory to the <code>Logger</code> abstract class that returns this type, or we can even implement this in another library:</p>
<pre><code>// Implemented in another library, not in `Logger`&#39;s library.
class LogAboveSeverity implements Logger {
  // Only logs messages with this severity or more severe.
  final Severity _severity;

  final Logger _logger;

  LogAboveSeverity(this._severity) : _logger = Logger();

  LogAboveSeverity.withLogger(this._severity, this._logger);

  @override
  void log(String message, Severity severity) { /* ... */ }
}</code></pre>
<p>As a final example to demonstrate adding more operations (rather than more state), we can have a logger that logs to a file, with a <code>flush</code> operation:</p>
<pre><code>class FileLogger implements Logger {
  final File _file;

  FileLogger(this._file);

  @override
  void log(String message, Severity severity) {/* ... */}

  void flush() {/* ... */}
}</code></pre>
<p>In summary:</p>
<ul>
<li>We started with a simple logging and database library and wrote an app.</li>
<li>We added more capabilities to the logging and database libraries for testing and also in production use. In particular, we added:
<ul>
<li>New functionality to the logger library, to disable logging, or logging to a file.</li>
<li>A new dependency to the database library for logging database operations. We also allowed the users to override the default logger used.</li>
</ul></li>
</ul>
<p>Crucially, we didn’t have to change any types while doing these changes, and the new code is still as type safe as before.</p>
<p>The logger and database libraries evolved in a completely backwards compatible way.</p>
<p>Since none of the types used in our application changed, <code>MyApp</code> methods didn’t have to change at all.</p>
<p>When we decide to take advantage of the new functionality, we updated only how we construct the logger and db handle instances in our app. Rest of the app didn’t change.</p>
<p>Now let’s consider how something like this could be done in Haskell.</p>
<h1 id="attempting-it-in-haskell">Attempting it in Haskell</h1>
<p>Immediately at the start, we have a few choices on how to represent it.</p>
<p><strong>Option 1:</strong> An ADT, with callback fields to be able to add different types of loggers later:</p>
<pre><code>data Logger = MkLogger
    { _log :: Message -&gt; Severity -&gt; IO ()
    }

simpleLogger :: IO Logger

data Severity = Info | Error | Fatal
    deriving (Eq, Ord)

log :: Logger -&gt; String -&gt; Severity -&gt; IO ()</code></pre>
<p>In this representation, extra state like the minimum severity level in our <code>_LogAboveSeverity</code> is not added to the type, but captured by the closures:</p>
<pre><code>logAboveSeverity :: Severity -&gt; IO Logger
logAboveSeverity minSeverity = MkLogger
    { _log = \message severity -&gt; if severity &gt;= minSeverity then ... else pure ()
    }</code></pre>
<p>If we need to update some of the state shared by the closures, the state needs to be stored in some kind of reference type like <code>IORef</code>.</p>
<p>Similar to the OOP code, the <code>FileLogger</code> needs to be a separate type:</p>
<pre><code>data FileLogger = MkFileLogger
  { _logger :: Logger   -- callbacks capture the file descriptor/buffer and write to it
  , _flush  :: IO ()    -- similarly captures the file descriptor/buffer, flushes it
  }

logFileLogger :: FileLogger -&gt; String -&gt; Severity -&gt; IO ()
logFileLogger = log . _logger</code></pre>
<p>However, unlike our OOP example, existing code that uses the <code>Logger</code> type and <code>log</code> function cannot work with this new type. There needs to be some refactoring, and how the user code will need to be refactored depends on how we want to expose this new type to the users.</p>
<p><strong>Option 2:</strong> A typeclass that we can implement for our concrete logger types:</p>
<pre><code>class Logger a where
    log :: a -&gt; String -&gt; Severity -&gt; IO ()

data SimpleLogger = MkSimpleLogger { ... }

simpleLogger :: IO SimpleLogger
simpleLogger = ...

instance Logger SimpleLogger where
  log = ...</code></pre>
<p>To allow backwards-compatible changes in the logger library, we need to hide the concrete logger class:</p>
<pre><code>module Logger
    ( Logger
    , simpleLogger -- I can export this without exporting its return type
    ) where

...</code></pre>
<p>With this module, we have to either add a type parameter to the functions and other types that use <code>Logger</code>, or use existentials.</p>
<p>Adding a type parameter is not a backwards compatible change, and in general it can cause snowball effect of propagating the type parameter to the direct users, and then their users, and so on, creating a massive change and difficult to use types.</p>
<p>The problem with existentials is that they are limited in how you can use them, and are somewhat strange in some areas. In our application we can do this:</p>
<pre><code>data MyApp = forall a . Logger a =&gt; MkMyApp
  { _logger :: a
  }</code></pre>
<p>But we can’t have a local variable with this existential type:</p>
<pre><code>createMyApp :: IO MyApp
createMyApp = do
  -- I can&#39;t add a type annotation to myLogger without the concrete type
  myLogger &lt;- simpleLogger      -- simpleLogger :: IO SimpleLogger
  return MkMyApp { _logger = myLogger }</code></pre>
<p>I also cannot have an existential type in a function argument:</p>
<pre><code>-- The type signature is accepted by the compiler, but the value cannot be used.
doStuffWithLogging :: (forall a . Logger a =&gt; a) -&gt; IO ()
doStuffWithLogging logger = log logger &quot;test&quot; Info -- some obscure type error</code></pre>
<p>Instead we have to “pack” the logger value with its typeclass dictionary in a new type:</p>
<pre><code>data LoggerBox = forall a . Logger a =&gt; LoggerBox a

doStuffWithLogging :: LoggerBox -&gt; IO ()
doStuffWithLogging (LoggerBox logger) = log logger &quot;test&quot; Info</code></pre>
<p>Other problems and limitations of this approach:</p>
<ul>
<li>The syntax is just awful to the point where it’s deterrent: <code>forall a . Logger a =&gt; ... a ...</code> instead of just <code>Logger</code>.</li>
<li>It allows implementing <code>FileLogger</code>, but
<ul>
<li>All subtypes need to be a new typeclass + an implementation (in OOP: just one class).</li>
<li>This cannot be used for safe downcasting of a <code>Logger</code> value to <code>FileLogger</code>, without knowing the concrete type of the <code>FileLogger</code>.</li>
</ul></li>
</ul>
<h1 id="effect-monad-approach">Effect monad approach</h1>
<p>The effect monad approach is a variation of option (2) without existentials. Instead of</p>
<pre><code>class Logger a where
    log :: a -&gt; String -&gt; Severity -&gt; IO ()</code></pre>
<p>We add the ability to log in a monad type parameter:</p>
<pre><code>class MonadLogger m where
    log :: String -&gt; Severity -&gt; m ()</code></pre>
<p>Then provide a “monad transformer” for each of the logger implementations:</p>
<pre><code>newtype SimpleLoggerT m a = SimpleLoggerT { runSimpleLoggerT :: m a }

instance MonadIO m =&gt; MonadLogger (SimpleLoggerT m) where
  log msg sev = SimpleLoggerT { runSimpleLoggerT = liftIO (logStdout msg sev) }

newtype FileLoggerT m a = FileLoggerT { runFileLoggerT :: Handle -&gt; m a }

instance MonadIO m =&gt; MonadLogger (FileLoggerT m) where
  log msg sev = FileLoggerT { runFileLoggerT = \handle -&gt; liftIO (logFile handle msg sev) }</code></pre>
<p>The database library does the same, and the app combines these together:</p>
<pre><code>newtype MyAppMonad a = ...

instance MonadLogger MyAppMonad where ...

instance MonadDb MyAppMonad where ...</code></pre>
<p>Because we have one type parameter that encapsulates all side effects (instead of one for logging, one for database operations), this avoids the issues with snowballed type parameters in the use sites.</p>
<p>The database library can also add a logger dependency without breaking the user code.</p>
<p>I think this is the best we can get in Haskell, and it’s quite similar to our OOP solution in terms of code changes needed to be done in the user code.</p>
<p>However for this to work the entire ecosystem of libraries need to do things this way. If the database library decides to use the ADT approach, we will need an “adapter”, e.g. a monad typeclass for the DB operations, with a concrete monad transformer type to call the DB library functions.</p>
<p>This is also the main problem with the composable effects libraries.</p>
<p>(There are also issues with how this kind of code performs in runtime, but that’s probably a topic for another blog post.)</p>
<h1 id="composable-effects">Composable effects</h1>
<p>Haskellers have been developing various ways of modelling side effects (such as DB operations, logging) as “effects” and various ways of composing them.</p>
<p>A simple and widespread way of doing this is via the effect monads, as we’ve seen in the previous section.</p>
<p>However these systems have a few drawbacks, compared to our OOP solution:</p>
<ul>
<li><p>Different effect libraries generally don’t work together. For example, <a href="https://hackage.haskell.org/package/mtl">mtl</a> and <a href="https://github.com/hasura/eff">eff</a> functions won’t work together without some kind of adapter turning one into the other.</p></li>
<li><p>Even if the entire Haskell ecosystem decides to use one particular effect system, things like using two different handlers for different parts of the program, such as the example of using different logger in the db library and the main app, requires type juggling. In some effect libraries this is not even possible.</p></li>
<li><p>Finally, note that the OOP code shown in this post are very basic and straightforward code that even a beginner in OOP can write. Any new person who joins the project, or any one time contributor who just wants to fix a bug and move on, will be able to work on either one of the libraries or the application code. It’s difficult to say the same with the composable effects libraries in Haskell.</p></li>
</ul>
<h1 id="conclusions">Conclusions</h1>
<p>Mainstream statically-typed OOP allows straightforward backwards compatible evolution of types, while keeping them easy to compose. I consider this to be one of the killer features of mainstream statically-typed OOP, and I believe it is an essential feature for programming with many people, over long periods of time.</p>
<p>Just like OOP, Haskell has design patterns, such as the effect monad pattern we’ve shown above. Some of these design patterns solve the problem nicely, but they need an entire ecosystem to follow the same pattern to be useful.</p>
<p>I think it would be beneficial for the functional programming community to stop dismissing OOP’s successes in the industry as an accident of history and try to understand what OOP does well.</p>
<hr />
<p>Thanks to Chris Penner and Matthías Páll Gissurarson for reviewing a draft of this blog post.</p>]]></summary>
</entry>
<entry>
    <title>My thoughts on OCaml</title>
    <link href="http://osa1.net/posts/2023-04-24-ocaml-thoughts.html" />
    <id>http://osa1.net/posts/2023-04-24-ocaml-thoughts.html</id>
    <published>2023-04-24T00:00:00Z</published>
    <updated>2023-04-24T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>Since 2013 I’ve had the chance to use OCaml a few times in different jobs, and I got frustrated and disappointed every time I had to use it. I just don’t enjoy writing OCaml.</p>
<p>In this post I want to summarize some of the reasons why I don’t like OCaml and why I wouldn’t choose it for a new project today.</p>
<h1 id="no-standard-and-easy-way-of-implementing-interfaces">No standard and easy way of implementing interfaces</h1>
<p>To me it’s absolutely essential that the language should have some way of defining interfaces, implementing those interfaces for the types, and programming against those interfaces.</p>
<p>In Haskell, this is done with typeclasses. Rust has a similar mechanism called traits. In languages with classes this is often done with abstract classes and “implementing” those classes in new classes (e.g. <code>implements</code> in Dart).</p>
<p>In OCaml there’s no way to do this. I have to explicitly pass functions along with my values, maybe in a product type, or with a functor, or as an argument.</p>
<p>Regardless of how I work around this limitation, it’s extremely inconvenient. Things that must be trivial in any code base, such as converting a value to a string for debugging purposes, become a chore, and sometimes even impossible.</p>
<p>As far as I know, there was at least one attempt at ameliorating this with modular implicits (implicit parameter passing), but I don’t know what happened to it since 2017. It looks like it’s still not a part of the language and the standard library is not using it.</p>
<h1 id="bad-standard-library">Bad standard library</h1>
<p>OCaml’s standard library is just bizarre. It has lots of small issues, and a few larger ones. It’s really just extremely painful to use.</p>
<p>Some examples of the issues:</p>
<ul>
<li><p>Zoo of printing/debugging and conversion functions such as <code>string_of_int</code>, <code>string_of_float</code>, <code>print_char</code>, <code>Int64.of_int</code>, <code>string_of_int</code>, …</p></li>
<li><p>Overly polymorphic operators with type <code>'a -&gt; 'a -&gt; bool</code> such as <code>=</code> (called “structural equality”, throws an exception if you pass a function) and <code>&gt;</code>. Code that uses these operators will probably not work on user-defined types as expected.</p></li>
<li><p>Standard types are sometimes persistent, sometimes mutable. <code>List</code>, <code>Map</code>, and <code>Set</code> are persistent. <code>Stack</code> and <code>Hashtbl</code> are mutable.</p></li>
<li><p>Inconsistent naming:</p>
<ul>
<li>Length function for <code>Map</code> is <code>cardinal</code>, length function for <code>Hashtbl</code> is <code>length</code>.</li>
<li>The “bytes” type is <code>Bytes.t</code>, the big int type is <code>Big_int.big_int</code> (instead of <code>Big_int.t</code>). The functions in these modules are also inconsistently named. <code>Big_int</code> functions are suffixed with <code>_big_int</code>, <code>Bytes</code> module functions are not prefixed or suffixed.</li>
</ul></li>
<li><p>The regex module uses global state: <code>string_match</code> runs a regex and sets some global state. <code>matched_string</code> returns the last matched string using the global state.</p></li>
<li><p>Lack of widely used operations such as <code>popcount</code> for integer types, unicode character operations.</p></li>
<li><p>It doesn’t have proper string and character types: <code>String</code> is a byte array, <code>char</code> is a byte.</p></li>
</ul>
<p>The bad state of OCaml’s standard library also causes fragmentation in the ecosystem with two competing alternatives: <a href="https://opensource.janestreet.com/core/">Core</a> and <a href="https://github.com/ocaml-batteries-team/batteries-included">Batteries</a>.</p>
<h1 id="syntax-problems">Syntax problems</h1>
<p>OCaml doesn’t have a single-line comment syntax.</p>
<p><a href="https://v2.ocaml.org/manual/expr.html">The expression syntax</a> has just too many issues. It’s inconsistent in how it uses delimiters. <code>for</code> and <code>while</code> end with <code>end</code>, but <code>let</code>, <code>if</code>, <code>match</code>, and <code>try</code> don’t, even though the right-most non-terminal is the same in all of these productions:</p>
<pre><code>expr ::= ...
      | while &lt;expr&gt; do &lt;expr&gt; done
      | for &lt;value-name&gt; = &lt;expr&gt; ( to | downto ) &lt;expr&gt; do &lt;expr&gt; done
      | let &lt;let-binding&gt; in &lt;expr&gt;
      | if &lt;expr&gt; then &lt;expr&gt; [ else &lt;expr&gt; ]
      | match &lt;expr&gt; with (| &lt;pattern&gt; [ when &lt;expr&gt; ] -&gt; &lt;expr&gt;)+
      | try &lt;expr&gt; with (| &lt;pattern&gt; [ when &lt;expr&gt; ] -&gt; &lt;expr&gt;)+
      ...</code></pre>
<p>It has <code>for</code> and <code>while</code>, but no <code>break</code> and <code>continue</code>. So you use exceptions with a <code>try</code> inside the loop for <code>continue</code>, and outside for <code>break</code>.</p>
<p>It also has lots of ambiguities, and some of these ambiguities are resolved in an unintuitive way. In addition to making OCaml <a href="https://github.com/ocaml/ocaml/blob/063894d3fa8f63fedf6959744510e1635dccb3ca/parsing/parser.mly#L798-L837">difficult to parse correctly</a>, this can actually cause incorrect reading of the code.</p>
<p>Most common example is probably nesting <code>match</code> and <code>try</code> expressions:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true"></a><span class="kw">match</span> e0 <span class="kw">with</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true"></a>| p1 -&gt; <span class="kw">try</span> e1 <span class="kw">with</span> p2 -&gt; e2</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true"></a>| p3 -&gt; e3</span></code></pre></div>
<p>Here <code>p3 -&gt; e3</code> is a part of the <code>try</code> expression.</p>
<p>Another example is the sequencing syntax <code>&lt;expr&gt; ; &lt;expr&gt;</code> and productions with <code>&lt;expr&gt;</code> as the right-most symbol:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true"></a><span class="kw">let</span> test1 b =</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true"></a>  <span class="kw">if</span> b <span class="kw">then</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true"></a>    <span class="dt">print_string</span> <span class="st">&quot;1&quot;</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true"></a>  <span class="kw">else</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true"></a>    <span class="dt">print_string</span> <span class="st">&quot;2&quot;</span>; <span class="dt">print_string</span> <span class="st">&quot;3&quot;</span></span></code></pre></div>
<p>Here <code>print_string "3"</code> is not a part of the <code>if</code> expression, so this function always prints “3”.</p>
<p>However, even though <code>match</code> also has <code>&lt;expr&gt;</code> as the right-most symbol, it has different precedence in comparison to semicolon:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true"></a><span class="kw">let</span> test2 b =</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true"></a>  <span class="kw">match</span> b <span class="kw">with</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true"></a>  | <span class="kw">true</span> -&gt; <span class="dt">print_string</span> <span class="st">&quot;1&quot;</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true"></a>  | <span class="kw">false</span> -&gt; <span class="dt">print_string</span> <span class="st">&quot;2&quot;</span>; <span class="dt">print_string</span> <span class="st">&quot;3&quot;</span></span></code></pre></div>
<p>Here <code>print_string "3"</code> is a part of the <code>false -&gt; ...</code> branch.</p>
<p>Try to guess how these functions are parsed:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true"></a><span class="co">(* Is the last print part of `else` or not? *)</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true"></a><span class="kw">let</span> test3 b =</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true"></a>  <span class="kw">if</span> b <span class="kw">then</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true"></a>    <span class="dt">print_string</span> <span class="st">&quot;1&quot;</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true"></a>  <span class="kw">else</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true"></a>    <span class="kw">let</span> x = <span class="st">&quot;2&quot;</span> <span class="kw">in</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true"></a>    <span class="dt">print_string</span> x;</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true"></a>    <span class="dt">print_string</span> <span class="st">&quot;3&quot;</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true"></a><span class="co">(* Is this well-typed? *)</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true"></a><span class="kw">let</span> test4 b =</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true"></a>  <span class="kw">if</span> b <span class="kw">then</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true"></a>    <span class="dv">1</span>, <span class="dv">2</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true"></a>  <span class="kw">else</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true"></a>    <span class="dv">3</span>, <span class="dv">4</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true"></a></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true"></a><span class="co">(* Is the type of this `(int * int) array -&gt; unit` or `int array -&gt; unit * int`? *)</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true"></a><span class="kw">let</span> test5 a = a.(<span class="dv">0</span>) &lt;- <span class="dv">1</span>, <span class="dv">2</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true"></a></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true"></a><span class="co">(* What if I replace `,` with `;`? Does this set the element 1 or 2? *)</span></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true"></a><span class="kw">let</span> test6 a = a.(<span class="dv">0</span>) &lt;- <span class="dv">1</span>; <span class="dv">2</span></span></code></pre></div>
<p>When writing OCaml you have to keep these rules in mind.</p>
<p>It also has <a href="https://en.wikipedia.org/wiki/Dangling_else">the “dangling else” problem</a>:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true"></a><span class="co">(* Is `else` part of the inner `if` or the outer? *)</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true"></a><span class="kw">if</span> e1 <span class="kw">then</span> <span class="kw">if</span> e2 <span class="kw">then</span> e3 <span class="kw">else</span> e4</span></code></pre></div>
<p>Finally, and I think this is probably the most strange thing about OCaml’s syntax and I’m not even sure what’s exactly happening here (I can’t find anything relevant in the language documentation), comments in OCaml are somehow tokenized and those tokens need to be terminated. They can be terminated inside another comment, or even outside. This is a bit difficult to explain but here’s a simple example:</p>
<pre><code>(* &quot; *)
print_string &quot;hi&quot;</code></pre>
<p>OCaml 5.0.0 rejects this program with this error:</p>
<pre><code>File &quot;./test.ml&quot;, line 2, characters 16-17:
2 | print_string &quot;hi&quot;
                    ^
  String literal begins here</code></pre>
<p>From the error message it seems like the <code>"</code> in the comment line actually starts a string literal, which is terminated in the first quote of <code>"hi"</code>. The closing double quote of <code>"hi"</code> thus starts another string literal, which is not terminated.</p>
<p>However that doesn’t explain why this works:</p>
<pre><code>(* &quot; *)
print_string &quot;hi&quot;
(* &quot; *)
print_string &quot;bye&quot;</code></pre>
<p>If my explanation of the previous version were correct this would fail with an unbound <code>hi</code> variable, but it works and prints “bye”!</p>
<h1 id="rest-of-the-package-is-also-not-that-good">Rest of the package is also not that good</h1>
<p>I’m not following developments in OCaml ecosystem too closely, but just two years ago it was common to use Makefiles to build OCaml projects. The language server barely worked on a project with less than 50 kloc. There was no standard way of doing compile-time metaprogramming and some projects even used the C preprocessor (cpp).</p>
<p>Some of these things probably improved in the meantime, but the overall package is still not good enough compared to the alternatives.</p>
<h1 id="but-at-least-its-a-functional-language">But at least it’s a functional language?</h1>
<p>Almost all modern statically typed languages have closures, higher-order functions/methods, lazy streams, and combinators that run efficiently. Persistent/immutable data structures can be implemented even in C.</p>
<p>Also, OCaml has no tracking of side-effects (like in Haskell), and the language and the standard library have lots of features and functions with mutation, such as the array update syntax, mutable record fields, <code>Hashtbl</code>, and the regex module.</p>
<p>The only thing that makes OCaml more “functional” than e.g. Dart, Java, or Rust is that it supports tail calls. While having tail calls is important for functional programming, I would happily give up on tail calls if that means not having the problems listed above.</p>
<p>Also keep in mind that when you mix imperative and functional styles tail calls become less important. For example, I don’t have to implement a stream <code>map</code> function in Dart with a tail call to map the rest of the stream, I can just use a <code>while</code> or <code>for</code> loop.</p>
<h1 id="when-should-i-use-it">When should I use it?</h1>
<p>In my opinion there is no reason to use OCaml in a new project in 2023. If you have a reason to think that OCaml is the best choice for a new project please let me know your use case, I’m genuinely curious.</p>]]></summary>
</entry>
<entry>
    <title>Fast polymorphic record access</title>
    <link href="http://osa1.net/posts/2023-01-23-fast-polymorphic-record-access.html" />
    <id>http://osa1.net/posts/2023-01-23-fast-polymorphic-record-access.html</id>
    <published>2023-01-23T00:00:00Z</published>
    <updated>2023-01-23T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>I like <a href="https://osa1.net/posts/2021-04-10-sums-and-products.html">anonymous records</a> and row polymorphism, but until recently I didn’t know how to generate efficient code for polymorphic record access. In this blog post I will summarize the different compilations of polymorphic record accesses that I’m aware of.</p>
<p>All of the ideas shown in this post can be used to access a record field when the record’s concrete type is not known, but the type system guarantees that it has the accessed field. This includes row polymorphism and record subtyping.</p>
<p>Most of the ideas also work when the record’s type is completely unknown and it may not have the accessed field, but some of the optimizations assume accesses cannot fail. Those optimizations can only be used on statically-typed but polymorphic records.</p>
<p>In some of the examples below I will use row polymorphism.</p>
<hr />
<h1 id="row-polymorphism-and-record-subtyping-briefly">Row polymorphism and record subtyping, briefly</h1>
<p>In this blog post we are interested in a specific application of row polymorphism to records. In short, row polymorphism allows type variables denoting sets of record fields, with their types. For example:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true"></a>f <span class="op">:</span> <span class="ot">∀</span> r <span class="op">.</span> { x <span class="op">:</span> <span class="dt">Int</span>, y <span class="op">:</span> <span class="dt">Int</span> <span class="op">|</span> r } <span class="ot">-&gt;</span> <span class="dt">Int</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true"></a>f a <span class="ot">=</span> a<span class="op">.</span>x <span class="op">+</span> a<span class="op">.</span>y</span></code></pre></div>
<p>Here the type variable <code>r</code> ranges over set of rows (or records). This function accepts any record as argument as long as the record has at least <code>x : Int</code> and <code>y : Int</code> fields.</p>
<p>The main difference between row polymorphism and record subtyping is that the type variable <code>r</code> can be used in the right-hand side of an arrow as well, allowing passing the record around without losing its concrete type. For example:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true"></a>mapAB <span class="op">:</span> <span class="ot">∀</span> r <span class="op">.</span> { a <span class="op">:</span> <span class="dt">Int</span>, b <span class="op">:</span> <span class="dt">Int</span> <span class="op">|</span> r } <span class="ot">-&gt;</span> (<span class="dt">Int</span> <span class="ot">-&gt;</span> <span class="dt">Int</span>) <span class="ot">-&gt;</span> { a <span class="op">:</span> <span class="dt">Int</span>, b <span class="op">:</span> <span class="dt">Int</span> <span class="op">|</span> r }</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true"></a>mapAB r f <span class="ot">=</span> { a <span class="ot">=</span> f r<span class="op">.</span>a, b <span class="ot">=</span> f r<span class="op">.</span>b, <span class="op">..</span> r }</span></code></pre></div>
<p>This function takes any record that has <code>a : Int</code> and <code>b : Int</code> fields, and returns a new record with updated <code>a</code> and <code>b</code> fields and the rest of the fields. If I pass it a record with type <code>{ a : Int, b : Int, name : String }</code> I get the same type back.</p>
<p>With subtyping, type of this function would look like:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true"></a>mapAB <span class="op">:</span> { a <span class="op">:</span> <span class="dt">Int</span>, b <span class="op">:</span> <span class="dt">Int</span> } <span class="ot">-&gt;</span> (<span class="dt">Int</span> <span class="ot">-&gt;</span> <span class="dt">Int</span>) <span class="ot">-&gt;</span> { a <span class="op">:</span> <span class="dt">Int</span>, b <span class="op">:</span> <span class="dt">Int</span> }</span></code></pre></div>
<p>In this version the return type just has <code>a</code> and <code>b</code> fields. Rest of the fields are lost. If I pass this a <code>{ a : Int, b : Int, name : String }</code> I get <code>{ a : Int, b : Int }</code> back. The <code>name</code> field is lost.</p>
<hr />
<p>Without subtyping, when the record type in a field access expression is known, it’s easy to generate efficient code: we use the same offsets used when compiling a record literal with the type.</p>
<p>With subtyping, and with row-polymorphism when the record type is not a concrete record type but is a record type with a row variable, type of <code>r</code> in <code>r.a</code> does not immediately give us where in the record’s payload the field <code>a</code> is.</p>
<p>Let’s look at how we might go about implementing record field access in these cases.</p>
<h1 id="records-as-maps">(0) Records as maps</h1>
<p>I don’t think this idea is used in statically-typed languages, but I wanted to include it for completeness.</p>
<p>We can implement records as maps with string keys. Field access then becomes a map lookup.</p>
<p>This is easy to implement because our language probably already has a map implementation in the standard library.</p>
<p>The disadvantages are:</p>
<ul>
<li><p>Depending on the map implementation, every field access require a <code>O(N)</code> or <code>O(log(N))</code> map lookup.</p></li>
<li><p>Map entries will be stored in a separate memory location (instead of in the record object’s payload), which will require pointer chasing to read the field value.</p></li>
<li><p>Unnecessary memory overhead caused by map fields that are not really necessary for records: such as the <code>capacity</code> and <code>size</code> fields.</p></li>
</ul>
<p>With whole-program compilation, we can improve the constant factors a bit by mapping labels (field names) in the program to unique integers. This way lookups don’t require string hashing or comparison, but this is still slow and memory-inefficient compared to other techniques we will discuss below.</p>
<h1 id="passing-accessors-as-parameters">(1) Passing accessors as parameters</h1>
<p>If you’re familiar with Haskell, this is the Haskell way of implementing row polymorphic records.</p>
<p>The idea is that when we pass a record to a row-polymorphic function, we also pass, implicitly, and as functions, the accessors that the function needs.</p>
<p>In Haskell, type of <code>mapAB</code> we’ve seen above would look like this:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true"></a>mapAB <span class="op">:</span> <span class="ot">∀</span> r <span class="op">.</span> (<span class="dt">HasField</span> r <span class="dt">&#39;A</span> <span class="dt">Int</span>, <span class="dt">HasField</span> r <span class="dt">&#39;B</span> <span class="dt">Int</span>) <span class="ot">=&gt;</span> <span class="dt">Record</span> r <span class="ot">-&gt;</span> (<span class="dt">Int</span> <span class="ot">-&gt;</span> <span class="dt">Int</span>) <span class="ot">-&gt;</span> <span class="dt">Record</span> r</span></code></pre></div>
<p>The runtime values for <code>HasField ...</code> constraints are the accessors. When calling this function we don’t explicitly pass these accessors, the compiler generates them. In a well-typed program, we either have these values in the call site, or we know how to generate them (e.g. the record type is concrete in the call site), so it’s possible for the compiler to generate and pass these arguments.</p>
<p>The main advantage of this approach is that it doesn’t require any language support specifically for records.</p>
<p>The main disadvantages are:</p>
<ul>
<li><p>Every field access is a function call.</p></li>
<li><p>Parameter passing per field per record does not scale well and causes messy and slow generated code. For example, suppose we want to take two records with fields <code>x : Int</code> and <code>y : Int</code>:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true"></a>f <span class="op">:</span> <span class="ot">∀</span> r <span class="op">.</span> (<span class="dt">HasField</span> r <span class="dt">&#39;X</span> <span class="dt">Int</span>, <span class="dt">HasField</span> r <span class="dt">&#39;Y</span> <span class="dt">Int</span>) <span class="ot">=&gt;</span> <span class="dt">Record</span> r <span class="ot">-&gt;</span> <span class="dt">Record</span> r <span class="ot">-&gt;</span> <span class="op">...</span></span></code></pre></div>
<p>This function takes two implicit arguments, but it has a limitation that the record arguments need to have the same record types. I can’t call this function with two different records:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true"></a>f { x <span class="ot">=</span> <span class="dv">123</span>, y <span class="ot">=</span> <span class="dv">456</span>, a <span class="ot">=</span> <span class="st">&quot;hi&quot;</span> } { x <span class="ot">=</span> <span class="dv">0</span>, y <span class="ot">=</span> <span class="op">-</span><span class="dv">1</span>, b <span class="ot">=</span> false }</span></code></pre></div>
<p>For this to work I need two row variables:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true"></a>f <span class="op">:</span> <span class="ot">∀</span> r1 r2 <span class="op">.</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true"></a>    (<span class="dt">HasField</span> r1 <span class="dt">&#39;X</span> <span class="dt">Int</span>, <span class="dt">HasField</span> r1 <span class="dt">&#39;Y</span> <span class="dt">Int</span>,</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true"></a>     <span class="dt">HasField</span> r2 <span class="dt">&#39;X</span> <span class="dt">Int</span>, <span class="dt">HasField</span> r2 <span class="dt">&#39;Y</span> <span class="dt">Int</span>) <span class="ot">=&gt;</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true"></a>    <span class="dt">Record</span> r1 <span class="ot">-&gt;</span> <span class="dt">Record</span> r2 <span class="ot">-&gt;</span> <span class="op">...</span></span></code></pre></div>
<p>This version works, but it also takes 4 implicit arguments.</p></li>
</ul>
<h1 id="prerequisite-integers-for-labels">Prerequisite: integers for labels</h1>
<p>Starting with the next approach, we will require mapping labels (field names) to integers in compile-time, to be used as indices.</p>
<p>Because these integers for labels will be used in record allocation and field accesses, it is possible that a label we see later in a program will cause different code generation for a record field access that we’ve already seen.</p>
<p>We have two options:</p>
<ul>
<li><p>We can avoid this problem with a whole-program pass to collect all labels in the program.</p>
<p>This is trivial with a whole-program compiler as a front-end pass can store all labels seen in a component (library, module) somewhere and we can map those labels to integers before code generation.</p></li>
<li><p>We can have a link-time step to update record allocation and field access code with the integers for the labels.</p></li>
</ul>
<p>In the rest of the post, labels will always get integers based on their lexicographical order and we will call these integers for labels just “labels”.</p>
<p>For example, if I have labels <code>a</code>, <code>c</code>, <code>b</code>, <code>d</code> in my program, their numbers will be 1, 3, 2, 4, respectively.</p>
<h1 id="per-record-label-to-field-offset-tables">(2) Per-record label-to-field-offset tables</h1>
<p>With integers as labels we can add a table to every record (records with the same set of keys sharing the same table) mapping labels in the program to offsets in the record’s payload. For example, the table for a record with fields <code>a</code> and <code>c</code> when the program has labels <code>a</code>, <code>b</code>, <code>c</code>, <code>d</code>, looks like this:</p>
<pre><code>[ 0, _, 1, _ ]</code></pre>
<p>This table is indexed by the label and the value gives the offset in the record’s payload for the field. <code>_</code> means the record does not have the field. In a well-typed program we won’t ever see a <code>_</code> value being read from a table.</p>
<p>This approach is quite wasteful as every table will have as many entries as number of labels in the program, but we will compress these tables below to reasonable sizes.</p>
<p>We will call these tables “record offset tables” or “offset tables” in short. When compiling a record access we need to get the record’s offset table. For this we add an extra word (pointer) to record objects pointing to their offset tables. We then generate this code for a record field access:</p>
<pre><code>record[record[OFFSET_TABLE_INDEX][label]]</code></pre>
<p><code>OFFSET_TABLE_INDEX</code> is the constant for where the offset table pointer is in record objects.</p>
<p>Offset tables are generated per record shape (set of labels), so the total number of tables shouldn’t be too large.</p>
<p>Since the <code>_</code> entries won’t ever be used, we can shrink the tables with trailing <code>_</code> entries. In our example above with a record with <code>a</code> and <code>c</code> fields, the last <code>_</code> entry can be omitted:</p>
<pre><code>[ 0, _, 1 ]</code></pre>
<h1 id="making-the-tables-global">(2.1) Making the tables global</h1>
<p>Because offset tables are per-shape, and the total number of record shapes in a program should be small, if we allocate a few bits in record object headers for the “shape index” of the record, this index can be used to index a global table mapping record shapes to their offset tables.</p>
<p>Generated code for record access expressions will look like:</p>
<pre><code>record[RECORD_OFFSET_TABLES[getRecordShapeId(record)][label]]</code></pre>
<p><code>getRecordShapeId</code> will read the bits in the object header for the record shape id. Depending on the actual header layout, it will look something like:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode c"><code class="sourceCode c"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true"></a><span class="dt">int</span> getRecordShapeId(Object* object) {</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true"></a>  <span class="cf">return</span> (object-&gt;header &amp; RECORD_ID_MASK) &gt;&gt; HEADER_BITS;</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true"></a>}</span></code></pre></div>
<p>With record shape IDs in headers and a global table mapping shape IDs to offset tables, we no longer need an extra word in record objects for the offset table pointer.</p>
<p>Here’s an example of offset tables when we have labels <code>a</code>, <code>b</code>, <code>x</code>, <code>y</code>, and two records <code>0: {a, b}</code> and <code>1: {x, y}</code>:</p>
<pre><code>RECORD_0_OFFSET_TABLE = [
  0, // label a
  1, // label b
  _, // label x
  _, // label y
];

RECORD_1_OFFSET_TABLE = [
  _, // label a
  _, // label b
  0, // label x
  1, // label y
];

RECORD_OFFSET_TABLES = [
  RECORD_0_OFFSET_TABLE, // record 0
  RECORD_1_OFFSET_TABLE, // record 1
];</code></pre>
<p>As before, the offset table for record 0 can be shrunk as:</p>
<pre><code>RECORD_0_OFFSET_TABLE = [
  0, // label a
  1, // label b
];</code></pre>
<h1 id="sharing-label-ids-and-record-shapes">(2.2) Sharing label IDs and record shapes</h1>
<p>Labels that are not used in the same record program can be given the same ID.</p>
<p>In the example above, this allows us to have a single table for both records:</p>
<pre><code>RECORD_0_1_OFFSET_TABLE = [
  0, // label a or x
  1, // label b or y
];

RECORD_OFFSET_TABLES = [
  RECORD_0_1_OFFSET_TABLE, // record 0
  RECORD_0_1_OFFSET_TABLE, // record 1
];</code></pre>
<p>The problem of assigning IDs to labels is very similar to stack allocation when spilling during register allocation. We have practically infinite amount of IDs (stack space), but we want to reuse the same ID for labels as long as they’re never used in the same record (live at the same time).</p>
<p>After sharing label IDs, some of the shapes may be identical, as in our example. We can give those shapes the same ID and avoid redundant entries in the offset tables.</p>
<p>With this, our example with two records <code>{a, b}</code> and <code>{x, y}</code> compiles to just one offset table:</p>
<pre><code>RECORD_0_1_OFFSET_TABLE = [
  0, // label a or x
  1, // label b or y
];

RECORD_OFFSET_TABLES = [
  RECORD_0_1_OFFSET_TABLE, // record 0 and 1
];</code></pre>
<h1 id="flattening-the-table">(2.3) Flattening the table</h1>
<p>Suppose we have these record shapes in a program:</p>
<ul>
<li><code>{a, b, q}</code></li>
<li><code>{x, y, q}</code></li>
</ul>
<p>The <code>RECORD_OFFSET_TABLES</code> table is currently an array of pointers, and indexing the offset table still requires pointer chasing.</p>
<p>To avoid pointer chasing we can flatten the table.</p>
<p>For our current program, the tables, without flattening, look like this:</p>
<pre><code>RECORD_0_OFFSET_TABLE = [
  0, // label a
  1, // label b
  _, // label x
  _, // label y
  2, // label q
];

RECORD_1_OFFSET_TABLE = [
  _, // label a
  _, // label b
  0, // label x
  1, // label y
  2, // label q
];

RECORD_OFFSET_TABLES = [
  RECORD_0_OFFSET_TABLE,
  RECORD_1_OFFSET_TABLE,
];</code></pre>
<p>We can flatten this as:</p>
<pre><code>RECORD_0_OFFSET_TABLE = [
  0, // label a
  1, // label b
  _, // label x
  _, // label y
  2, // label q
];

RECORD_1_OFFSET_TABLE = [
  _, // label a
  _, // label b
  0, // label x
  1, // label y
  2, // label q
];

RECORD_LABEL_OFFSETS = [
  0, // record 0, label a
  1, // record 0, label b
  _, // record 0, label x
  _, // record 0, label y
  2, // record 0, label z

  _, // record 1, label a
  _, // record 1, label b
  0, // record 1, label x
  1, // record 1, label y
  2, // record 1, label z
];</code></pre>
<p>Field indexing then becomes:</p>
<pre><code>record[RECORD_LABEL_OFFSETS[(getRecordShapeId(record) * NUM_LABELS) + label]]</code></pre>
<p>With this version we eliminate one layer of indirection.</p>
<h1 id="removing-the-constant-factor">(2.4) Removing the constant factor</h1>
<p>The idea here is not too important on its own, but it will enable further improvements.</p>
<p>The <code>NUM_LABELS</code> factor in field access code above can be eliminated by incrementing record shape IDs by <code>NUM_LABELS</code> instead of 1. In our example, instead of having record IDs 0 and 1, we will have 0 and 5 (incremented by the number of labels in the program).</p>
<p>Since there may be large number of labels in a program and we may have only a few bits to store the record IDs, an alternative would be to convert the table to label-major order like this:</p>
<pre><code>RECORD_LABEL_OFFSETS = [
  0, // label a, record 0
  _, // label a, record 1

  1, // label b, record 0
  _, // label b, record 1

  _, // label x, record 0
  1, // label x, record 1

  _, // label y, record 0
  2, // label y, record 1

  3, // label z, record 0
  3, // label z, record 1
];</code></pre>
<p>With this table, indexing code becomes:</p>
<pre><code>record[RECORD_LABEL_OFFSETS[(label * NUM_RECORDS) + getRecordShapeId(record)]]</code></pre>
<p>We can then eliminate the <code>NUM_RECORDS</code> factor the same way, by incrementing label IDs by <code>NUM_RECORDS</code> instead of 1, and index with:</p>
<pre><code>record[RECORD_LABEL_OFFSETS[label + getRecordShapeId(record)]]</code></pre>
<h1 id="compacting-the-table-further">(2.5) Compacting the table further</h1>
<p>Now that the table index of a label is <code>label + shape_id</code> and we have a single table, we can shift the entries in the table by decrementing label IDs.</p>
<p>For this it doesn’t matter whether we store in label-major or record-major order. Which one of these will generate a smaller table will probably depend on the program. As an example, suppose we store the table in label-major order, and we have these records in the program:</p>
<ul>
<li><code>0: {x, y, z, t}</code></li>
<li><code>1: {x, y}</code></li>
<li><code>2: {z, t}</code></li>
</ul>
<p>The table will look like:</p>
<pre><code>[ 0, 0, _,   // label x
  1, 1, _,   // label y
  2, _, 0,   // label z
  3, _, 1 ]  // label t</code></pre>
<p>Record IDs will be 0, 1, 2, and label IDs will be 0, 3, 6, 9.</p>
<p>We can use the unused slot for label x, record 2, by decrementing the label index for <code>y</code> by one. If we then do the same for <code>z</code>, the label IDs become 0, 2, 4, 7, and the table becomes:</p>
<pre><code>[ 0, 0,      // label x
  1, 1,      // label y
  2, _, 0,   // label z
  3, _, 1 ]  // label t</code></pre>
<p>This idea can be used to fill any gaps in previous label rows, as long as the used slots in a row fits into the gaps. For example, if we have a table like:</p>
<pre><code>[ 0, _, _, 1,  // label x
  _, 0, 1, _,  // label y
  ... ]</code></pre>
<p>We can decrement <code>y</code>’s ID to fit it into the row for label <code>x</code>:</p>
<pre><code>[ 0, 0, 1, 1,  // label x and y, interleaved
  ... ]</code></pre>
<h1 id="conclusions">Conclusions</h1>
<p>Collecting and numbering all labels in the program allows using a global table for mapping labels to offsets.</p>
<p>These offset tables can be made smaller by</p>
<ul>
<li>Giving same number to labels that don’t occur in the same record</li>
<li>Giving same ID to records that become identical after the previous step</li>
<li>Tweaking label numbers so that rows without overlapping entries can be merged into a single row</li>
</ul>
<p>The result is a very compact representation of record objects (no extra words in the header or unused space in the payload needed) and a fast polymorphic field access.</p>
<p>The offset table should also be small in practice, because different parts of the program will probably use disjoint set of names, and different labels and records will have the same IDs. In the remaining cases, tweaking label IDs to compact the table should help.</p>
<h1 id="references">References</h1>
<p>I’ve learned about the global table approach and some of the optimizations from the Dart compiler, which implements virtual calls using a “global dispatch table” (GDT), indexed by <code>classID + methodID</code> in call sites. See <a href="https://mrale.ph/dartvm/#global-dispatch-table-gdt">“Introduction to Dart VM”</a> for a description of how Dart AOT and JIT generate GDTs.</p>
<p>If you are interested in seeing some code, <a href="https://github.com/dart-lang/sdk/blob/ba8f0bd947c613013ed4659ea44da851bf35a99f/pkg/dart2wasm/lib/dispatch_table.dart#L411-L442">here</a> is where we generate the GDT in dart2wasm (Dart’s Wasm backend). The outer loop finds a selector ID (label ID in our examples) for a row (list of records in our examples, list of classes in dart2wasm). The inner loop <code>do { ... } while (!fits)</code> starts from the first row with gaps, and tries to fit the current row into the gaps. In the worst case it skips all of the rows, in which case rest of the code appends the table with the new row.</p>
<p><a href="https://github.com/dart-lang/language/blob/master/accepted/future-releases/records/records-feature-specification.md">Dart will soon have records</a>, and for the <a href="https://github.com/dart-lang/sdk/issues/50014">dart2wasm implementation of records</a> I’m thinking of using some of the ideas described in this post. Dart records do not support width subtyping (you can’t pass <code>{x, y, z}</code> where <code>{x, y}</code> is expected), but because of the <code>dynamic</code> type, we can have a dynamically typed record that we index.</p>
<hr />
<p>Thanks to <a href="https://twitter.com/josecalderon">José Manuel Calderón Trilla</a> for his feedback on a draft of this blog post.</p>]]></summary>
</entry>
<entry>
    <title>Products and sums, named and anonymous</title>
    <link href="http://osa1.net/posts/2021-04-10-sums-and-products.html" />
    <id>http://osa1.net/posts/2021-04-10-sums-and-products.html</id>
    <published>2021-04-10T00:00:00Z</published>
    <updated>2021-04-10T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>I was recently thinking about why do so many languages have tuples, which can be thought of as simple anonymous products (more on the definition of this below), but not something similar for sums. Both sum and product types are widely used, so it seems inconsistent to have anonymous products but not sums.</p>
<p>I recently <a href="https://twitter.com/_osa1/status/1379260986574667776">tweeted about this</a> and got helpful responses that made me realize that I got my definitions wrong. As I think more about what “anonymous type” means it became clear to me that the it’s not just tuples or other types with special syntax, instead of names. It’s more complicated than that.</p>
<p>So in this post I’d like to briefly talk about products and sums, and how are names used in type checking. I will then show a different way of type checking, and some examples from two widely used languages. Finally, I will argue that types are called “named” or “anonymous” depending on how they are checked.</p>
<p>Note that I’m not using any of these words as they are used in category theory or any other field of mathematics. These are mainly how I see them used in widely used PLs like Haskell, Rust, and OCaml, and in PL papers and books.</p>
<h1 id="products">Products</h1>
<p>A value of a product type contains zero or more fields with potentially different types. Some example product types are:</p>
<ul>
<li><code>data Coordinate = Coordinate { x :: Int, y :: Int }</code>: a product with two <code>Int</code> fields</li>
<li><code>data D = D Int String Float</code>: a product with <code>Int</code>, <code>String</code>, and <code>Float</code> fields</li>
<li><code>data Empty = Empty</code>: a product with no fields</li>
</ul>
<p>Note that the way you access the fields does not matter. In the examples above, fields of a <code>Coordinate</code> value can be accessed with pattern matching, or with the generated functions <code>x</code> and <code>y</code>. In the second example, we can only access the fields with pattern matching.</p>
<p>What matters is: products contain zero or more fields. The fields can have different types.</p>
<h1 id="sums">Sums</h1>
<p>A sum type specifies multiple “variants” (or “alternatives”), where each variant has a “name” (or “tag”, more on this later) and some number of fields.</p>
<p>A value of a sum type holds a name (or tag), and the fields of the variant with that name.</p>
<p>For example, if you have a parser for integers, you will want to return an integer when parsing succeeds, or an error message when something goes wrong. The sum type for the return value of your parse function would look like:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true"></a><span class="kw">data</span> <span class="dt">ParseResult</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true"></a>  <span class="ot">=</span> <span class="dt">Success</span> <span class="dt">Int</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true"></a>  <span class="op">|</span> <span class="dt">Fail</span> <span class="dt">String</span></span></code></pre></div>
<p>Here, <code>Success</code> and <code>Fail</code> are names of the variants. <code>Success</code> variant has an <code>Int</code> field, and <code>Fail</code> variant has a <code>String</code> field.</p>
<p>A value of this type does not contain an <code>Int</code> and <code>String</code> at the same time. It’s either a <code>Fail</code> with a <code>String</code> field, or a <code>Success</code> with an <code>Int</code> field.</p>
<p>The way you access the fields is with pattern matching:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true"></a><span class="kw">case</span> parse_result <span class="kw">of</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true"></a>   <span class="dt">Success</span> int <span class="ot">-&gt;</span> <span class="op">...</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true"></a>   <span class="dt">Fail</span> error_message <span class="ot">-&gt;</span> <span class="op">...</span></span></code></pre></div>
<h1 id="names-in-type-checking-nominal-typing">Names in type checking (nominal typing)</h1>
<p>If I have two types, named <code>T1</code> and <code>T2</code>, no matter how they are defined, they are considered different in Haskell, and most other widely used typed languages (Rust, Java, …). This is called “nominal” type checking, where differently named types are considered different, even if they are “structurally” the same. For example, <code>data T1 = T Int</code> and <code>data T2 = T Int</code> are structurally the same, but you can’t apply a value of type <code>T2</code> to a function that expects <code>T1</code>.</p>
<p>What “structurally same” mean is open to interpretation. We will come to this later.</p>
<p>In addition, all types have names<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>, even types like tuples, which may look like they don’t have names, like our <code>Coordinate</code> or <code>ParseResult</code> have.</p>
<p>Tuples in most languages are just a bunch of product types, like the ones you can define yourself. They are often pre-defined for arities 0 to some number, and they have a special, “mixfix” syntax, with parentheses and commas to separate the fields. Other than that, they are no different than the ones you can define yourself.</p>
<p>You can see GHC’s definition of tuples <a href="https://github.com/ghc/ghc/blob/a951e06921f05df1601d9c3a39efcede27f3330c/libraries/ghc-prim/GHC/Tuple.hs#L34-L58">here</a>. In GHC, you can use the name directly if you don’t want the mixfix syntax, like <code>(,) 1 2</code>. So the name for an 2-ary tuple is <code>(,)</code> in Haskell, and it has a special syntax so you can write more readable <code>(1, 2)</code> (or <code>(Int, Int)</code> in type context). Other than syntax, there’s nothing special about tuples.</p>
<p>So it’s clear that most languages don’t have anonymous types. All types have some kind of names, and two types are only “compatible” if the names match.</p>
<p>Before defining what anonymous types are, I would like to give two examples, from PureScript and OCaml, where types are not checked based on their names, but based on their “structure”.</p>
<h1 id="structural-type-checking-for-products">Structural type checking for products</h1>
<p>A record is a product type with named (or “labelled”) fields. Our <code>Coordinate</code> example is a record.</p>
<p>In PureScript, records can be defined without giving names to them. For example:</p>
<pre class="purescript"><code>f :: { x :: Int, y :: Int } -&gt; Int
f a = a.x + a.y</code></pre>
<p>Here, <code>f</code> is a function that takes a record with two <code>Int</code> fields, named <code>x</code> and <code>y</code>, as an argument.</p>
<p>Here is a more interesting version of the same function:</p>
<pre class="purescript"><code>f :: forall r . { x :: Int, y :: Int | r } -&gt; Int
f a = a.x + a.y</code></pre>
<p>This version takes a record with <em>at least</em> <code>x :: Int</code> and <code>y :: Int</code> fields, but it can have more fields. Using this version, this code type checks:</p>
<pre class="purescript"><code>f { x: 1, y: 2, z: 3, t: 4 }</code></pre>
<p>The <code>r</code> in this type is not too important. Important part is, in PureScript, records are not type checked nominally. Indeed, in the example above, type of the record with 4 fields is not defined, and no names are used for the record in the type signature of <code>f</code>.</p>
<p>You might think that the record braces and commas are similar to the tuple syntax, so the name could be something like <code>{,}</code>, maybe applied to <code>x :: Int</code> somehow (assuming there is a type-level representation of field names).</p>
<p>However, even if that’s the case, type checking of these types are quite different than tuples. We’ve already seen that we can pass a record with more fields. You can also reorder fields in the function type signature<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>, or in the record expression, and it still works.</p>
<p>So type checking for PureScript is quite different than Haskell tuples.</p>
<p>This kind of type checking where you look at the “structure” rather than just the names is called structural type checking.</p>
<p>Now let’s take a look at an example for sum types.</p>
<h1 id="structural-type-checking-for-sum-types">Structural type checking for sum types</h1>
<p>OCaml has named sum types, just like Haskell’s. Here is the OCaml version of our <code>ParseResult</code> type:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true"></a><span class="kw">type</span> parse_result =</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true"></a>  | Success <span class="kw">of</span> <span class="dt">int</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true"></a>  | Fail <span class="kw">of</span> <span class="dt">string</span></span></code></pre></div>
<p>Name of this type is <code>parse_result</code> (following OCaml naming conventions), and it is type checked exactly the same way it is type checked in Haskell.</p>
<p>A second way of defining sum types in OCaml, and without names, is with <a href="https://ocaml.org/manual/lablexamples.html#s:polymorphic-variants">polymorphic variants</a>. Here’s the polymorphic variant for the same type:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true"></a><span class="kw">type</span> parse_result = [ `Success <span class="kw">of</span> <span class="dt">int</span> | `Fail <span class="kw">of</span> <span class="dt">string</span> ]</span></code></pre></div>
<p>Crucially, even though we use a similar syntax with the <code>type</code> keyword, this is a type synonym. The right-hand side of this definition is an anonymous sum with two variants, tagged <code>`Success</code> and <code>`Fail</code>, with <code>int</code> and <code>string</code> fields, respectively.</p>
<p>Now, suppose I have a parse result handler, which, in addition to the success and failure cases, handles some “other” case as well:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true"></a><span class="kw">let</span> f = <span class="kw">function</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true"></a>  | `Success i -&gt; <span class="dt">Printf</span>.printf <span class="st">&quot;Parse result: %d</span><span class="ch">\n</span><span class="st">&quot;</span> i</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true"></a>  | `Fail msg -&gt; <span class="dt">Printf</span>.printf <span class="st">&quot;Parse failed: %s</span><span class="ch">\n</span><span class="st">&quot;</span> msg</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true"></a>  | `Other -&gt; <span class="dt">Printf</span>.printf <span class="st">&quot;Wat?</span><span class="ch">\n</span><span class="st">&quot;</span></span></code></pre></div>
<p>Type of this function as inferred by the OCaml compiler is:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true"></a>[&lt; `Fail <span class="kw">of</span> <span class="dt">string</span> | `Other | `Success <span class="kw">of</span> x ] -&gt; <span class="dt">unit</span></span></code></pre></div>
<p>What this type says is that the function accepts any polymorphic variant that has the tags <code>Fail</code>, <code>Other</code>, and <code>Success</code> (with the specified field types), or some subset of these tags. So if I have a value of type <code>parse_result</code>:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true"></a><span class="kw">let</span> x : parse_result = `Success <span class="dv">123</span></span></code></pre></div>
<p>I can pass it to <code>f</code>, even though <code>f</code>’s argument type is not exactly <code>parse_result</code>. Here’s the full example, run in <a href="https://github.com/ocaml-community/utop">utop</a>: (<code>utop #</code> part is the prompt, lines after <code>;;</code> are utop outputs)</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode ocaml"><code class="sourceCode ocaml"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true"></a>utop # <span class="kw">type</span> parse_result = [ `Success <span class="kw">of</span> <span class="dt">int</span> | `Fail <span class="kw">of</span> <span class="dt">string</span> ];;</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true"></a><span class="kw">type</span> parse_result = [ `Fail <span class="kw">of</span> <span class="dt">string</span> | `Success <span class="kw">of</span> <span class="dt">int</span> ]</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true"></a>utop # <span class="kw">let</span> f = <span class="kw">function</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true"></a>  | `Success i -&gt; <span class="dt">Printf</span>.printf <span class="st">&quot;Parse result: %d</span><span class="ch">\n</span><span class="st">&quot;</span> i</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true"></a>  | `Fail msg -&gt; <span class="dt">Printf</span>.printf <span class="st">&quot;Parse failed: %s</span><span class="ch">\n</span><span class="st">&quot;</span> msg</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true"></a>  | `Other -&gt; <span class="dt">Printf</span>.printf <span class="st">&quot;Wat?</span><span class="ch">\n</span><span class="st">&quot;</span>;;</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true"></a><span class="kw">val</span> f : [&lt; `Fail <span class="kw">of</span> <span class="dt">string</span> | `Other | `Success <span class="kw">of</span> <span class="dt">int</span> ] -&gt; <span class="dt">unit</span> = &lt;<span class="kw">fun</span>&gt;</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true"></a></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true"></a>utop # <span class="kw">let</span> x : parse_result = `Success <span class="dv">123</span>;;</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true"></a><span class="kw">val</span> x : parse_result = `Success <span class="dv">123</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true"></a></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true"></a>utop # f x;;</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true"></a>Parse result: <span class="dv">123</span></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true"></a>- : <span class="dt">unit</span> = ()</span></code></pre></div>
<p>Neat!</p>
<p>Similar to PureScript records, and unlike Haskell tuples, type checking for OCaml polymorhic records is structural, not nominal.</p>
<h1 id="names---nominal---structural">Names -&gt; nominal, ??? -&gt; structural</h1>
<p>Now that we have seen structural type checking as an alternative to name-based (nominal) type checking, and some examples, here is my attempt at defining anonymous types: If named types are type checked nominally, then the types that are structurally type checked are called “anonymous”.</p>
<p>In other words:</p>
<ul>
<li>Nominally type checked types are named</li>
<li>Structurally type checked types are anonymous</li>
</ul>
<p>According to this definition, Haskell and many other languages don’t have anonymous types. PureScript records are an example to anonymous products, and OCaml polymorphic variants are an example to anonymous sums.</p>
<h1 id="conclusions">Conclusions</h1>
<p>Named types are checked nominally, anonymous types are checked structurally. According to this definition, Haskell, and many other languages, don’t have anonymous types, as all types are nominally checked.</p>
<p>Tuples are no exception: they have names, and type checked nominally.</p>
<p>PureScript records and OCaml polymorphic variants are great examples to anonymous products and sums, respectively.</p>
<!---




























# Tuples: named or anonymous?

Tuples in languages like Haskell, Rust, and OCaml, are pre-defined product
types that are often used for returning multiple values from functions, without
having to define a new type for the return value.

For example, the tuple type `(Int, Int)` in Haskell is a product with two `Int`
fields. This type is similar to our `Coordinate` example above.

Because tuples don't have "names", like our `Coordinate` has, it may seem like
tuples are unnamed, or "anonymous". This is actually not true. Tuples in most
languages are just a bunch of product types, like the ones you can define
yourself. They are often pre-defined for arities 0 to some number, and they have
a special, "mixfix" syntax, with parentheses and commas to separate the fields.
Other than that, they are no different than the ones you can define yourself.

As an example, you can see GHC's definition of tuples [here][2]. In GHC, you can
use the name directly if you don't want the mixfix syntax, like `(,) 1 2`. So
the name for an 2-ary tuple is `(,)` in Haskell, and it has a special syntax so
you can write more readable `(1, 2)` (or `(Int, Int)` in type context).

So it's clear that tuples in Haskell are not anonymous, they have names. But
what are anonymous types then? Does Haskell even have anonymous types? Before
defining anonymous types, let's briefly talk about how are names used in type
checking.

# Nominal and structural type checking

If I have two types, named `T1` and `T2`, no matter how they are defined, they
are considered different in Haskell, and most other widely used typed languages
(Rust, Java, ...). This is called "nominal" type checking, where differently
named types are considered different, even if they are "structurally" the same.
For example, `data T1 = T Int` and `data T2 = T Int` are structurally the same,
but you can't apply a value of type `T2` to a function that expects `T1`.


What does "structurally same" mean here is open to interpretation, but the
crucial part is with structural type checking, types can have different set of
fields or variants and still be compatible. We will shortly see examples of
this.

Now, here's the difficulty with defining anonymous types (and a point of
confusion, at least for me). Regardless of the syntax, I will have to introduce
some kind of type constructors for anonymous products and sums. One might always
consider those constructors as the names of the types (with type parameters
applied for the fields/ variants).

For example, if I use `*` syntax for anonymous products, like `Int * Int * Bool`
for a product type like `(Int, Int, Bool)`, you might argue that the "name" here
is `*`, and the desugared version is something like `(*) Int ((*) Int Bool)`,
and you would probably be right! In many (most?) type systems, including
Haskell's, complex types are made by applying types to type constructors. The
type constructors used for constructing product or sum types can be thought of
as the name of the types.

In that sense I think there really isn't any *obviously anonymous* types where
you will know it when you see it. Every type is constructed by applying some
number of arguments to a "name" (usually called "type constructors").

# Names -> nominal, ??? -> structural

With these definitions in mind, here's my attempt at defining anonymous types.
If named types are type checked nominally (where different names mean types are
incompatible), then the types that are structurally type checked are called
"anonymous".

In other words:

- Nominally type checked types are named
- Structurally type checked types are anonymous

In the simple structural type checking rule we've seen above, anonymous
products (constructed with `*`) and tuples (constructed with the mixfix tuple
syntax) are type checked exactly the same way. Let's add one more rule to make
them different:

- Before applying the rules given before, rearrange the type arguments to make
  the first argument of `*` a non-`*` type.

Example: if I have `(Int * Int) * (Int * Int)`, this rule rearranges it to make
it `Int * (Int * (Int * Int))`.

(This rule effectively makes `*` associative)

With this new rule we now accept these two types as compatible:

- `(Int * (Int * Int)) * Int`
- `(Int * Int) * (Int * Int)`

as they are both rearranged before checking as `Int * (Int * (Int * Int))`.

(Whether this rule is useful or desired is a different matter)

With this structural equality rule, type checking of tuples and products
constructed with `*` are different, and we call tuples named types and `*`
products anonymous.

# Anonymous sum types

Instead of inventing syntax and defining type checking for anonymous sum types,
like we did for products, I will show an example of anonymous sums in an
existing programming language: OCaml's [polymorphic variants][3].

Here's the OCaml version of our `ParseResult` type:

```ocaml
type parse_result =
  | Success of int
  | Fail of string
```

This type is nominally checked, so if you have a function that expects
`parse_result` argument, you have to pass it a `Success` or `Fail`. Anything
else will cause a type error.

Here's the polymorphic variant for the same type:

```ocaml
type parse_result = [ `Success of int | `Fail of string ]
```

Crucially, even though we use a similar syntax with the `type` keyword, this is
a type synonym. The right-hand side of this definition is an anonymous sum with
two variants, labelled `` `Success`` and `` `Fail``, with `int` and `string`
fields, respectively.

Here's an example of structural type checking of polymorphic variants. Suppose I
have a parse result handler, which, in addition to the success and failure
cases, handles some "other" case as well:

```ocaml
let f = function
  | `Success i -> Printf.printf "Parse result: %d\n" i
  | `Fail msg -> Printf.printf "Parse failed: %s\n" msg
  | `Other -> Printf.printf "Wat?\n"
```

Type of this function as inferred by the OCaml compiler is:

```ocaml
[< `Fail of string | `Other | `Success of x ] -> unit
```

What this type says is that the function accepts any polymorphic variant that
has the tags `Fail`, `Other`, and `Success` (with the specified field types), or
some subset of these tags. So if I have a value of type `parse_result`:

```ocaml
let x : parse_result = `Success 123
```

I can pass it to `f`, even though `f`'s argument type is not exactly
`parse_result`. Here's the full example, run in [utop][9]: (`utop #` part is the
prompt, lines after `;;` are utop outputs)

```ocaml
utop # type parse_result = [ `Success of int | `Fail of string ];;
type parse_result = [ `Fail of string | `Success of int ]

utop # let f = function
  | `Success i -> Printf.printf "Parse result: %d\n" i
  | `Fail msg -> Printf.printf "Parse failed: %s\n" msg
  | `Other -> Printf.printf "Wat?\n";;
val f : [< `Fail of string | `Other | `Success of int ] -> unit = <fun>

utop # let x : parse_result = `Success 123;;
val x : parse_result = `Success 123

utop # f x;;
Parse result: 123
- : unit = ()
```

Neat!

# Conclusions

Named types are checked nominally, anonymous types are checked structurally.
According to this definition, Haskell, and many other languages, don't have
anonymous types, as all types are nominally checked.

OCaml's [polymorphic variants][3] are a great example to anonymous sums.

For real-world anonymous products, it would be a shame to not mention records
and row types[^1]. A record is a product type with labelled fields, for example:
`{ a : Int, b : Bool }`. Row types allow (among other things) very flexible type
checking of records, where you can (without subtyping) pass a record with more
fields when less is expected. With row types, unlike subtyping, if you have a
function that expects a record type like `{ a : Int, b : Bool }` and returns the
argument after using or modifying the fields `a` and/or `b`, and pass the
function a record with more fields, like `{ a : Int, b : Bool, c : String }`, as
the return type you get your original record with 3 fields. More concretely:

```
{ a : Int, b : Bool, ..r } -> { a : Int, b : Bool, ..r }
```

is the type of a function that takes a record with fields `a` and `b` with the
specified types, and *possibly* more fields. These extra fields are represented
by `r`, which appears in both input and output types. This means you don't lose
the extra fields if you pass a record with more fields to this function, unlike
in a system with subtyping. With subtyping, if you have a function with type

```
{ a : Int, b : Bool } -> { a : Int, b : Bool }
```

and pass `{ a : Int, b : Bool, c : String }` to such a function, you lose the
field `c` in the return value, because the return type only mentions `a` and
`b`, without the "extra stuff" part as we've seen in the row polymorphic
version.

(As as aside, I'd like to mention that I really like row types. My first
exposure to them was back in 2013 ([1][5], [2][6]): I implemented two type
systems, one with row types and one for a multi-stage language, proved soundness
of the systems, gave a type and term translation from one to the other, and
proved that if your program is well-typed, then the translation of it is also
well-typed.)

My favorite paper on row-polymorphic records is probably (1). (2) uses row types
for algebraic effects. (3) uses rows for variants (sums).

-->
<hr />
<p>Thanks to <a href="https://twitter.com/_gilmi/">@_gilmi</a> and <a href="https://twitter.com/madgen_/">@madgen_</a> for their helpful comments on a draft of this blog post.</p>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>With the exception of type synonyms. Type synonyms can be considered as simple macros for substituting types for names before type checking.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>In Haskell, reordering stuff at the type level is often done with type families (type-level functions). Types are still checked nominally, but by rearranging them before type checking you can often have something somewhat similar to structural checking.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></summary>
</entry>
<entry>
    <title>Proving soundness of simply typed multi-staged lambda-calculus</title>
    <link href="http://osa1.net/posts/2014-03-06-proving-simply-typed-multi-staged-lc.html" />
    <id>http://osa1.net/posts/2014-03-06-proving-simply-typed-multi-staged-lc.html</id>
    <published>2014-03-06T00:00:00Z</published>
    <updated>2014-03-06T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>First part of my first non-trivial(e.g. something other than a Software Foundations exercise) Coq program is finally done. I learned Coq from Software Foundations, but I didn’t finish to book. I still know only very basic tactics, which I think is one of the reasons why my proofs are so long. You can see the source <a href="https://github.com/osa1/StagedLambda/blob/master/Lc.v">here</a>.</p>
<p>Some random notes about implementation:</p>
<ul>
<li>I’m looking to improve proofs. I have a lot more to implement for the rest of the program until deadline, so I may not be able to refactor current proofs very much, but at one point I want to simplify the proofs and use more advanced tactics.</li>
<li>Currently only <code>inversion</code>, <code>destruct</code>, <code>induction</code>, <code>rewrite</code>, <code>assumption</code>, <code>assert</code>, <code>constructor</code>, <code>auro</code>, <code>simpl</code>, <code>intro</code>/<code>intros</code>, <code>apply</code>, <code>right</code>/<code>left</code>, <code>exists</code>, <code>unfold</code>, <code>subst</code>, <code>reflexivity</code>, <code>remember</code> and <code>generalize</code> tactics are used.</li>
<li>Language definition is almost the same as in papers. There is one difference, we implemented substitutions as a function. In reality, substitution in multi-staged lambda-calculus is not a function. I believe this doesn’t effect correctness of theorems. At one point I’ll refactor the code and define substitution as a relation.</li>
<li>I used <code>Case</code>, <code>SCase</code>, <code>SSCase</code> … constructs from SFlib extensively.</li>
</ul>
<p>Now I’m going to implement lambda-calculus with row-polymorphic records. I expect this to be at least 2x harder, since polymorphism is involved. Let’s see how it goes …</p>]]></summary>
</entry>
<entry>
    <title>Internship report - type inference, row polymorphism, and multi-stage programming</title>
    <link href="http://osa1.net/posts/2013-04-15-internship.report.html" />
    <id>http://osa1.net/posts/2013-04-15-internship.report.html</id>
    <published>2013-04-15T00:00:00Z</published>
    <updated>2013-04-15T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>This is the report of my 3.5-month internship fulfilled at <a href="http://ozyegin.edu.tr/">Ozyegin University</a> under <a href="http://faculty.ozyegin.edu.tr/aktemur/">Prof. Baris Aktemur</a>’s supervision.</p>
<p>I wrote this report as a short and informal introduction to the topics I worked on in my internship, and since I know nobody will ever read this internship report, I decided publishing it in my blog with hoping someone benefit from it.</p>
<p>A note before reading: Things went in an unexpected way shortly after I started writing the report: I got bored. So each chapter got shorter and shorter, leaving tons of interesting and important stuff unmentioned. Sorry for that.</p>
<h2 id="contents">Contents:</h2>
<ul>
<li>Introduction to type systems and polymorphism</li>
<li>Hindley-Damas-Milner type system and type inference</li>
<li>Row polymorphism</li>
<li>Multi-stage porgramming</li>
<li>Our work - extending multi-stage language with subtyping and Wallce: a module type inference system</li>
</ul>
<h1 id="introduction-to-type-systems-and-polymorphism">Introduction to type systems and polymorphism</h1>
<p>As far as software engineering concerned, a type system is a static analysis method that aims to prove a program ‘won’t go wrong’<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>, meaning that the program will not crash at runtime. Static type checking can also be used for some compiler optimisations. Also, some languages have mechanisms for dispatching on types.</p>
<p>Type systems (in this text, this term is used as ‘static type systems’) do that by inspecting program terms and proving some properties and relations among them. This work is done before run-time, and most type information can be deleted so that it won’t cause any runtime costs.</p>
<p>For most type systems, process of type checking is ‘syntactic’. This means two things; first, the type checker only needs program text and it doesn’t need any information that can only be obtained at run-time. And second, type checking a program term only depends on type checking subterms of that program. These two properties also give us a somewhat simple way to express type checking on terms in a symbolic and formal way.</p>
<p>Type systems are ‘conservative’, meaning they will reject some ‘correct’ programs. A correct program means a program that won’t crash at runtime. For instance, this program(in OCaml syntax):</p>
<pre><code>if true then 1 else 42(5)</code></pre>
<p>is actually correct, and it can be given the type <code>int</code> (since we know it always evaluates to <code>1</code>, which has type <code>int</code>), but almost all type systems reject this program because of the ‘else’ branch expression <code>42(5)</code> that is ill-typed.</p>
<p>Research on type systems aims to have more “powerful” type systems. Being more powerful means accepting <em>more</em> correct programs and also being able to encode more <em>invariants</em> in the type system, so that more <em>incorrect</em> programs will be rejected.</p>
<p>Being wrong and being incorrect is used with different meanings in this text: a wrong program will crash at run-time, but incorrect program will result with a wrong answer. For instance, a wrong implemented algorithm may not be <em>wrong</em>, but it’s <em>incorrect</em>. So it will work fine, but return a wrong result.</p>
<p>As an instance of accepting more correct programs, this code is not typeable under simply-typed lambda calculus:</p>
<pre><code>let id a = a in
(id 1, id true)</code></pre>
<p>But it’s well-typed in polymorphic lambda calculus. As a second example, this is not typeable under polymorphic lambda calculus:</p>
<pre><code>add5 : float -&gt; float
...
add5 10</code></pre>
<p>Here <code>10</code> has type <code>int</code>, so we can’t apply it to a function that expects a float value. Type systems with subtyping overcome this problem by defining a common supertype of types. In our case, <code>float</code> is already a supertype of <code>int</code>, so this example is well-typed in the presence of subtyping.</p>
<p>As an example of being able to encode more <em>invariants</em> in the type system, let’s look to the type of a sort function from a dependently-typed programming language:<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<pre><code>sort : List Int -&gt; List Int</code></pre>
<p>This type can be inhabited with some wrong functions. For instance, a function that takes a list and returns an empty list will have this type, just like <em>reverse</em> and <em>shuffle</em> functions.</p>
<pre><code>sort : Vect Int n -&gt; Vect Int n</code></pre>
<p>This type has a invariant encoded in it: length of it’s input and output should be equal. This means our first function that takes a list and returns an empty list now can’t be inhabited by this type. We have a more precise type.</p>
<pre><code>sort : (xs : Vect Int n) -&gt;
       (ys : Vect Int n ** Permutation xs ys)</code></pre>
<p>This type is even more precise, a function that takes a list and returns a list with some random elements can’t get this type.</p>
<p>One problem with more expressive type systems is that there may be some types that can’t be inferred by type system, but the type can be given by programmer. In some cases the type system may not be able to prove that an expresion has the denoted type. In case of dependently typed languages, programmer is required to give some proofs to the type checker. But even in much simpler type systems programmer may have to give some types manually.</p>
<p>–</p>
<p>A word about the terms ‘polymorphism’ and ‘subtyping’: Polymorphic type systems can give multiple types to a single piece of code, thus providing reuse. This is done by giving the code a general type and then creating instances of type depending on the use. As an example, let us start with a simple OCaml function, <code>fst</code>.</p>
<p><code>fst</code> returns first element of any pair, it has the type <code>('a * 'b) -&gt; 'a</code> (the language of types is described in next section). This means for any pair with type of <code>('a * 'b)</code>, <code>fst</code> will return value with type of <code>'a'</code>. When applied on a value typed <code>(bool * int)</code> it will return <code>bool</code>, and when applied on a value typed <code>( (string * bool) * string )</code> it will return <code>(string * bool)</code> etc.</p>
<p>Subtyping defines a relationship that if a type <code>t1</code> is subtype of a type <code>t2</code>, this means that <code>t1</code> can be given to context where <code>t2</code> is expected. This usually means <code>t1</code> already has all properties of <code>t2</code>.</p>
<p>For instance, a record with fields <code>a: int, b: bool</code> can be passed to a function that expects a parameter typed <code>{a: int}</code>. The same effect can also be obtained by row polymorphism, which we’ll see in it’s own chapter.</p>
<p>Subtyping is generally divided into two branches; nominal and structural subtyping. Structural subtyping relation is syntactic, ie. type system can decide if one type is subtype of another type just by inspecting their forms (like in the example above). Nominal subtyping requires definition from the programmer or a pre-defined type lattice from the language designer, ie. there is no way for a type system to infer subtyping relation between int and float types if this relation is not built-in; similarly a Square is not a subtype of Shape unless the programmer manually defines this subtyping relation.</p>
<h1 id="hindley-damas-milner-type-system-and-type-inference">Hindley-Damas-Milner type system and type inference</h1>
<p>Manually giving types to program terms may be impractical, since programs can contain hundreds of definitions. Type inference is the process of deriving types for program terms. As an example of language with type inference, OCaml programs with no type annotations can be type-inferred and checked. This results in more concise programs. Type annotations can still be given for documentation or error reporting purposes.</p>
<p>Hindley-Damas-Milner(abbreviated as HM henceforth) type system is likely to be the most widely-used type system. It has a great property that if a term is typeable under HM type system, HM inference algorithm will result with a most general type (also called ‘principal type’).</p>
<p>HM is the system lies behind statically typed functional languages like the ML family of languages and Haskell.</p>
<p>In HM, a polymorphic type is specified as a ‘type scheme’. A type scheme is a type with universally quantified type variables. Type system’s job then is to give terms of a program type schemes so that for each instantiation of the scheme, the result is well-typed.</p>
<p>HM type system produces polymorphic types (type schemes) only for some expressions, in the case of ML family of languages, it’s the <code>let</code> expression. A <code>let</code> expression binds a name to an expression and introduces a scope that the name is bound to the expression. HM type system gives a most general type to that name, and so every use of this name in the scope is valid if a valid instantiation of the required type can be obtained from the type scheme it’s given.</p>
<pre><code>let id a = a in
(id 1, id true)</code></pre>
<p>Here the <code>id</code> function is given the type scheme of <code>forall a. a -&gt; a</code>. The variable <code>a</code> is called to be ‘universally quantified’. In <code>id 1</code>, <code>id</code>s type is instantiated from type scheme as <code>int -&gt; int</code> by substituting <code>a</code> with <code>int</code>. Same operation is done in <code>id true</code> and <code>id</code> is given the type <code>bool -&gt; bool</code>. So this term is well-typed under HM.</p>
<p>Inferring a type scheme is done by assigning fresh type variables to terms with unknown types and unifying this variables with concrete types while type checking rest of the term. At the end, types that are not unified with concrete types will remain polymorphic.</p>
<p>This is also called let-polymorphism because polymorphic type (type scheme) generation is only done in let expressions.</p>
<p>Note that not all names defined in a let expression can be generalized. As an example, see this classic example:</p>
<pre><code>let c = ref (fun x -&gt; x) in
c := (fun x -&gt; x+1);
!c true</code></pre>
<p>Giving <code>c</code> the type <code>forall a. ('a -&gt; 'a) ref</code> (in OCaml syntax) makes this example well-typed, which is wrong. Instead, type system should have given the type <code>( '_a -&gt; '_a) ref</code>, then during the type checking <code>c := (fun x -&gt; x + 1)</code>, <code>_a</code> should be unified with <code>int</code>. Note that since <code>('_a -&gt; '_a) ref</code> doesn’t quantify the variable <code>a</code>, after this point, type of <code>c</code> will be <code>(int -&gt; int_ ref</code>, so applying it a bool value will fail.</p>
<p>Let-polymorphism has some interesting properties. It’s simple, but it still accepts realistic programs. And even though its worst-case efficiency is exponential on the size of input program, most realistic programs don’t hit the worst case and have linear time complexity. For this reason, let-polymorphism is named as the ‘sweet-spot’ of type systems.</p>
<p>Another great property of HM type system is that it can be reduced to constraint generation and solving steps. These separate steps lead to a more modular algorithm, with changeable constraint generators and solvers. HM(X)<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> is a formalization of this observation.</p>
<h1 id="row-polymorphism">Row polymorphism</h1>
<p>Rows are a way to encode ‘labeled products’. A product type is a type that contains some other types; for example, a pair is a product type of two other types (<code>(int * bool)</code> is a product type with <code>int</code> and <code>bool</code> parts).</p>
<p>A ‘labeled sum’ is a product type, but with labels. For instance:</p>
<pre><code>type error = { code : int; reason : string }</code></pre>
<p>is just like an <code>(int * string)</code> type, but it has labels for subparts. There are severals ways to encode labeled products as rows. Here’s one way:</p>
<pre><code>type error = { code : abs int; reason : abs int; rho[code,int] }</code></pre>
<p>This syntax is similar to Rémy’s, which was also used in our project. Last <code>rho</code> is the polymorphic part, it means that an error type is a row type with <code>code</code> and <code>reason</code> fields with gives types, but with a polymorphic part that can be unified with any record except the fields labeled with <code>code</code> or <code>int</code> (some systems accept field repetition, like in Daan Leijen’s “Extensible records with scoped labels”).</p>
<p>Row polymorphism gives us a subtyping-like effect. For example, this function:</p>
<pre><code>fun x -&gt; x.a + 1</code></pre>
<p>Now has the type <code>{ a : int; rho[a] } -&gt; int</code> and we can apply it to the record <code>{ x : some_type; y : some_other_type; a : int; ... }</code>, like in structural subtyping.</p>
<p>The way row polymorphism and structural subtyping accept this term are completely different though. In row polymorphism, parameter type <code>{a : int; rho[a] }</code> is being unified with <code>{ x : some_type; y : some_other_type; a : int; ... }</code>, so parameter type of function is actually getting more specialized. On the other hand, in subtyping, extra fields of actual parameter is being forgotten, like casting a type to it’s super type.</p>
<p>More words will be said about row polymorphism in later sections.</p>
<h1 id="multi-stage-programming">Multi-stage programming</h1>
<p>Multi-stage programming languages makes distinguished evaluation stages in run-time. Some part of the program can be generated depending on some user input.</p>
<p>Evaluation is done at stage 0. A program generated in staged 0 has stage 1, program generated in stage 1 has stage 2 etc. This notion of separate stages has some semantic effects. Each stage has a scope, but most systems offer a mechanism to lift a value from a lower stage.</p>
<p>Being able to generate code in run-time gives us a way to specialize algorithms depending on user input. Let’s look at a classic example:</p>
<pre><code>let rec pow_body_gen n =
  if n = 0 then &lt;1&gt;
  else &lt;a * ~(pow_body_gen (n-1))&gt;;;</code></pre>
<p><code>pow_body_gen</code> takes an integer <code>n</code> and returns the code <code>a * a * a * ... * 1</code>, a to the power of n. With the help of this function, we can generate a specialized function <code>power_five</code> which raises an integer to the power 5, but without running a loop or recursively calling a function:</p>
<pre><code>let rec power_five a = &lt;let a = ~lift(a) in ~(pow_body_gen 5)&gt;;;</code></pre>
<p>For example, output of <code>power_five 12</code> in our interpreter is <code>&lt;let a = 12 in (a * (a * (a * (a * (a * 1)))))&gt;</code>. This is a code value. Notice the <code>a</code> value, which is multiplied by itself, 5 times.</p>
<p>To actually run this code, we can call <code>run</code>(stage primitives will be explained shortly):</p>
<pre><code>&gt; run(power_five 12);;
248832</code></pre>
<p>Our multi-stage language has three primitives for staging operations: angle brackets indicate a staged expression, contents of staged expression will be run in stage <code>n+1</code>, where n is the current stage level. Tilde (~) ‘unboxes’ a staged expression, expressions indicated with a unboxing operator will be run in stage <code>n-1</code>. Unboxing operator can only be used in stages n &gt; 0. Finally, <code>run</code> primitive is used to run a closed code value at stage 0.</p>
<p>Note that code values can be open, but only closed code values can be run. Our <code>pow_body_gen</code> function returns a code value with an unbound variable <code>a</code>, then in <code>power_five</code>, we define <code>a</code> in stage 1, and by unboxing <code>pow_body_gen 5</code>, we actually generate a bigger code value, with <code>a</code> defined. Now our code value is closed, so it can be run.</p>
<p>There is also a relation between multi-stage programming and partial evaluation, but this is out of this text’s scope.</p>
<h1 id="our-work">Our work</h1>
<p>Wallace is a type inference library, supporting subtyping. “Its goal is to serve as a plug-in component in the design of a constraint-based type-checker, regardless of the programming language being analyzed.”<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<p>We used Wallace to get subtyping in our language. Wallace comes with an example language, called ‘toy’, which has extensible records built-in. For simplicity, instead of generating constraints for Wallace, we first translated our multi-stage language to record calculus. Translation to record calculus preserves semantics, but eliminates staged computations<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>. Our record calculus is then translated to ‘toy’. ‘toy’ infers types for us, by generating and passing constraints to Wallace.</p>
<p>We then compared our type system with<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>. This expression:</p>
<pre><code>run((fun c -&gt; &lt;(let x = 1 in ~c, let y = 1 in ~c)&gt;) &lt;1&gt;);;</code></pre>
<p>is not typeable under polymorphic type system, but is well-typed in our system with subtyping. By mapping the calculated type back to our multi-stage language, we got subtyping in the multi-stage language for free.</p>
<p>Wallace is used as is, without any modifications. But we changed ‘toy’ language for our purposes. We first added some numerical types with subtyping relations. ‘toy’ had a problem that it was generalizing every let bindings, without value restriction. To overcome this problem, we added an extra lambda wrapper in the translator when value restriction is needed. Since our type system already had value restriction, it was trivial to apply the same test to check if an expression is expansive. Expansive expressions are those that may allocate new memory cells. Expansive expressions are not generalized.</p>
<p>Another problem with ‘toy’ language is that it doesn’t support toplevel declarations, and adding that wasn’t easy. To overcome this without any serious modifications in ‘toy’s source, we collected all well-typed toplevel expressions and declarations, and then generated a <code>let .. in ..</code> chain of toplevel declarations to give ’toy’ type checker. This means for every new toplevel phrase, ‘toy’ now type checks all old expressions too. This may not be efficient but works fine with our purposes, and it was easy to implement.</p>
<p>Implementing type inference: Implementing a type inference system is hard to do elegantly. One problem is already discussed <a href="/posts/2013-02-15-rowlar-kindlar.html">in my blog (in Turkish)</a>. Every non-trivial type system requires types to be separated into kinds. After that, every type variable will also have kind information and it’s a type error to unify a type variable with wrong kinds. Type applications also checked with the help of kinds.</p>
<p>Kinds are generally checked at run-time. Type systems with kinds usually lead to a simpler type language. For instance, almost all types mentioned in this text can be represented by this simply type language:</p>
<pre><code>type kind =
  | KStar                 (* kind of term types *)
  | KRow                  (* kind of row types *)
  | KArr of (kind * kind) (* kind of type constructors *)

type ty =
  | TCon of tycon     (* constant *)
  | TVar of tyvar     (* type variable *)
  | TApp of (ty * ty)
      (* type application, to be well-typed
         kind of first ty should be KArr (k2, k)
         and second ty should be k2 *)
and tyvar = (tyvarlink ref * kind)
and tyvarlink =
  | NoLink of id (* just a type variable *)
  | LinkTo of ty (* equated to a ty *)
and tycon = (id * kind) (* kind should be always KStar *)</code></pre>
<p>We decided to take a different path, instead of keeping kind information for every type, we created new type constructors for every type and then separating differently kinded type variables with different types. This lead us to this type language:</p>
<pre><code>type ty =
 | TInt
 | TBool
 | TUnit

 | TPair of ty * ty

 | TList of ty
 | TRef  of ty

 | TFun of ty * ty
 | TRec of tyrec
 | TVar of typevar

 | TBox of tyrec * ty

...

and typevar = (tyvarlink * int) ref
and tyvarlink = ty link

and fieldvar = (fieldvarlink * int) ref
and fieldvarlink = field link

and recvar = (recvarlink * int * IdSet.t) ref
and recvarlink = tyrec link</code></pre>
<p>We also needed a sum type for handling differently kinded type variables:</p>
<pre><code>and linkvar =
 | TV of typevar
 | FV of fieldvar
 | RV of recvar</code></pre>
<p>This also made our algorithms more complex, since we had more cases to handle.</p>
<p>This representation is not without any advantages though. For instance, writing a pretty-printer was very easy, because we could easily print a pair and a function or a list and a ref differently, even though they share similar structure, with simple pattern matching.</p>
<p>A note on pretty-printing: OCaml has a great pretty-printer library in stdlib. It’s simple but powerful at the same time, allowing printing most complex structures easily. It interprets some characters in input strings as boxes/spaces indicators etc. and formats the text. For instance, this code:</p>
<pre><code>printf &quot;@[&lt;hov 2&gt;~(&quot;; print_exp exp; printf &quot;)@]&quot;</code></pre>
<p>Here <code>@[&lt;hov 2&gt;</code> part creates a new “horizontal or vertical” box. This box has a property that when a line is split to two lines, every other newline pointers will also be split into separate lines. So contents of this box is either displayed as a single line, or separated from every newline pointers. Later, <code>print_exp exp;</code> part prints the expression inside the box. And lastly, `printf “)@]” closes the paren, then closes the box, so that the text comes later will not be bound with this line split rule.</p>
<p>This gives an easy and concise way to print even most complex structures.</p>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>A Theory of Type Polymorphism in Programming – Robin Milner<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>Code examples taken from <a href="http://vimeo.com/61576198">Dependently Typed Functional Programming with Idris</a> course slides.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p><a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.73.3971">Notes on HM(X)</a><a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p><a href="http://gallium.inria.fr/~fpottier/wallace/">Wallace</a><a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" role="doc-endnote"><p>Choi, Aktemur, Yi, Tatsuta: Static Analysis of Multi-Staged Programs via Unstaging Translation<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6" role="doc-endnote"><p>Kim, Yi, Calcagno: A Polymorphic Modal Type System for List-Like Multi-Staged Languages<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></summary>
</entry>

</feed>
