<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>osa1.net - Posts tagged coq</title>
    <link href="http://osa1.net/tags/coq.xml" rel="self" />
    <link href="http://osa1.net" />
    <id>http://osa1.net/tags/coq.xml</id>
    <author>
        <name>Ömer Sinan Ağacan</name>
        <email>omeragaca@gmail.com</email>
    </author>
    <updated>2014-07-12T00:00:00Z</updated>
    <entry>
    <title>Coq exercises for beginners</title>
    <link href="http://osa1.net/posts/2014-07-12-fun-coq-exercises.html" />
    <id>http://osa1.net/posts/2014-07-12-fun-coq-exercises.html</id>
    <published>2014-07-12T00:00:00Z</published>
    <updated>2014-07-12T00:00:00Z</updated>
    <summary type="html"><![CDATA[<style>
.solution {
  background-color: rgb(245, 245, 245);
  padding-left: 1em;
}
</style>


<p>Formalizing abstractions/data structures and proving theorems about them in Coq is so much fun. I made up some simple exercises that consist of encoding some abstractions and laws we know from algebra and functional programming and then proving that some particular set + some operations on that set obeys the laws.</p>
<p>Using my amazing(!) JavaScript skills, I set up some “show/hide answer” buttons after each exercise. Exercises are easy, but the latter ones are relatively harder. Some abstractions/laws are inspired by Haskell.</p>
<p>Please note that I’m a beginner so my solutions probably have some flaws if you want to use them in large-scale verified programs :) I’m currently learning about typeclasses and records of Coq and I’m open to suggestions for improvements.</p>
<p>In exercises, when we talk that an abstraction should obey some laws, you need to enforce it in construction. e.g. You need to make constructors in a way that user would have to prove that the data structure + operations obey the laws.</p>
<pre class="coq"><code>Require Import List.
Import ListNotations.

Open Scope list_scope.</code></pre>
<h1 id="exercise-1">Exercise 1</h1>
<p>A <a href="http://en.wikipedia.org/wiki/Semigroup">semigroup</a> is a set together with an associative binary function. For example, natural numbers and addition function form a semigroup, because we know/can prove that addition function is associative. More precisely: (in Coq syntax)</p>
<pre class="coq"><code>forall (n1 n2 n3 : nat), n1 + (n2 + 3) = (n1 + n2) + n3.</code></pre>
<p>Encode semigroups in Coq.</p>
<div id="ex-div-1.1" class="solution">
<pre class="coq"><code>Inductive semigroup (A : Type) (Op : A -&gt; A -&gt; A) : Prop :=
| Semigroup_intro :
    (forall (a1 a2 a3 : A), Op a1 (Op a2 a3) = Op (Op a1 a2) a3) -&gt; semigroup A Op.</code></pre>
</div>
<button id="ex-btn-1.1"></button>

<p>Now prove that lists together with append operation form a semigroup. Use standard Coq lists and <code>app</code> function.</p>
<div id="ex-div-1.2" class="solution">
<pre class="coq"><code>Theorem list_semigroup : forall A, semigroup (list A) (@app A).
Proof.
  intro. apply Semigroup_intro. intros.
  induction a1.
  + reflexivity.
  + simpl. f_equal. induction a2; auto.
Qed.</code></pre>
</div>
<button id="ex-btn-1.2"></button>


<h1 id="exercise-2">Exercise 2</h1>
<p>A <a href="http://en.wikipedia.org/wiki/Monoid">monoid</a> is a semigroup with an identity element. In our addition example, identity element is 0, because when applied to the monoid function(addition) as first or second argument, results is the other argument:</p>
<pre class="coq"><code>forall (n : nat), 0 + n = n /\ n + 0 = n.</code></pre>
<p>Encode monoids in Coq.</p>
<div id="ex-div-2.1" class="solution">
<pre class="coq"><code>Inductive monoid A Op (sg : semigroup A Op) (U : A) : Prop :=
| Monoid_intro :
    semigroup A Op -&gt; (forall (a : A), Op U a = Op a U /\ Op U a = a) -&gt; monoid A Op sg U.</code></pre>
</div>
<button id="ex-btn-2.1"></button>

<p>Now prove that lists with empty list as unit element together with the proof that lists are monoids as we proved in previous exercise, form a monoid.</p>
<div id="ex-div-2.2" class="solution">
<pre class="coq"><code>Theorem list_monoid : forall A, monoid (list A) (@app A) (@list_semigroup A) [].
Proof.
  intro. apply Monoid_intro. apply list_semigroup.
  intro. split.
  + rewrite app_nil_r. reflexivity.
  + reflexivity.
Qed.</code></pre>
</div>
<button id="ex-btn-2.2"></button>

<h1 id="exercise-3">Exercise 3</h1>
<p>In this exercise and exercise 4, we’ll be talking about Haskell definitions of abstractions, instead of algebra definitions. (although they may coincide)</p>
<p>A functor is a type with one argument(in Haskell terms, a type with kind <code>* -&gt; *</code>) and a function, together with some laws. If you’re unfamiliar with functors of Haskell, you may want to skip this, or read <a href="http://www.haskell.org/haskellwiki/Typeclassopedia#Functor">Typeclassopedia</a>.</p>
<p>A Coq definition would use these to encode functors:</p>
<ul>
<li>Functor type: <code>F : Type -&gt; Type</code></li>
<li>Functor operation: <code>forall t1 t2, (t1 -&gt; t2) -&gt; f t1 -&gt; f t2</code> (let’s call it fmap)</li>
</ul>
<p>A functor should obey these laws:</p>
<ul>
<li><code>fmap id = id</code></li>
<li><code>fmap (fun x =&gt; g (h x)) = fun x =&gt; (fmap g (fmap h x))</code></li>
</ul>
<p>Encode functors in Coq.</p>
<div id="ex-div-3.1" class="solution">
<pre class="coq"><code>Inductive functor (F : Type -&gt; Type) : (forall t1 t2, (t1 -&gt; t2) -&gt; f t1 -&gt; f t2) -&gt; Prop :=
| Functor_intro
    (fmap : forall t1 t2, (t1 -&gt; t2) -&gt; F t1 -&gt; F t2)
    (l1   : forall t f, fmap t t id f = f)
    (l2   : forall t1 t2 t3, forall (f : F t1) (p : t2 -&gt; t3) (q : t1 -&gt; t2),
              fmap t1 t3 (fun a =&gt; p (q a)) f = fmap t2 t3 p (fmap t1 t2 q f)) :
    functor F fmap.</code></pre>
</div>
<button id="ex-btn-3.1"></button>

<p>Now prove that lists with standard map function form a functor.</p>
<div id="ex-div-3.2" class="solution">
<pre class="coq"><code>Theorem list_functor : functor list map.
Proof.
  apply Functor_intro.
  + intros. induction f. reflexivity. simpl. rewrite IHf. reflexivity.
  + intros. induction f. reflexivity. simpl. f_equal. apply IHf.
Qed.</code></pre>
</div>
<button id="ex-btn-3.2"></button>

<h1 id="exercise-4">Exercise 4</h1>
<p>A monad is a functor with two more operations; let’s call <code>bind</code> and <code>lift</code> and some more laws. In Coq syntax: (<code>F</code> is our functor type)</p>
<ul>
<li>bind: <code>forall t1 t2, F t1 -&gt; (t1 -&gt; F t2) -&gt; F t2</code></li>
<li>lift: <code>forall t, t -&gt; F t</code></li>
</ul>
<p>Laws:</p>
<ul>
<li>Left identity: <code>forall t1 t2 a f, bind t1 t2 (lift t1 a) f = f a</code></li>
<li>Right identity: <code>right_id : forall t m, bind t t m (lift t) = m</code></li>
<li>Associativity: <code>forall t1 t2 t3 m f g, bind t2 t3 (bind t1 t2 m f) g = bind t1 t3 m (fun x =&gt; bind t2 t3 (f x) g)</code></li>
</ul>
<p>Encode monads in Coq.</p>
<div id="ex-div-4.1" class="solution">
<pre class="coq"><code>Inductive monad : (Type -&gt; Type) -&gt; Prop :=
| Monad_intro
    (F        : Type -&gt; Type)
    (fmap     : forall t1 t2, (t1 -&gt; t2) -&gt; F t1 -&gt; F t2)
    (Fp       : functor F fmap)
    (lift     : forall t, t -&gt; F t)
    (bind     : forall t1 t2, F t1 -&gt; (t1 -&gt; F t2) -&gt; F t2)
    (left_id  : forall t1 t2 a f, bind t1 t2 (lift t1 a) f = f a)
    (right_id : forall t m, bind t t m (lift t) = m)
    (assoc    : forall t1 t2 t3 m f g,
                  bind t2 t3 (bind t1 t2 m f) g = bind t1 t3 m (fun x =&gt; bind t2 t3 (f x) g)) :
    monad F.</code></pre>
</div>
<button id="ex-btn-4.1"></button>

<p>Now prove that lists form a monad. You need to figure out what functions to use for <code>lift</code> and <code>bind</code>.</p>
<div id="ex-div-4.2" class="solution">
<p><code>lift</code> function:</p>
<pre class="coq"><code>(* I couldn&#39;t find this in stdlib so let&#39;s define *)
Definition singleton (A : Type) (x : A) := [x].</code></pre>
<p>For <code>bind</code>, you can use standard <code>flat_map</code> function, but it’s argument order is reversed. So instead I rolled my own version:</p>
<pre class="coq"><code>Fixpoint concat {A : Type} (l : list (list A)) : list A :=
  match l with
  | []     =&gt; []
  | h :: t =&gt; app h (concat t)
  end.

(* I don&#39;t like argument order of flat_map in stdlib ... *)
Definition concatMap (A : Type) (B : Type) (l : list A) (f : A -&gt; list B) : list B :=
  concat (map f l).</code></pre>
<p>Now most involved proof in this exercises: (still very easy)</p>
<pre class="coq"><code>Theorem list_monad : monad list.
Proof.
  apply Monad_intro with (fmap := map) (lift := singleton) (bind := concatMap).
  + apply list_functor.
  + intros. unfold concatMap. simpl. rewrite app_nil_r. reflexivity.
  + intros. unfold concatMap. induction m.
    - reflexivity.
    - simpl. f_equal. apply IHm.
  + intros. induction m as [|h t].
    - reflexivity.
    - unfold concatMap in *. simpl. rewrite &lt;- IHt. 
      assert (forall A (l1 : list (list A)) (l2 : list (list A)),
                concat l1 ++ concat l2 = concat (l1 ++ l2)) as H.
        intros. induction l1; auto.
          simpl. rewrite &lt;- app_assoc. rewrite IHl1. auto.
      rewrite H. f_equal. rewrite map_app. reflexivity.
Qed.</code></pre>
</div>
<button id="ex-btn-4.2"></button>

<h1 id="exercise-5">Exercise 5</h1>
<p>Prove that standard <code>option</code> type with some operations form a semigroup, monoid, functor and monad. You need to find relevant operations.</p>
<p>What restriction you’ll need on <code>option</code>s type argument? (<code>A</code> in <code>option A</code>) Does it need to form a monoid for <code>option</code> to form a monoid?</p>
<div id="ex-div-5.1" class="solution">
<pre class="coq"><code>Definition map_option (A B : Type) (f : A -&gt; B) (opt : option A) :=
  match opt with
  | None =&gt; None
  | Some t =&gt; Some (f t)
  end.

Definition append_option A OpA (sg : semigroup A OpA) (a b : option A) : option A :=
  match a, b with
  | None, None =&gt; None
  | None, Some b&#39; =&gt; Some b&#39;
  | Some a&#39;, None =&gt; Some a&#39;
  | Some a&#39;, Some b&#39; =&gt; Some (OpA a&#39; b&#39;)
  end.

Theorem option_semigroup : forall A OpA (sg : semigroup A OpA),
  semigroup (option A) (append_option A OpA sg).
Proof.
  intros. apply Semigroup_intro. intros. destruct a1.
  + destruct a2.
    - destruct a3.
      * simpl. f_equal. inversion sg. apply H.
      * simpl. reflexivity.
    - destruct a3; simpl; reflexivity.
  + destruct a2; destruct a3; auto.
Qed.

Theorem option_monoid : forall A OpA (sg : semigroup A OpA),
  monoid (option A) (append_option A OpA sg) (option_semigroup A OpA sg) None.
Proof.
  intros. apply Monoid_intro. apply option_semigroup.
  intros. split. auto. destruct a; auto.
Qed.

Definition option_map A B (f : A -&gt; B) (o : option A) : option B :=
  match o with
  | None =&gt; None
  | Some a =&gt; Some (f a)
  end.

Theorem option_functor : functor option option_map.
Proof.
  apply Functor_intro; intros; destruct f; auto.
Qed.

Definition option_bind A B (o1 : option A) (f : A -&gt; option B) : option B :=
  match o1 with
  | None =&gt; None
  | Some a =&gt; f a
  end.

Theorem option_monad : monad option.
Proof.
  apply Monad_intro with (fmap := option_map) (lift := Some) (bind := option_bind).
  + apply option_functor.
  + intros. auto.
  + intros. destruct m; auto.
  + intros. destruct m; auto.
Qed.</code></pre>
</div>
<button id="ex-btn-5.1"></button>


<h1 id="exercise-6">Exercise 6</h1>
<p>I only have a partial solution to this one and it’s not strictly a Coq exercise, but it’s still fun :)</p>
<p>A <a href="http://en.wikipedia.org/wiki/Group_(mathematics)">group</a> is a monoid with inverse element of every element. In Coq syntax:</p>
<pre class="coq"><code>forall e, exists e_i -&gt; op e e_1 = U</code></pre>
<p>where <code>op</code> is monoid operation and <code>U</code> is unit of monoid.</p>
<p>Can you come up with a data structure that forms a group?</p>
<div id="ex-div-6.1" class="solution">

<p>Someone at Coq IRC channel suggested diffs. Do you think a diff could form a group? What would associative operation, unit element, and inverse elements be?</p>
<div id="ex-div-6.2" class="solution">
<p>Diffs don’t form a group. Composing two diffs is the merge operation, which is partial function. We can’t always merge two diffs. (merge conflicts)</p>
So I don’t have an answer to this exercise, If you know examples to this one, please write at comments :)
</div>
<button id="ex-btn-6.2"></button>

</div>
<button id="ex-btn-6.1"></button>



<script>
function showMsg(exNum) {
  return "Show solution (ex. " + exNum + ")";
}

function hideMsg(exNum) {
  return "Hide solution (ex. " + exNum + ")";
}

function setUpShowHide(exNum) {
  var div = document.getElementById("ex-div-" + exNum);
  var btn = document.getElementById("ex-btn-" + exNum);
  btn.innerHTML = showMsg(exNum);
  btn.onclick = function() {
    if (div.style.display !== 'none') {
      div.style.display = 'none';
      btn.innerHTML = showMsg(exNum);
    } else {
      div.style.display = 'block';
      btn.innerHTML = hideMsg(exNum);
    }
  }
  div.style.display = 'none';
}

setUpShowHide(1.1);
setUpShowHide(1.2);
setUpShowHide(2.1);
setUpShowHide(2.2);
setUpShowHide(3.1);
setUpShowHide(3.2);
setUpShowHide(4.1);
setUpShowHide(4.2);
setUpShowHide(5.1);
setUpShowHide(6.2);
setUpShowHide(6.1);
</script>

]]></summary>
</entry>
<entry>
    <title>A complicated proof of a simple theorem</title>
    <link href="http://osa1.net/posts/2014-07-10-complicated-proof-simple-thm.html" />
    <id>http://osa1.net/posts/2014-07-10-complicated-proof-simple-thm.html</id>
    <published>2014-07-10T00:00:00Z</published>
    <updated>2014-07-10T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>It’s that time of the year again in which I’m suddenly very interested about verification and interactive theorem proving. I’m currently working on proving properties of some well-known algorithms.</p>
<p>At least in the beginning even the simplest properties may turn out to be very hard to prove. In this post I’m going to give an example of a very simple theorem which turns out to be hard to prove for beginners like me.</p>
<p>Let’s say I want to divide peano nats by 10. I can define <code>div10</code> in two ways: 1) I can implement a function that directly divides by 10 2) I can implement <code>div2</code> and <code>div5</code> and use those two to implement <code>div10</code>:</p>
<pre class="coq"><code>(* first definition *)
Fixpoint div10 (n : nat) : nat :=
  match n with
  | S (S (S (S (S (S (S (S (S (S n&#39;))))))))) =&gt; S (div10 n&#39;)
  | _ =&gt; 0
  end.

(* second definition *)
Fixpoint div5 (n : nat) : nat :=
  match n with
  | S (S (S (S (S n&#39;)))) =&gt; S (div5 n&#39;)
  | _ =&gt; 0
  end.

Fixpoint div2 (n : nat) : nat :=
  match n with
  | S (S n&#39;) =&gt; S (div2 n&#39;)
  | _ =&gt; 0
  end.

Definition div10&#39; (n : nat) : nat := div5 (div2 n).</code></pre>
<p>It may be obvious enough that those two definitions are equal(in the sense that they always terminate and given same arguments they return same values) but as an exercise let’s try to prove it:</p>
<pre class="coq"><code>Theorem divs_eq : forall n, div10 n = div10&#39; n.</code></pre>
<p>It’s obvious that we need to do induction on n, but unfortunately that leads to very complicated induction hypothesis and I can’t make any use of them:</p>
<pre class="coq"><code>Proof.
  unfold div10&#39;. intro.
  do 10 (induction n; auto). simpl. f_equal. 
Abort.</code></pre>
<p>At this point goal is same as our original goal, but induction hypothesis are about a hundred lines long so I can’t make any use of it. (and I don’t understand why induction hypothesis getting that big)</p>
<p>It turns out that by using a different induction scheme we can easily prove this. Instead of using standard induction scheme of nats “prove P 0, assume P n and prove P (S n)” we can use “prove P 0, P 1, … P 9, assume P n and prove P (10 + n)”:</p>
<pre class="coq"><code>Definition nat10ind : forall (P : nat -&gt; Prop),
  P 0 -&gt; P 1 -&gt; P 2 -&gt; P 3 -&gt; P 4 -&gt; P 5 -&gt; P 6 -&gt; P 7 -&gt; P 8 -&gt; P 9 -&gt;
  (forall n, P n -&gt; P (10 + n)) -&gt; forall n, P n.

  intros.
  assert (P n /\ P (1 + n) /\ P (2 + n) /\ P (3 + n) /\ P (4 + n) /\ P (5 + n) /\
          P (6 + n) /\ P (7 + n) /\ P (8 + n) /\ P (9 + n)).
  + induction n.
    - simpl. repeat split; assumption.
    - repeat (match goal with H : _ /\ _ |- _ =&gt; destruct H end).
      repeat split; try assumption.
      apply H9. assumption.
  + destruct H10. assumption.
Defined.</code></pre>
<p>Now our main theorem is very easy to prove:</p>
<pre class="coq"><code>Theorem divs_eq : forall n, div10 n = div10&#39; n.
Proof.
  unfold div10&#39;. intro.
  induction n using nat10ind; auto.
  simpl. f_equal. assumption.
Qed.</code></pre>
<p>One thing that I found weird in <code>nat10ind</code> definition is that the expression <code>match goal with ...</code> is kind of special in that <code>goal</code> is not an identifier that represents some part of the context. Rather, <code>match goal with ...</code> is a special syntax to pattern match against the whole context, with hypothesis and goals<sup><a href="#fn1" class="footnoteRef" id="fnref1">1</a></sup>. <code>H : _ /\ _ |- _</code> part is then matching for any hypothesis with form <code>_ /\ _</code> and binding it to <code>H</code>, ignoring the current goal(RHS of turnstile shown as <code>|-</code>).</p>
<hr />
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>See <a href="http://coq.inria.fr/distrib/current/refman/Reference-Manual011.html#sec447">tacexpr_1 production</a> of Coq expression syntax in reference manual for details.<a href="#fnref1">↩</a></p></li>
</ol>
</div>]]></summary>
</entry>
<entry>
    <title>Proving soundness of simply typed multi-staged lambda-calculus</title>
    <link href="http://osa1.net/posts/2014-03-06-proving-simply-typed-multi-staged-lc.html" />
    <id>http://osa1.net/posts/2014-03-06-proving-simply-typed-multi-staged-lc.html</id>
    <published>2014-03-06T00:00:00Z</published>
    <updated>2014-03-06T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>First part of my first non-trivial(e.g. something other than a Software Foundations exercise) Coq program is finally done. I learned Coq from Software Foundations, but I didn’t finish to book. I still know only very basic tactics, which I think one of the reasons why my proofs are so long. You can see the source <a href="https://github.com/osa1/StagedLambda/blob/master/Lc.v">here</a>.</p>
<p>Some random notes about implementation:</p>
<ul>
<li>I’m looking to improve proofs. I have a lot more to implement for the rest of the program until deadline, so I may not be able to refactor current proofs very much, but at one point I want to simplify the proofs and use more advanced tactics.</li>
<li>Currently only <code>inversion</code>, <code>destruct</code>, <code>induction</code>, <code>rewrite</code>, <code>assumption</code>, <code>assert</code>, <code>constructor</code>, <code>auro</code>, <code>simpl</code>, <code>intro</code>/<code>intros</code>, <code>apply</code>, <code>right</code>/<code>left</code>, <code>exists</code>, <code>unfold</code>, <code>subst</code>, <code>reflexivity</code>, <code>remember</code> and <code>generalize</code> tactics are used.</li>
<li>Language definition is almost the same as in papers. There is one difference, we implemented substitutions as a function. In reality, substitution in multi-staged lambda-calculus is not a function. I believe this doesn’t effect correctness of theorems. At one point I’ll refactor the code and define substitution as a relation.</li>
<li>I used <code>Case</code>, <code>SCase</code>, <code>SSCase</code> … constructs from SFlib extensively.</li>
</ul>
<p>Now I’m going to implement lambda-calculus with row-polymorphic records. I expect this to be at least 2x harder, since polymorphism is involved. Let’s see how it goes …</p>]]></summary>
</entry>
<entry>
    <title>On proof automations -- part 3</title>
    <link href="http://osa1.net/posts/2013-09-23-proof-automation-3.html" />
    <id>http://osa1.net/posts/2013-09-23-proof-automation-3.html</id>
    <published>2013-09-23T00:00:00Z</published>
    <updated>2013-09-23T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>Over the past several days I learned few things that changed my ideas about proof automation. See my first two posts about proof automation here: <a href="/posts/2013-09-11-proof-automation.html">#1</a> <a href="/posts/2013-09-17-proof-automation-2.html">#2</a></p>
<p>First, it’s really possible to use proof automation without first writing the whole proof. After started working on theorems defined on more complex languages, I realized one thing: Most of the cases of inductions/inversions are trivial to solve. Generally you only interested in one or two cases, while you have in total 20 cases. Why not use automation to eliminate 18 cases automatically? Proving that parts manually only makes it more obscure.</p>
<p>Second thing I learned is that automations like <code>auto</code>, <code>eauto</code>, <code>iauto</code> never perform <code>rewrite</code>, <code>subst</code>, case analysis(<code>inversion</code>, <code>destruct</code>, maybe others?) and induction. I think this is very important point, because this means that your proofs still have to resemble the structure of your non-automated proofs(i.e. structure of your terms). Which prevents your proofs from being too much magical.</p>
<p>Now let’s work on an example to see how some automation is good thing. I’ll prove progress property of STLC(simply-typed lambda calculus), as defined in <a href="www.cis.upenn.edu/~bcpierce/sf/">Software Foundations</a> chapter <a href="http://www.cis.upenn.edu/~bcpierce/sf/Stlc.html">Stlc</a>.</p>
<p>Here’s an initial version without using any kind of automation.</p>
<pre class="coq"><code>Theorem progress_no_auto : forall t T,
     empty |- t \in T -&gt;
     value t \/ exists t&#39;, t ==&gt; t&#39;.
Proof.
  intros t T TD. remember (@empty ty).
  (* induction on typing derivation *)
  has_type_cases (induction TD) Case.
  Case &quot;T_Var&quot;.
    (* this case is impossible because vars are not well-typed in empty env,
       find a contradiction *)
    rewrite Heqp in H. inversion H.
  Case &quot;T_Abs&quot;.
    (* lambdas are values *)
    left. apply v_abs.
  Case &quot;T_App&quot;.
    (* well-typed applications should take a step *)
    right. destruct IHTD1. assumption.
    SCase &quot;t1 is a value&quot;. destruct IHTD2. assumption.
      SSCase &quot;t2 is a value&quot;.
        (* t1 should be a lambda *)
        inversion TD1; subst; inversion H. (* inversion on typing derivation of t1,
                                              most cases can be eliminated with inversion H,
                                              because t1 can only be lambda expression to be well-typed *)
        SSSCase &quot;t1 is tabs&quot;. eexists. apply ST_AppAbs. assumption.
      SSCase &quot;t2 can take a step&quot;.
        inversion H0. eexists. apply ST_App2. assumption. apply H1.
    SCase &quot;t1 can take a step&quot;. inversion H. eexists. apply ST_App1. apply H0.
  Case &quot;T_True&quot;. left. constructor.
  Case &quot;T_False&quot;. left. constructor.
  Case &quot;T_If&quot;.
    (* well-typed T_If can always take a step *)
    destruct IHTD1. assumption.
    SCase &quot;t1 is value&quot;. inversion H; subst.
      SSCase &quot;t1 is a lambda&quot;. inversion TD1.
      SSCase &quot;t1 is ttrue&quot;. right. exists t2. constructor.
      SSCase &quot;t1 is tfalse&quot;. right. exists t3. constructor.
    SCase &quot;t1 can take a step&quot;. inversion H; subst. right.
      exists (tif x0 t2 t3). constructor. assumption.
Qed.</code></pre>
<p>I leave comments to explain what’s going on. The point here is not to get shortest proof, but to show with some automation proofs could be more readable.</p>
<p>When you look to this proof you can realize that our interesting cases are:</p>
<ul>
<li><code>T_Var</code> case, in which we need to show that it’s impossible.</li>
<li><code>T_App</code> and <code>T_If</code> cases, in which we need to use induction hypothesis.</li>
</ul>
<p><code>T_Abs</code>, <code>T_True</code> and <code>T_False</code> cases are completely trivial and we don’t want to make our proof harder to write, read and maintain more.</p>
<p>Second thing to realize that, if you count <code>;</code> operator as a automation, then it makes me using automation even in that proof. In the case of <code>T_App</code>, I’m eliminating all cases of <code>t1</code>(because for <code>T_App</code> to be well-typed, <code>t1</code> can only be a lambda abstraction – other cases should be eliminated with contradictions) with <code>inversion H; subst.</code>. Without <code>;</code> operator, I would have to write this:</p>
<pre class="coq"><code>inversion TD1; subst.
SSSCase &quot;t1 is tvar&quot;. inversion H.
SSSCase &quot;t1 is tabs&quot;. eexists. apply ST_AppAbs. assumption.
SSSCase &quot;t1 is tapp&quot;. inversion H.
SSSCase &quot;t1 is tif&quot;. inversion H.</code></pre>
<p>and when the language gets bigger, more cases with just <code>inversion H</code> would follow.</p>
<p>Now here’s an automated version:</p>
<pre class="coq"><code>Theorem progress : forall t T,
     empty |- t \in T -&gt;
     value t \/ exists t&#39;, t ==&gt; t&#39;.
Proof with eauto.
  intros t T TD. remember (@empty ty). has_type_cases (induction TD) Case; auto.
  Case &quot;T_Var&quot;. rewrite Heqp in H. inversion H.
  Case &quot;T_App&quot;. right. destruct IHTD1...
    SCase &quot;t1 is a value&quot;. destruct IHTD2...
      SSCase &quot;t2 is a value&quot;. inversion TD1; subst; try (solve by inversion)...
      SSCase &quot;t2 can take a step&quot;. inversion H0; subst...
    SCase &quot;t1 can take a step&quot;. inversion H; subst...
  Case &quot;T_If&quot;. right. destruct IHTD1. assumption.
    SCase &quot;t1 is a value&quot;. inversion H; subst; eauto.
      SSCase &quot;t1 is tabs&quot;. solve by inversion.
    SCase &quot;t1 can take a step&quot;. inversion H...
Qed.</code></pre>
<p>IMO this proof is a lot more simple and readable. It eliminates all trivial cases and only leave us with interesting ones. And even in that cases we simply finished with <code>...</code> or explicitly running <code>auto</code>/<code>eauto</code>.</p>]]></summary>
</entry>
<entry>
    <title>On proof automations -- part 2</title>
    <link href="http://osa1.net/posts/2013-09-17-proof-automation-2.html" />
    <id>http://osa1.net/posts/2013-09-17-proof-automation-2.html</id>
    <published>2013-09-17T00:00:00Z</published>
    <updated>2013-09-17T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p><a href="http://adam.chlipala.net/">Adam Chilpala’s</a> <a href="http://adam.chlipala.net/cpdt/">Certified Programming with Dependent Types</a> has a great discussion on writing automated proofs and readability of proofs. Highly recommended. Chapter 16(page 319 on current revision).</p>
<p>I think one key thing to realize about interactive theorem proving is that there is no perfect, or maybe even good, solution to the problem of proof representation. Tactics help, but without automation tactic based proofs are just too fragile(every change in definitions implies changes in proofs) and verbose. But automated proofs are very hard to read, and it’s very hard to modify automated proofs for changes. CPDT book has a great example of this. Let’s think of this proof(I’ll skip the definitions, just think some simple arithmetic expressions language just like you could find in any elementary programming language theory book)</p>
<pre class="coq"><code>Theorem eval_times : forall k e,
    eval (times k e) = k * eval e.
Proof.
  induction e as [| ? IHe1 ? IHe 2]; [
    trivial
    | simpl; rewrite IHe1; rewrite IHe2; rewrite mult_plus_distr_l; trivial ].
Qed.</code></pre>
<p>And now let’s say you added one more constructor to arithmetic expression syntax. How to change this proof to adopt the changes? I don’t think there’s a way to do that without first writing the version without automation. And that brings us back to my first post.</p>
<p>I think the reason why I think too much about this stuff instead of writing some proofs is that I look for elegance and simplicity in my programs. And only if necessary I try to make my programs efficient(fast, small, whatever). After some simple trivial proofs that I did for learning I never found my proofs satisfactory. They are either very very long and repetitive or very hard to understand and modify. It’s very hard to find a sweet-spot in proofs that is not very repetitive and long and still possible to read and understand.</p>
<p>You can always apply methods you learned to make your programs easier to read and modify, like moving some code to a new function with a useful name and formal parameters etc. but in the case of proofs it’s also very hard to find what pieces are considered worthy to be a lemma and what would be useful name for it.</p>
<p>My ideas about proof automation is changed from “no way I use them” to “okay they can be very useful for eliminating repetition without adding obscurity to the proof” in one day. I’ll continue writing Coq proofs for while and probably add new posts on this topic later.</p>]]></summary>
</entry>
<entry>
    <title>On proof automations</title>
    <link href="http://osa1.net/posts/2013-09-11-proof-automation.html" />
    <id>http://osa1.net/posts/2013-09-11-proof-automation.html</id>
    <published>2013-09-11T00:00:00Z</published>
    <updated>2013-09-11T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>While working with constructive systems, at the lowest level you can directly construct proof objects by writing programs. But for realistic scenarios, this is probably too hard to do. Coq and Idris like languages provide an alternative way to construct proof objects: tactics. Tactics provide a way to derive proofs in a way that is somewhat like informal proofs given in spoken languages. But theorems you’re trying to prove is already too complex, and in some parts, very repetitive.</p>
<p>That <em>tactics</em> Coq like theorem provers provide also help you construct proofs with some automation commands. For instance, you can apply same steps several times, until you prove some subgoals. Or you can search for a proof by applying some steps in different combinations.</p>
<p>What doesn’t make sense to me is that in general you cannot really use that automation methods before manually writing long and hard proof by hand and showing that your theorem is really a theorem and it indeed has some repetitive parts.</p>
<p>And at this point you’re probably better off leaving the proof as-is because replacing explicit steps with some magic commands(I’ll come to this point later) only makes the proof harder to read, without any other advantages.</p>
<p>I have similar ideas for <code>auto</code> tactic. What it does is that it searches for a proof by applying different combinations of <code>apply</code> and <code>intros</code> tactics(with some limitations – it only uses apply for hypothesis and hint database which you generate while defining inductive definitions).</p>
<p>Now my problem with <code>auto</code> is that you can only use it when you absolutely sure that your theorem is indeed a theorem – that is, it has a proof. And how can you know that your theorem is correct? The only way to know this is to write a proof for it. Now let’s say <code>auto</code> tactic fails, and when this happens it can’t really say anything. Why did it fail? It may be because your proof is actually too long and it searched for a while but couldn’t find a proof(you can specify search depth as an optional parameter of <code>auto</code> tactic). Or maybe your hint base is not good enough. Or maybe your theorem is not actually a theorem and you cannot prove it.</p>
<p>So both proof search tactics and repetition elimination tactics have this same common problem: you cannot know that they work before writing the proof itself. And automation generally makes the proof unreadable.</p>
<p>Adam Chilpala’s “Certified Programming with Dependent Types” book encourages that automated way of proving. You have that complicated theorem that you don’t even understand what it is saying? No problem, <code>crush</code> tactic(which is not standard, distributed with CPDT book) will prove it for you in one command.</p>
<p>I’m not saying that <code>crush</code> like tactics are necessarily bad and you shouldn’t use it. I’d probably use it in real world when I need a proof that <code>crush</code> can generate and I don’t have time or motivation to do it myself.</p>
<p>And repetition elimination tactics are useful when you have a syntax tree with 15 constructors and you’re proving some theorem that’s relevant with only one of that constructors – 14 subgoals can be easily proved by some clever use of <code>;</code> operator in Coq.</p>
<p>What I’m saying that is several things:</p>
<ul>
<li>Automated proofs can make proofs harder to read and most of the time you already need to have written the long proof to see that it has repetitive parts or at least it is provable. (certainly there are cases where you’ll find yourself trying to prove something wrong)</li>
<li>Once a long proof is written, there is little or no need to remove that proof and replace it with an automated version. (one reason to do that is to have same proof working even after some inductive definitions is expanded/changed)</li>
<li>Most importantly, <code>crush</code> like automation tactics are pure black magic and doesn’t really teach you anthing. As a hobbyist who self-study all this, I prefer learning the principles instead of scripts and magic commands and black boxes.</li>
</ul>
<p>For this reasons I’m trying to use proof automation as little as possible.</p>]]></summary>
</entry>

</feed>
