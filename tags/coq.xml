<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>osa1.net - Posts tagged coq</title>
    <link href="http://osa1.net/tags/coq.xml" rel="self" />
    <link href="http://osa1.net" />
    <id>http://osa1.net/tags/coq.xml</id>
    <author>
        <name>Ömer Sinan Ağacan</name>
        <email>omeragaca@gmail.com</email>
    </author>
    <updated>2014-09-23T00:00:00Z</updated>
    <entry>
    <title>Two challenges for dependently typed languages</title>
    <link href="http://osa1.net/posts/2014-09-23-two-challenges-for-dep-typed-langs.html" />
    <id>http://osa1.net/posts/2014-09-23-two-challenges-for-dep-typed-langs.html</id>
    <published>2014-09-23T00:00:00Z</published>
    <updated>2014-09-23T00:00:00Z</updated>
    <summary type="html"><![CDATA[<pre class="coq"><code>Require Import Fin.
Require Import Arith.
Require Import Omega.
Require Import Program.</code></pre>
<p>I propose two challenges that I think are very useful to see how easy to use a dependently typed language is. Different dependently typed languages use different type theories as their trusted cores and admit different axioms. In my experience, this significantly effects user experience. For example, lack of dependent pattern matching in Coq is leading to horribly verbose, hard to write, read and understand pattern matching code(see convoy and transport patterns).</p>
<p>Adding some extra axioms to Coq’s trusted but sometimes too simple core can make the experience significantly better(see <a href="http://coq.inria.fr/distrib/current/stdlib/Coq.Logic.JMeq.html">JMec</a>, <a href="http://coq.inria.fr/distrib/current/stdlib/Coq.Program.Equality.html">Program</a> and <a href="http://coq.inria.fr/distrib/current/refman/Reference-Manual010.html">dependent destruction</a>). On the other hand, languages like Idris handles some of the cases that Coq can’t handle by default.</p>
<p>The challenge is to solve two problems defined below in different dependently typed languages. We can then compare programs for 1) simplicity of code 2) use of extra axioms 3) giving away totality or some other useful properties.</p>
<p>Solutions of one of the challenges may also be compared for erasure of types. Problems are defined in Coq.</p>
<h1 id="challenge-1-cardinality">Challenge 1: Cardinality</h1>
<p>This is a theorem proving exercise. <code>Cardinality</code> of a type is defined as bijection of the type with <code>Fin.t n</code> for some <code>n</code>:</p>
<pre class="coq"><code>Inductive cardinality (A : Type) (n : nat) : Prop :=
| cardinality_intro
    (to_fin    : A -&gt; Fin.t n)
    (from_fin  : Fin.t n -&gt; A)
    (bijection :
      (forall x, from_fin (to_fin x) = x) /\ (forall y, to_fin (from_fin y) = y)).</code></pre>
<p>You may want to solve this simple exercise just to warm-up:</p>
<pre class="coq"><code>Theorem cardinality_bool : cardinality bool 2.
Admitted.</code></pre>
<p>Now the challenge is to prove that cardinality of</p>
<pre class="coq"><code>Inductive T (A : Type) : Type :=
| T1 : T A
| T2 : A -&gt; T A
| T3 : A -&gt; A -&gt; T A.</code></pre>
<p>.. is <code>1 + N + N * N</code> where <code>N</code> is cardinality of <code>A</code>:</p>
<pre class="coq"><code>Theorem cardinality_T : forall A N,
  cardinality A N -&gt; cardinality (T A) (1 + N + N * N).
Admitted.</code></pre>
<p><strong>UPDATE:</strong> First solution for this challange came from <a href="http://homes.cs.washington.edu/~jrw12/">jrw</a>, in <a href="http://lpaste.net/111568">Coq</a>. UPDATE: He also wrote an awesome blog post: <a href="http://homes.cs.washington.edu/~jrw12/more-cardinality.html">Reasoning about Cardinalities of Sums and Products</a>.</p>
<p><strong>UPDATE 2:</strong> See comments for more solutions.</p>
<h1 id="challenge-2-verified-coinductive-definition-of-pascals-triangle">Challenge 2: Verified, CoInductive definition of Pascal’s Triangle</h1>
<p>This should be easier. What’s good about this one is that we can compare languages for performance and/or erasure properties. Basically we expect type arguments to be absent in extracted/executed code.</p>
<p>Here’s one way to define the problem:</p>
<pre class="coq"><code>CoInductive triangle_t (T : Type) : nat -&gt; Type :=
| triangle : forall (n : nat), Vector.t T n -&gt; triangle_t T (S n) -&gt; triangle_t T n.

Definition pascal : triangle_t nat 0. Admitted.

Definition pascal_nth (row col : nat) : col &lt;= row -&gt; nat. Admitted.</code></pre>
<p>Extracted/executed version of [pascal_nth] and any auxiliary function should be clean from type arguments.</p>
<p>Now the verification part:</p>
<pre class="coq"><code>Theorem pascal_first_col_is_1 : forall row,
  pascal_nth row 0 (le_0_n row) = 1.
Admitted.

Theorem pascal_last_col_is_1 : forall n,
  pascal_nth n n (le_n n) = 1.
Admitted.

Lemma pascal_correct_aux : forall row col,
  col &lt; row -&gt;
  row &gt;= 1 -&gt;
  col &gt;= 1 -&gt;
  col - 1 &lt;= row - 1.
Proof. intros. omega. Qed.

Lemma lt_le : forall a b,
  a &lt; b -&gt; a &lt;= b.
Proof. intros. omega. Qed.

Lemma pascal_correct_aux&#39; : forall row col,
  col &lt; row -&gt;
  row &gt;= 1 -&gt;
  col &gt;= 1 -&gt;
  col &lt;= row - 1.
Proof. intros. omega. Qed.

Theorem pascal_correct : forall row col
  (pf : col &lt; row)
  (pf_row : row &gt;= 1)
  (pf_col : col &gt;= 1),
  pascal_nth row col (lt_le col row pf) =
      pascal_nth (row - 1) (col - 1) (pascal_correct_aux row col pf pf_row pf_col)
    + pascal_nth (row - 1) col (pascal_correct_aux&#39; row col pf pf_row pf_col).
Admitted.</code></pre>
<p>Feel free to change the definitions. The point is to 1) have strict [nth] function that fail 2) prove that the data structure has property of <a href="http://en.wikipedia.org/wiki/Pascal%27s_triangle">Pascal’s triangle</a>.</p>
<p>I’m hoping to come up with some solutions given in Coq over the next couple of weeks.</p>]]></summary>
</entry>
<entry>
    <title>Problems with tactic generated programs</title>
    <link href="http://osa1.net/posts/2014-09-13-problems-with-tactics.html" />
    <id>http://osa1.net/posts/2014-09-13-problems-with-tactics.html</id>
    <published>2014-09-13T00:00:00Z</published>
    <updated>2014-09-13T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>I never managed to write anything with dependent types in Coq, without using tactics. Pattern matching is never giving enough information in cases to allow me to generate the term I want. <a href="http://adam.chlipala.net/cpdt/">Certified Programming with Dependent Types</a> book describes “Convoy pattern”, which is a way to write dependently typed terms in Coq without using tactics. I later learned that standard <a href="http://coq.inria.fr/distrib/current/refman/Reference-Manual010.html#hevea_tactic87">inversion</a> tactic also uses something like a convoy pattern, except it’s not as fine grained as what a user would write by hand.</p>
<p>Unfortunately, even after learning about convoy pattern, I’m still not satisfied with what I get when I write dependently typed definitions without using tactics. The reason is that convoy pattern is incredibly ugly and hard to read and understand. Even if I embrace the ugliness, most of the time I have no idea how to use the pattern so that 1) it’s not horribly verbose 2) does what I want.</p>
<p>I had a small verification idea: I was going to implement <a href="http://en.wikipedia.org/wiki/Pascals_triangle">Pascal’s triangle</a> as a co-inductive type and implement operations on it. Later I was hoping to prove that <code>p(n, k) = p(n-1, k-1) + p(n-1, k)</code> where <code>k</code> is column and <code>p</code> is row.</p>
<p>I struggled for this for a long time. The problem was that I was not comfortable with tactics and I couldn’t define dependently typed terms using fixpoints or definitions. Now that I’m comfortable enough with tactics, I finally managed to <a href="https://gist.github.com/osa1/47ed1dd4267fa379259d">implement what I want</a>.</p>
<p>Here are a few things that I also mentioned in <a href="https://sympa.inria.fr/sympa/arc/coq-club/2014-09/msg00088.html">my mail to Coq-club</a>:</p>
<ul>
<li>The whole thing looks a bit complex but actually implementing it was very easy. Definitely a lot easier than how it’d be if I wanted to implement it without tactics.</li>
<li>Even though generated Coq terms are huge, generated OCaml is actually very good. It still has some problems though. <code>snoc</code> has a redundant argument which could be eliminated by using OCaml lists for vectors. First argument of <code>sum-pairs</code> is redundant because it’s just vector’s length so we could use <code>Vector.t</code>’s field instead. Similarly, I think <code>pascal_aux</code>’s first argument may also be removed etc. (see generated OCaml from the Github link above)</li>
</ul>
<p>If you look at the Coq code, you’ll realize that I couldn’t prove even the simplest fact about my definition. This is the problem with tactic-generated terms. The reason I can’t prove anything is that <code>simpl</code> just doesn’t work anymore, and there are no workarounds. The terms are so huge and complex, nothing is provable anymore.</p>
<p>I got very good responses about alternatives and problems in Coq-club mailing list. One of them was the suggestion of giving functions types that 1) show the properties I’m trying to show in separate theorems 2) still subject to erasure.</p>
<p>This approach has an obvious problem. There won’t be a program/proof distinction anymore. As a programmer I don’t like this at all. Also, making sure that proof terms will be erased is hard.(see rest of the discussion from the mailing list link above) I don’t think I’ll follow this idea.</p>
<p>Another alternative is just using Agda. I have my problems with Agda which I’m deferring to another post for now. Some of my excuses may actually not be Agda’s problem but rather they may be Coq’s advantage. In any case, I probably won’t be using Agda.</p>
<p>So now I’m stuck with Coq – I can’t define anything without using tactics, but when I use tactics for definitions(instead of proofs) then I can’t prove anything.</p>]]></summary>
</entry>
<entry>
    <title>Proving sorting algorithms correct</title>
    <link href="http://osa1.net/posts/2014-09-08-proving-sorting-correct.html" />
    <id>http://osa1.net/posts/2014-09-08-proving-sorting-correct.html</id>
    <published>2014-09-08T00:00:00Z</published>
    <updated>2014-09-08T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>I’ve been working on proving some sorting algorithms correct in Coq. After 600 lines of Coq proofs, I managed to prove correctness of <a href="http://en.wikipedia.org/wiki/Insertion_sort">insertion sort</a>, <a href="http://en.wikipedia.org/wiki/Selection_sort">selection sort</a>, and a weird sorting algorithm called <a href="http://austingwalters.com/everyday-algorithms-pancake-sort/">pancake sort</a>. This post is a <a href="https://github.com/osa1/blog/blob/master/theories/SortingCorrect.v">literate Coq file</a> which you can download and execute in a Coq IDE, step by step. To keep the post shorter and easier to read and make it more like an exercise, I hided most of the proofs in HTML version.</p>
<p>Goals of this post are:</p>
<ul>
<li>Provide some guidance for starters who are interested in proving algorithms correct.</li>
<li>Demonstrate how to prove properties of functions with accumulators and functions that work on list indexes, instead of elements of the list.</li>
<li>Demonstrate how to use “timeout” arguments to convince Coq that a function is really terminating on all inputs.</li>
</ul>
<p>A note before starting: I deliberately ignored advanced proof automation tools and go with more primitive way of proving. This is for two reasons: 1) I don’t like magic, and since I don’t understand underlying mechanics of advanced proofs tactics like [crush], it’s magic to me 2) They’re sometimes so powerful, they get in your way to understand what’s really happening in the proof.</p>
<p>If you’re reading this post as an exercise and filling the proofs yourself, that should not be a problem for you. Otherwise you may find my proofs more verbose than what’s necessary.</p>
<p>Let’s start with standard stuff: imports. These are for some list helpers, list syntax, Peano definitions + functions etc. Just standard stuff. Only exception is the [Permutation] library, which I’ll soon explain why it’s necessary.</p>
<pre class="coq"><code>Require Import List.
Import ListNotations.
Open Scope list_scope.
Require Import Arith.
Require Import Omega.
Require Import Permutation.</code></pre>
<p>Our goal is to define some sorting algorithms and then prove them correct, but for that we first need to define what we mean by for a sorting algorithm to be “correct”. For some functions definition of correctness may be tricky to give, but in the context of sorting I think it’s obvious:</p>
<ul>
<li>Result should be a permutation of it’s parameter.</li>
<li>Result should be in sorted order.</li>
</ul>
<p>Definition of a “permutation” is defined in the [Permutation] library we’ve just included, and it comes with lots of very useful lemmas. I suggest you to go to [Permutation] library’s documentation(you can go to the docs by clicking [Permutation] link in the imports part above) and just skim through the definition and lemmas, and convince yourself that the definition is really enough to show that two lists are really a permutation.</p>
<p>For being sorted, we need to define what does that mean.</p>
<pre class="coq"><code>Inductive sorted : list nat -&gt; Prop :=
| Sorted_nil : sorted []
| Sorted_singleton : forall e, sorted [e]
| Sorted_cons : forall e h t, sorted (h :: t) -&gt; e &lt;= h -&gt; sorted (e :: h :: t).
(* begin hide *)
Hint Constructors sorted.
(* end hide *)</code></pre>
<p>Again, convince yourself that <code>sorted l</code> really encodes what we intuitively know about being sorted. Main property that should hold for any <code>l</code> that is <code>sorted l</code> is that for any consecutive elements <code>a</code> and <code>b</code> in the list, <code>a &lt;= b</code> should hold, which our third constructor shows.</p>
<p>Using <code>Permutation</code> and <code>sorted</code>, we can define what does being correct for a sorting algorithm mean:</p>
<pre class="coq"><code>Inductive Sorting_correct (algo : list nat -&gt; list nat) :=
| sorting_correct_intro :
    (forall l, sorted (algo l) /\ Permutation l (algo l)) -&gt; Sorting_correct algo.</code></pre>
<p>Note that in both definition of <code>sorted</code> and <code>Sorting_correct</code>, we used list of nats instead of any lists of ordered elements. This is really just to make definitions and proofs simpler. Generalization may be done as an exercise.</p>
<p>Now with those definitions, we can prove our first and easiest-to-prove algorithm.</p>
<h1 id="insertion-sort">Insertion sort</h1>
<p>We define insertion sort as a right fold:</p>
<pre class="coq"><code>Fixpoint insert (i : nat) (l : list nat) :=
  match l with
  | []     =&gt; [i]
  | h :: t =&gt; if leb i h then i :: h :: t else h :: insert i t
  end.

Definition insertion_sort := fold_right insert [].
(* begin hide *)
Hint Unfold insertion_sort.
(* end hide *)</code></pre>
<p>Now if we could show that <code>insert h t</code> on list <code>h :: t</code> returns a permutation, we could easily show that insertion sort returns a permutation, because all it does is to call <code>insert</code> on list elements, using <code>fold_right</code>.</p>
<pre class="coq"><code>(** ***** Exercise: 1 star. *)
Lemma insert_perm : forall h t, Permutation (insert h t) (h :: t).
(* begin hide *)
Proof.
  intros. induction t as [|h&#39; t&#39;]; auto.
  simpl. destruct (leb h h&#39;); auto.
  apply perm_trans with (h&#39; :: h :: t&#39;); auto. apply perm_swap.
Qed.
(* end hide *)

(** *****
  Exercise: 2 stars.
  Use [insert_perm] and transitivity, symmetry and reflexivity of [Permutation]. *)
Theorem Permutation_insertion_sort: forall l, Permutation l (insertion_sort l).
(* begin hide *)
Proof.
  intros. induction l as [|h t]; auto.
  assert (Permutation (h :: t) (h :: insertion_sort t)); auto.
  simpl. rewrite H. rewrite insert_perm. reflexivity.
Qed.
(* end hide *)</code></pre>
<p>To show that it returns sorted, we use a similar approach. We first show that if <code>sorted l</code>, then <code>sorted (insert e l)</code>. Empty and singleton lists are sorted by definition. Using these facts, we can easily prove that insertion sort really return sorted.</p>
<pre class="coq"><code>(** *****
  Exercise: 2 star.
  You need to use some lemmas about [leb] that are already included. Use [SearchAbout leb]. *)
Lemma insert_sorted_preserve : forall e l,
  sorted l -&gt; sorted (insert e l).
(* begin hide *)
Proof.
  intros e l h. induction h; simpl; auto with *.
  + remember (leb e e0) as bh. symmetry in Heqbh. destruct bh.
    - apply leb_complete in Heqbh. auto.
    - apply leb_complete_conv in Heqbh. auto with *.
  + simpl. remember (leb e e0) as bh. symmetry in Heqbh. destruct bh.
     - apply leb_complete in Heqbh. auto.
     - remember (leb e h) as bh&#39;. symmetry in Heqbh&#39;. destruct bh&#39;.
       * apply leb_complete in Heqbh&#39;. apply leb_complete_conv in Heqbh. auto with *.
       * apply leb_complete_conv in Heqbh. constructor; auto.
         simpl in IHh. rewrite Heqbh&#39; in IHh. apply IHh.
Qed.
Hint Resolve insert_sorted_preserve.
(* end hide *)

(** ***** Exercise: 2 star. Use [insert_sorted_preserve]. *)
Theorem insertion_sort_sorted : forall l, sorted (insertion_sort l).
(* begin hide *)
Proof.
  intros. induction l as [|h t]; simpl; auto.
Qed.
(* end hide *)</code></pre>
<p>With permutation and sorted properties proved, we can show that our insertion sort implementation is correct:</p>
<pre class="coq"><code>Theorem insertion_sort_correct : Sorting_correct insertion_sort.
Proof.
  constructor. split. apply insertion_sort_sorted. apply Permutation_insertion_sort.
Qed.</code></pre>
<h1 id="selection-sort">Selection sort</h1>
<p>Next, we move to selection sort. This one is significantly harder and most stuff we define in this section will be used in the next section.</p>
<p>Before implementing the algorithm, we need some auxiliary functions. <code>find_min_idx</code> is both used in selection sort and pancake sort. As you can imagine from the name, it returns index of one of the smallest elements in a list.</p>
<p>Note that it’s very very hard to reason about this definition. The reason is that we’re using index of a list, instead of head/tail of it. Simple induction-based proofs that we do before simply don’t work on this definition.</p>
<pre class="coq"><code>Fixpoint find_min_idx_aux (l : list nat) (min min_idx cur_idx : nat) : nat :=
  match l with
  | []     =&gt; min_idx
  | h :: t =&gt; if leb h min then find_min_idx_aux t h cur_idx (S cur_idx)
                           else find_min_idx_aux t min min_idx (S cur_idx)
  end.

Definition find_min_idx (l : list nat) : nat :=
  match l with
  | []     =&gt; 0
  | h :: t =&gt; find_min_idx_aux t h 0 1
  end.</code></pre>
<p>An invariant of <code>find_min_idx</code>:</p>
<pre class="coq"><code>(** ***** Exercise: 1 star. *)
Lemma find_min_idx_len_inv : forall l m mi ci r,
  find_min_idx_aux l m mi ci = r -&gt;
  mi &lt; ci -&gt;
  r &lt; ci + length l.
(* begin hide *)
Proof.
  induction l as [|h t].
  + intros. simpl in H. omega.
  + intros. simpl in *. destruct (leb h m); apply IHt in H; omega.
Qed.
(* end hide *)</code></pre>
<p>Make sure you understand what it’s saying. We need this lemma to eliminate some impossible cases in proofs.</p>
<p>We also need a “membership” predicate. This is used to express the idea of “for all elements in the list …”, which is expressed in Coq like <code>forall e, member e l -&gt; ...</code>.</p>
<pre class="coq"><code>Inductive member {A} : A -&gt; list A -&gt; Prop :=
| Member_head : forall e l, member e (e :: l)
| Member_tail : forall e h t, member e t -&gt; member e (h :: t).
(* begin hide *)
Hint Constructors member.
(* end hide *)</code></pre>
<p>Again, convince yourself that this really expresses that idea. Here’s a property of member:</p>
<pre class="coq"><code>(** ***** Exercise: 1 star. *)
Lemma member_preserved_by_perm : forall A (l : list A) l&#39; e,
  Permutation l l&#39; -&gt;
  member e l -&gt;
  member e l&#39;.
(* begin hide *)
Proof.
  intros A l l&#39; e H. induction H; auto.
  + intro. inversion H0; auto.
  + intro. inversion H; auto. inversion H2; auto.
Qed.
(* end hide *)</code></pre>
<p>Now using <code>member</code>, we can express correctness of <code>find_min_idx</code> and prove it. Note that this is a very hard exercise. I couldn’t solve it for a week, and even after that period I couldn’t solve it without help from Coq IRC channel. Feel free to cheat by looking the source of this post.</p>
<pre class="coq"><code>(* begin hide *)
Lemma find_min_idx_aux_ret_ge_mi :
  forall l m mi ci r,
    find_min_idx_aux l m mi ci = r -&gt;
    mi &lt;= ci -&gt;
    mi &lt;= r.
Proof.
  induction l as [|h t].
  + intros. simpl in H. omega.
  + intros. simpl in H. destruct (leb h m); apply IHt in H; omega.
Qed.

Lemma find_min_idx_aux_correct_same :
  forall l m mi ci,
    mi &lt; ci -&gt;
    find_min_idx_aux l m mi ci = mi -&gt;
    forall e,
      member e l -&gt;
      m &lt;= e.
Proof.
  intro. remember (length l) as len. generalize dependent l. induction len as [|len&#39;].
  + intros. destruct l. inversion H1. inversion Heqlen.
  + intros.
    (* l should be in form h :: t with length t = len&#39; *)
    destruct l as [|h t]; inversion Heqlen; clear Heqlen.

    simpl in H0. destruct (leb h m) eqn: eq.
    - pose proof (find_min_idx_aux_ret_ge_mi t h ci (S ci) mi H0).
      assert (ci &lt;= S ci) by omega. apply H2 in H4. omega.
    - inversion_clear H1.
      * apply leb_complete_conv in eq. omega.
      * apply IHlen&#39; with (l := t) (mi := mi) (ci := S ci); auto.
Qed.

Lemma find_min_idx_aux_cases :
  forall l r m mi ci,
    find_min_idx_aux l m mi ci = r -&gt;
    r = mi \/ ci &lt;= r.
Proof.
  induction l as [|h t].
  + intros. simpl in H. omega.
  + intros. simpl in H. destruct (leb h m) eqn: eq.
    - apply IHt in H. inversion H; right; omega.
    - apply IHt in H. inversion H; auto. right. omega.
Qed.

Lemma find_min_idx_aux_correct_succ_le_min :
  forall l n m mi ci,
    find_min_idx_aux l m mi ci = n + ci -&gt;
    mi &lt; ci -&gt;
    nth n l 0 &lt;= m.
Proof.
  induction l as [|h t].
  + intros. destruct n; simpl; omega.
  + intros. simpl in H. destruct (leb h m) eqn: eq.
    - destruct n as [|n&#39;].
      * apply leb_complete in eq. auto.
      * simpl. apply leb_complete in eq.
        subst.  pose proof (IHt n&#39; h ci (S ci)).
        assert (S n&#39; + ci = n&#39; + S ci) by omega.
        rewrite H2 in H. apply H1 in H; omega.
    - destruct n as [|n&#39;].
      * apply find_min_idx_aux_cases in H. inversion H; omega.
      * simpl. apply IHt with (mi := mi) (ci := S ci); omega.
Qed.

Lemma find_min_idx_aux_correct_succ :
  forall l n m mi ci,
    find_min_idx_aux l m mi ci = n + ci -&gt;
    mi &lt; ci -&gt;
    forall e,
      member e l -&gt;
      nth n l 0 &lt;= e.
Proof.
  induction l as [|h t].
  + intros. inversion H1.
  + intros. simpl in H. destruct (leb h m) eqn: eq.
    - inversion_clear H1.
      * simpl. destruct n as [|n&#39;]. reflexivity.
        apply find_min_idx_aux_correct_succ_le_min with (mi := ci) (ci := S ci); omega.
      * simpl. destruct n as [|n&#39;].
        { apply find_min_idx_aux_correct_same with (l := t) (mi := ci) (ci := S ci);
          try omega; auto. }
        { apply IHt with (m := h) (mi := ci) (ci := S ci); try omega; auto. }
    - inversion_clear H1.
      * simpl. destruct n as [|n&#39;]. reflexivity.
        assert (S n&#39; + ci = n&#39; + S ci) by omega. rewrite H1 in H; clear H1.
        pose proof (find_min_idx_aux_correct_succ_le_min t n&#39; m mi (S ci) H).
        assert (mi &lt; S ci) by omega. apply H1 in H2; clear H1.
        SearchAbout leb. apply leb_complete_conv in eq. omega.
      * simpl. destruct n as [|n&#39;].
        { simpl in H. pose proof (find_min_idx_aux_cases t ci m mi (S ci) H).
          inversion H1; omega. }
        { apply IHt with (m := m) (mi := mi) (ci := S ci); try omega; auto. }
Qed.
(* end hide *)
(** ***** Exercise: 5 stars. [find_min_idx] really returns index of smallest element. *)
Lemma find_min_idx_correct : forall n l,
  find_min_idx l = n -&gt;
  (forall e, member e l -&gt; nth n l 0 &lt;= e).
(* begin hide *)
Proof.
  destruct l as [|h t].
  + intros. inversion H0.
  + intros. destruct n as [|n&#39;].
    - simpl in H. simpl. inversion H0.
      * reflexivity.
      * apply find_min_idx_aux_correct_same with (l := t) (mi := 0) (ci := 1); auto.
    - simpl in H. simpl. inversion_clear H0.
      * apply find_min_idx_aux_correct_succ_le_min with (mi := 0) (ci := 1); omega.
      * apply find_min_idx_aux_correct_succ with (m := h) (mi := 0) (ci := 1); try omega; auto.
Qed.
(* end hide *)</code></pre>
<p>Now, in selection sort, we run several operations on argument. One of them is <code>find_min_idx</code> which we already defined. Another one of them is <code>replace</code> operation, which replaces nth element in a list with given element. Here’s a definition and some properties:</p>
<pre><code>Fixpoint replace {A} (l : list A) (idx : nat) (e : A) : list A :=
  match l with
  | [] =&gt; []
  | h :: t =&gt;
      match idx with
      | 0 =&gt; e :: t
      | S idx&#39; =&gt; h :: replace t idx&#39; e
      end
  end.

(** ***** Exercise: 1 star. First part of correctness proof of [replace]. *)
Lemma replace_nth : forall l i e l&#39;,
  i &lt; length l -&gt;
  replace l i e = l&#39; -&gt;
  nth i l&#39; 0 = e.
(* begin hide *)
Proof.
  induction l as [|h t]; intros.
  + inversion H.
  + simpl in *. destruct i as [|i&#39;]; subst; auto.
    - apply lt_S_n in H. simpl. apply IHt; auto.
Qed.
(* end hide *)

(** ***** Exercise: 1 star.
    Second part of correctness proof of [replace]. [replace] preserves length of the list. *)
Lemma replace_len : forall A l i (e : A),
  i &lt; length l -&gt;
  length l = length (replace l i e).
(* begin hide *)
Proof.
  induction l as [|h t]; intros.
  + inversion H.
  + simpl in *. destruct i as [|i&#39;]; subst; auto.
    - simpl. f_equal. apply IHt with (i := i&#39;) (e := e); auto; omega.
Qed.
(* end hide *)</code></pre>
<p>Using <code>replace</code>, we can define <code>selection_sort</code>. Note that our naive implementation doesn’t convince Coq that it terminates on all input. An easy workaround is to add a <code>step</code> parameter. Observing that selection sort algorithm terminates in <code>length l</code> steps, where <code>l</code> is the input, we can define this:</p>
<pre class="coq"><code>Fixpoint selection_sort_aux (l : list nat) (step : nat) : list nat :=
  match step with
  | 0       =&gt; l
  | S step&#39; =&gt;
      match l with
      | []     =&gt; []
      | h :: t =&gt;
          match find_min_idx_aux t h 0 1 with
          | 0     =&gt; h :: selection_sort_aux t step&#39;
          | S min =&gt; nth min t 0 :: selection_sort_aux (replace t min h) step&#39;
          end
      end
  end.

Definition selection_sort (l : list nat) : list nat := selection_sort_aux l (length l).
(* begin hide *)
Hint Unfold selection_sort.
(* end hide *)</code></pre>
<p>We have all we need to prove permutation property. It’s still not easy though. I’m putting an extra lemma here as a tip.</p>
<pre class="coq"><code>(* begin hide *)
Lemma perm_cons_inside : forall A (a : A) b c d e,
  Permutation (a :: b) (c :: d) -&gt;
  Permutation (a :: e :: b) (c :: e :: d).
Proof.
  intros.
  assert (Permutation (e :: a :: b) (a :: e :: b)) by constructor.
  rewrite &lt;- H0.
  assert (Permutation (c :: e :: d) (e :: c :: d)) by constructor.
  rewrite H1. auto.
Qed.
(* end hide *)
(** ***** Exercise: 2 stars. *)
Lemma replace_perm_head : forall n h t,
  n &lt; length t -&gt;
  Permutation (h :: t) (nth n t 0 :: replace t n h).
(* begin hide *)
Proof.
  induction n as [|n&#39;].
  + intros. destruct t; simpl in *. omega. constructor.
  + intros. destruct t as [|th tt]. inversion H.
    assert (n&#39; &lt; length tt). { simpl in H. omega. } clear H. simpl.
    pose proof IHn&#39; h tt H0.
    apply perm_cons_inside. assumption.
Qed.
(* end hide *)

(** ***** Exercise: 3 stars. Use [replace_perm_head]. *)
Theorem Permutation_selection_sort : forall l, Permutation l (selection_sort l).
(* begin hide *)
Proof.
  intro. remember (length l) as len. generalize dependent l.
  induction len as [|len&#39;]; intros; destruct l; auto; inversion Heqlen; clear Heqlen.
  unfold selection_sort in *. simpl. destruct (find_min_idx_aux l n 0 1) as [|n&#39;] eqn: ret; auto.

  (* selection sort on (replace l n&#39; n) should return a permutation *)
  pose proof IHlen&#39; (replace l n&#39; n).
  (* .. but to show that, we first need to show that (replace l n&#39; n) has same length with l.
     to be able to use [replace_len], we need to show [n&#39; &lt; length l] *)
  apply find_min_idx_len_inv in ret. simpl in *. rewrite &lt;- H0 in ret.
  assert (n&#39; &lt; length l) by omega; clear ret.
  (* now we know [n&#39; &lt; length l], we can show [length (replace l n&#39; n) = length l]. *)
  pose proof replace_len nat l n&#39; n H1. assert (len&#39; = length (replace l n&#39; n)) by omega.
  (* show that recursive call returns a permutation *)
  apply H in H3.

  assert (Permutation (nth n&#39; l 0 :: selection_sort_aux (replace l n&#39; n) (length l))
                      (nth n&#39; l 0 :: replace l n&#39; n)).
  { apply perm_skip. rewrite H2. rewrite &lt;- H3. reflexivity. }

  rewrite H4. apply replace_perm_head. auto. auto.
Qed.
(* end hide *)</code></pre>
<p>Proving sorted property is again, not easy. I’m putting some lemmas as pointers. Lemmas are very easy to prove, leaving only the master theorem as a challange.</p>
<pre class="coq"><code>(** ***** Exercise: 1 star. *)
Lemma member_replace : forall A l i (x y : A),
  member x (replace l i y) -&gt;
  x = y \/ member x l.
(* begin hide *)
Proof.
  intros A l. induction l as [|h t].
  + intros. inversion H.
  + intros. destruct i as [|i&#39;]; simpl in *; inversion H; auto.
    subst. pose proof IHt i&#39; x y H2. inversion H0; auto.
Qed.
(* end hide *)

(** ***** Exercise: 2 stars. Use [member_replace] and [find_min_idx_correct]. *)
Lemma swap_min_replace : forall h t min_idx,
  find_min_idx (h :: t) = S min_idx -&gt;
  (forall e, member e (replace t min_idx h) -&gt; nth min_idx t 0 &lt;= e).
(* begin hide *)
Proof.
  intros.
  pose proof H0. apply member_replace in H0.
  pose proof H. apply find_min_idx_correct with (e := e) in H2; auto.
  inversion H0; subst; auto.
Qed.
(* end hide *)

(** ***** Exercise: 1 star. *)
Lemma sorted_min_tail : forall h t,
  sorted t -&gt;
  (forall e, member e t -&gt; h &lt;= e) -&gt;
  sorted (h :: t).
(* begin hide *)
Proof.
  intros. destruct t as [|ht tt]; auto.
Qed.
(* end hide *)

(** ***** Exercise: 2 stars. Use [member_preserved_by_perm]. *)
Lemma min_preserved_by_perm : forall m l l&#39;,
  Permutation l l&#39; -&gt;
  (forall e, member e l -&gt; m &lt;= e) -&gt;
  (forall e, member e l&#39; -&gt; m &lt;= e).
(* begin hide *)
Proof.
  intros m l l&#39; H. induction H; intros; auto.
  + apply H0. apply member_preserved_by_perm with (l&#39; := x :: l) in H1.
    - apply H1.
    - constructor. apply Permutation_sym in H. apply H.
  + apply member_preserved_by_perm with (l&#39; := y :: x :: l) in H0. auto. constructor.
Qed.
(* end hide *)


(** ***** Exercise: 4 stars. Selection sort returns sorted. *)
Theorem selection_sort_sorted : forall l, sorted (selection_sort l).
(* begin hide *)
Proof.
  intro. remember (length l) as len. generalize dependent l.
  induction len as [|len&#39;]; intros; destruct l; inversion Heqlen; clear Heqlen; auto.
  unfold selection_sort. simpl. remember (find_min_idx_aux l n 0 1) as min_idx.
  destruct min_idx as [|min_idx&#39;].
  + symmetry in Heqmin_idx.
    assert (0 &lt; 1) by omega. pose proof find_min_idx_aux_correct_same l n 0 1 H Heqmin_idx.
    apply sorted_min_tail. apply IHlen&#39;. apply H0.
    apply min_preserved_by_perm with (l := l). apply Permutation_selection_sort. apply H1.

  + symmetry in Heqmin_idx. pose proof swap_min_replace.
    unfold find_min_idx in H. pose proof H n l min_idx&#39; Heqmin_idx. clear H.

    assert (length l = length (replace l min_idx&#39; n)).
    { apply replace_len. apply find_min_idx_len_inv in Heqmin_idx; omega. }

    assert (sorted (selection_sort_aux (replace l min_idx&#39; n) (length l))).
    { unfold selection_sort in IHlen&#39;.
      rewrite H. apply IHlen&#39;. rewrite H0. apply replace_len.
      apply find_min_idx_len_inv in Heqmin_idx; omega.
    }

    assert (Permutation (replace l min_idx&#39; n)
                        (selection_sort_aux (replace l min_idx&#39; n) (length l))).
    { rewrite H. apply Permutation_selection_sort. }

    apply sorted_min_tail. auto. intros.

    pose proof min_preserved_by_perm
                (nth min_idx&#39; l 0)
                (replace l min_idx&#39; n)
                (selection_sort_aux (replace l min_idx&#39; n) (length l)) H3 H1. auto.
Qed.
(* end hide *)

(** ***** Exercise: 1 star. Show that selection sort is correct. *)
Theorem selection_sort_correct : Sorting_correct selection_sort.
(* begin hide *)
Proof.
  constructor. intros. split. apply selection_sort_sorted. apply Permutation_selection_sort.
Qed.
(* end hide *)</code></pre>
<h1 id="pancake-sort">Pancake sort</h1>
<p>This is getting harder and harder. We’ll define pancake sort in terms of <code>rev_at</code> which reverses a list at given index and <code>map_rest</code> which is like <code>map</code>, but applies the function to the rest of the list.</p>
<pre class="coq"><code>Fixpoint rev_at {A} idx (l : list A) : list A :=
  match idx with
  | 0      =&gt; rev l
  | S idx&#39; =&gt;
      match l with
      | []     =&gt; []
      | h :: t =&gt; h :: rev_at idx&#39; t
      end
  end.

Fixpoint map_rest_aux {A} (l : list A) (f : list A -&gt; list A) (timeout : nat) : list A :=
  match timeout with
  | 0 =&gt; l
  | S timeout&#39; =&gt;
      match f l with
      | [] =&gt; []
      | h :: t =&gt; h :: map_rest_aux t f timeout&#39;
      end
  end.

Definition map_rest {A} (l : list A) (f : list A -&gt; list A) : list A :=
  map_rest_aux l f (length l).</code></pre>
<p>Two things to note: 1) <code>rev_at</code> returns the original list if <code>idx</code> is larger than <code>length l - 1</code>. 2) <code>map_rest</code> only works as expected if <code>f</code> preserves length of it’s argument in it’s return value.</p>
<p>There’s a very useful property that we need to show before showing that pancake sort returns a permutation: if <code>f l</code> returns a permutation, than <code>map_rest l f</code> returns a permutation:</p>
<pre class="coq"><code>(** ***** Exercise: 2 stars. *)
Lemma Permutation_map_rest :
  forall A (f : list A -&gt; list A),
  (forall (l : list A), Permutation l (f l)) -&gt;
  (forall (l : list A), Permutation l (map_rest l f)).
(* begin hide *)
Proof.
  intros. remember (length l) as n. symmetry in Heqn. generalize dependent l. induction n as [|n&#39;].
  + intros. destruct l. constructor. inversion Heqn.
  + intros. destruct l.
    - inversion Heqn.
    - inversion Heqn; clear Heqn.
      unfold map_rest. simpl. destruct (f (a :: l)) as [|h t] eqn: Heqret.
      * pose proof H (a :: l). rewrite Heqret in H0. apply H0.
      * pose proof H (a :: l). rewrite Heqret in H0.
        apply Permutation_trans with (l   := a :: l)
                                     (l&#39;  := h :: t)
                                     (l&#39;&#39; := h :: map_rest_aux t f (length l)); auto.
        apply perm_skip. unfold map_rest in IHn&#39;.
        apply Permutation_length in H0. inversion H0. rewrite H3.
        apply IHn&#39; with (l := t).
        rewrite &lt;- H3. apply H1.
Qed.
(* end hide *)</code></pre>
<p>Using this definition we can define pancake sort:</p>
<pre class="coq"><code>Definition flip_pancakes (l : list nat) : list nat :=
  let min_idx := find_min_idx l in
  rev_at 0 (rev_at min_idx l).

Definition pancake_sort (l : list nat) : list nat := map_rest l flip_pancakes.</code></pre>
<p>Showing permutation property is actually very easy. All we need is two simple lemmas:</p>
<pre class="coq"><code>(** ***** Exercise: 2 stars. Use already included standard permutation lemmas. *)
Lemma Permutation_rev_at : forall A n (l : list A),
  Permutation l (rev_at n l).
(* begin hide *)
Proof.
  intros. generalize dependent n. induction l.
  + destruct n; constructor.
  + destruct n as [|n&#39;].
    - simpl. rewrite Permutation_cons_append. apply Permutation_app_tail. apply Permutation_rev.
    - simpl. apply Permutation_cons. apply IHl.
Qed.
(* end hide *)

(** ***** Exercise: 2 stars. Use transitivity of permutation and [Permutation_rev_at]. *)
Lemma Permutation_flip_pancakes : forall l, Permutation l (flip_pancakes l).
(* begin hide *)
Proof.
  intro. unfold flip_pancakes.
  apply perm_trans with
    (l   := l)
    (l&#39;  := rev_at (find_min_idx l) l)
    (l&#39;&#39; := rev_at 0 (rev_at (find_min_idx l) l)); apply Permutation_rev_at.
Qed.
(* end hide *)

(** ***** Exercise: 1 star. Use [Permutation_flip_pancakes]. *)
Theorem Permutation_pancake_sort : forall l, Permutation l (pancake_sort l).
(* begin hide *)
Proof.
  intros. induction l as [|h t].
  + auto.
  + unfold pancake_sort. unfold map_rest. apply Permutation_map_rest.
    intros. apply Permutation_flip_pancakes.
Qed.
(* end hide *)</code></pre>
<p>Now the hard part. I needed a lot of lemmas for showing sorted property. I’m listing lemmas I used in no particular order.</p>
<pre class="coq"><code>(** ***** Exercise: 2 stars.
    Smallest element is still smallest in a permutation.
    I found this a bit tricky. You may need to use another lemma we defined. *)
Lemma min_permutation : forall e l,
  (forall e&#39;, member e&#39; l -&gt; e &lt;= e&#39;) -&gt;
  (forall l&#39;, Permutation l l&#39; -&gt;
    (forall e&#39;, member e&#39; l&#39; -&gt; e &lt;= e&#39;)).
(* begin hide *)
Proof.
  (* tricky part is to see what lemma do you need *)
  intros. apply member_preserved_by_perm with (l&#39; := l) in H1.
  + auto.
  + apply Permutation_sym in H0. apply H0.
Qed.
(* end hide *)

(** ***** Exercise: 1 star. [flip_pancakes] preserves the length. *)
Theorem flip_pancakes_len : forall l, length l = length (flip_pancakes l).
(* begin hide *)
Proof.
  intros. apply Permutation_length. apply Permutation_flip_pancakes.
Qed.
(* end hide *)

(* begin hide *)
Lemma rev_at_1 : forall A n (l : list A),
  rev_at n l = firstn n l ++ rev (skipn n l).
Proof.
  intros. generalize dependent n. induction l as [|h t].
  + intros. destruct n; auto.
  + intros. destruct n as [|n&#39;].
    - reflexivity.
    - simpl. f_equal. apply IHt.
Qed.

Lemma skipn_head : forall n l h t,
  skipn n l = h :: t -&gt; nth n l 0 = h.
Proof.
  intros n l. generalize dependent n. induction l.
  + intros. destruct n; inversion H.
  + intros. destruct n as [|n&#39;].
    - simpl in *. inversion H. reflexivity.
    - simpl in *. apply IHl with (t := t). apply H.
Qed.

Lemma skipn_list : forall A n (l : list A),
  n &lt; length l -&gt;
  exists h t, skipn n l = h :: t.
Proof.
  intros. generalize dependent n. induction l.
  + intros. destruct n; inversion H.
  + intros. destruct n.
    - eexists. eexists. simpl. auto.
    - assert (n &lt; length l). { inversion H; omega. }
      pose proof IHl n H0. inversion H1. inversion H2. exists x. exists x0. auto.
Qed.
(* end hide *)

(** ***** Exercise: 4 stars.
    We can use [rev_at] to move smallest element to the end of the list,
    and then to the beginning of the list. Very tricky. Feel free to cheat. *)
Lemma rev_at_n : forall n l h t,
  n &lt; length l -&gt;
  rev_at 0 (rev_at n l) = h :: t -&gt;
  h = nth n l 0.
(* begin hide *)
Proof.
  intros. rewrite rev_at_1 in H0. simpl in H0. rewrite rev_at_1 in H0.
  rewrite rev_app_distr in H0. rewrite rev_involutive in H0.
  (* [skipn n l] should be in form [a :: b]. *)
  pose proof skipn_list nat n l H. inversion_clear H1. inversion_clear H2. rewrite H1 in H0.
  assert (x = h). { simpl in H0. inversion H0. reflexivity. }
  subst. pose proof skipn_head n l h x0 H1. auto.
Qed.
(* end hide *)

(* begin hide *)
Lemma find_min_idx_len : forall n l,
  find_min_idx l = n -&gt;
  l = [] \/ n &lt; length l.
Proof.
  intros. induction l as [|h t].
  + auto.
  + right. simpl in *. pose proof find_min_idx_len_inv t h 0 1 n H. auto.
Qed.
(* end hide *)

(** ***** Exercise: 3 stars. [flip_pancakes] moves smallest element to the head of the list. *)
Lemma flip_pancakes_min : forall l h&#39; t&#39;,
  flip_pancakes l = h&#39; :: t&#39; -&gt;
  (forall e&#39;, member e&#39; t&#39; -&gt; h&#39; &lt;= e&#39;).
(* begin hide *)
Proof.
  intros.
  pose proof (Permutation_flip_pancakes l). rewrite H in H1.
  unfold flip_pancakes in H. apply rev_at_n in H.
  + remember (find_min_idx l) as min.
    symmetry in Heqmin. pose proof (find_min_idx_correct min l Heqmin).
    pose proof (min_preserved_by_perm (nth min l 0) l (h&#39; :: t&#39;) H1 H2). rewrite &lt;- H in H3. auto.
  + destruct l. apply Permutation_nil in H1. inversion H1.
    pose proof find_min_idx_len (find_min_idx (n :: l)) (n :: l).
    assert (find_min_idx (n :: l) = find_min_idx (n :: l)) by reflexivity.
    apply H2 in H3. inversion H3.
    - inversion H4.
    - omega.
Qed.
(* end hide *)

(** ***** Exercise: 4 stars.
    You need a lot of lemmas we defined before, and it&#39;s a very long proof.
    Still not as hard as [find_min_idx_correct]. *)
Theorem pancake_sorted : forall l, sorted (pancake_sort l).
(* begin hide *)
Proof.
  intro. remember (length l) as len. generalize dependent l. induction len.
  + intros. destruct l. constructor. inversion Heqlen.
  + intros. destruct l as [|h t].
    - inversion Heqlen.
    - unfold pancake_sort in *. unfold map_rest in *. simpl.
      destruct (flip_pancakes (h :: t)) eqn: ret.
      * pose proof (flip_pancakes_len (h :: t)). rewrite ret in H. inversion H.
      * (* clean up the goal a bit, we want `sorted (n :: pancake_sort l)` *)
        assert (length l = length t) as leq.
        { pose proof (flip_pancakes_len (h :: t)). rewrite ret in H. inversion H. reflexivity. }
        rewrite &lt;- leq. clear leq.

        (* fold pancake_sort *)
        assert (map_rest_aux l flip_pancakes (length l) = pancake_sort l); auto.
        rewrite H. clear H.

        (* we can use inductive hypothesis to show pancake_sort l returns sorted *)
        assert (sorted (pancake_sort l)).
        { pose proof (flip_pancakes_len (h :: t)).
          rewrite ret in H. inversion H. inversion Heqlen. rewrite H1 in H2.
          pose proof (IHlen l H2). apply H0. }

        (* since n :: l = flip_pancakes (h :: t), n is smaller or equal than any element in l.
           we also know that pancake_sort returns a permutation.
           using that we can prove our goal. *)

        pose proof (flip_pancakes_min (h :: t) n l ret) as Nsmallest.

        destruct (pancake_sort l) as [|sh st] eqn: ret&#39;.
        { constructor. }
        { (* n should be smallest in sorted version too *)
          pose proof (Permutation_pancake_sort l). rewrite ret&#39; in H0.
          apply min_permutation with (e := n) (e&#39; := sh) in H0.
          + constructor. assumption. omega.
          + apply Nsmallest.
          + constructor.
        }
Qed.
(* end hide *)</code></pre>
<p>And finally, our master theorem:</p>
<pre class="coq"><code>Theorem pancake_sort_correct : Sorting_correct pancake_sort.
Proof.
  constructor. intro. split. apply pancake_sorted. apply Permutation_pancake_sort.
Qed.</code></pre>
<h1 id="notes-and-lessons-learned">Notes and lessons learned</h1>
<p>Implementation of algorithms are not what you’d expect to have in an imperative language. Rather than mutable arrays with O(1) access, I used classic functional style, using persistent linked lists. While that doesn’t affect runtime asymptotic complexities of sorting algorithms I used, it causes a lot of redundant allocations and worse iteration performance. Even the OCaml/Haskell extractions are not usable.</p>
<p>Still, proving on those purely functional definitions were very hard. The reason is that once you move away from basic “induction of subterm” style proofs, proving gets very hard very fast. Specifically, I found working on indexes and accumulators very hard.</p>
<p>As a next step, I’m hoping to define an imperative language in Coq, like IMP of Software Foundations but with mutable arrays, and prove same algorithms defined in that language correct.</p>]]></summary>
</entry>
<entry>
    <title>Coq exercises for beginners</title>
    <link href="http://osa1.net/posts/2014-07-12-fun-coq-exercises.html" />
    <id>http://osa1.net/posts/2014-07-12-fun-coq-exercises.html</id>
    <published>2014-07-12T00:00:00Z</published>
    <updated>2014-07-12T00:00:00Z</updated>
    <summary type="html"><![CDATA[<style>
.solution {
  background-color: rgb(245, 245, 245);
  padding-left: 1em;
}
</style>
<p>Formalizing abstractions/data structures and proving theorems about them in Coq is so much fun. I made up some simple exercises that consist of encoding some abstractions and laws we know from algebra and functional programming and then proving that some particular set + some operations on that set obeys the laws.</p>
<p>Using my amazing(!) JavaScript skills, I set up some “show/hide answer” buttons after each exercise. Exercises are easy, but the latter ones are relatively harder. Some abstractions/laws are inspired by Haskell.</p>
<p>Please note that I’m a beginner so my solutions probably have some flaws if you want to use them in large-scale verified programs :) I’m currently learning about typeclasses and records of Coq and I’m open to suggestions for improvements.</p>
<p>In exercises, when we talk that an abstraction should obey some laws, you need to enforce it in construction. e.g. You need to make constructors in a way that user would have to prove that the data structure + operations obey the laws.</p>
<pre class="coq"><code>Require Import List.
Import ListNotations.

Open Scope list_scope.</code></pre>
<h1 id="exercise-1">Exercise 1</h1>
<p>A <a href="http://en.wikipedia.org/wiki/Semigroup">semigroup</a> is a set together with an associative binary function. For example, natural numbers and addition function form a semigroup, because we know/can prove that addition function is associative. More precisely: (in Coq syntax)</p>
<pre class="coq"><code>forall (n1 n2 n3 : nat), n1 + (n2 + 3) = (n1 + n2) + n3.</code></pre>
<p>Encode semigroups in Coq.</p>
<div id="ex-div-1.1" class="solution">
<pre class="coq"><code>Inductive semigroup (A : Type) (Op : A -&gt; A -&gt; A) : Prop :=
| Semigroup_intro :
    (forall (a1 a2 a3 : A), Op a1 (Op a2 a3) = Op (Op a1 a2) a3) -&gt; semigroup A Op.</code></pre>
</div>
<button id="ex-btn-1.1">
</button>
<p>Now prove that lists together with append operation form a semigroup. Use standard Coq lists and <code>app</code> function.</p>
<div id="ex-div-1.2" class="solution">
<pre class="coq"><code>Theorem list_semigroup : forall A, semigroup (list A) (@app A).
Proof.
  intro. apply Semigroup_intro. intros.
  induction a1.
  + reflexivity.
  + simpl. f_equal. induction a2; auto.
Qed.</code></pre>
</div>
<button id="ex-btn-1.2">
</button>
<h1 id="exercise-2">Exercise 2</h1>
<p>A <a href="http://en.wikipedia.org/wiki/Monoid">monoid</a> is a semigroup with an identity element. In our addition example, identity element is 0, because when applied to the monoid function(addition) as first or second argument, results is the other argument:</p>
<pre class="coq"><code>forall (n : nat), 0 + n = n /\ n + 0 = n.</code></pre>
<p>Encode monoids in Coq.</p>
<div id="ex-div-2.1" class="solution">
<pre class="coq"><code>Inductive monoid A Op (sg : semigroup A Op) (U : A) : Prop :=
| Monoid_intro :
    semigroup A Op -&gt; (forall (a : A), Op U a = Op a U /\ Op U a = a) -&gt; monoid A Op sg U.</code></pre>
</div>
<button id="ex-btn-2.1">
</button>
<p>Now prove that lists with empty list as unit element together with the proof that lists are monoids as we proved in previous exercise, form a monoid.</p>
<div id="ex-div-2.2" class="solution">
<pre class="coq"><code>Theorem list_monoid : forall A, monoid (list A) (@app A) (@list_semigroup A) [].
Proof.
  intro. apply Monoid_intro. apply list_semigroup.
  intro. split.
  + rewrite app_nil_r. reflexivity.
  + reflexivity.
Qed.</code></pre>
</div>
<button id="ex-btn-2.2">
</button>
<h1 id="exercise-3">Exercise 3</h1>
<p>In this exercise and exercise 4, we’ll be talking about Haskell definitions of abstractions, instead of algebra definitions. (although they may coincide)</p>
<p>A functor is a type with one argument(in Haskell terms, a type with kind <code>* -&gt; *</code>) and a function, together with some laws. If you’re unfamiliar with functors of Haskell, you may want to skip this, or read <a href="http://www.haskell.org/haskellwiki/Typeclassopedia#Functor">Typeclassopedia</a>.</p>
<p>A Coq definition would use these to encode functors:</p>
<ul>
<li>Functor type: <code>F : Type -&gt; Type</code></li>
<li>Functor operation: <code>forall t1 t2, (t1 -&gt; t2) -&gt; f t1 -&gt; f t2</code> (let’s call it fmap)</li>
</ul>
<p>A functor should obey these laws:</p>
<ul>
<li><code>fmap id = id</code></li>
<li><code>fmap (fun x =&gt; g (h x)) = fun x =&gt; (fmap g (fmap h x))</code></li>
</ul>
<p>Encode functors in Coq.</p>
<div id="ex-div-3.1" class="solution">
<pre class="coq"><code>Inductive functor (F : Type -&gt; Type) : (forall t1 t2, (t1 -&gt; t2) -&gt; f t1 -&gt; f t2) -&gt; Prop :=
| Functor_intro
    (fmap : forall t1 t2, (t1 -&gt; t2) -&gt; F t1 -&gt; F t2)
    (l1   : forall t f, fmap t t id f = f)
    (l2   : forall t1 t2 t3, forall (f : F t1) (p : t2 -&gt; t3) (q : t1 -&gt; t2),
              fmap t1 t3 (fun a =&gt; p (q a)) f = fmap t2 t3 p (fmap t1 t2 q f)) :
    functor F fmap.</code></pre>
</div>
<button id="ex-btn-3.1">
</button>
<p>Now prove that lists with standard map function form a functor.</p>
<div id="ex-div-3.2" class="solution">
<pre class="coq"><code>Theorem list_functor : functor list map.
Proof.
  apply Functor_intro.
  + intros. induction f. reflexivity. simpl. rewrite IHf. reflexivity.
  + intros. induction f. reflexivity. simpl. f_equal. apply IHf.
Qed.</code></pre>
</div>
<button id="ex-btn-3.2">
</button>
<h1 id="exercise-4">Exercise 4</h1>
<p>A monad is a functor with two more operations; let’s call <code>bind</code> and <code>lift</code> and some more laws. In Coq syntax: (<code>F</code> is our functor type)</p>
<ul>
<li>bind: <code>forall t1 t2, F t1 -&gt; (t1 -&gt; F t2) -&gt; F t2</code></li>
<li>lift: <code>forall t, t -&gt; F t</code></li>
</ul>
<p>Laws:</p>
<ul>
<li>Left identity: <code>forall t1 t2 a f, bind t1 t2 (lift t1 a) f = f a</code></li>
<li>Right identity: <code>right_id : forall t m, bind t t m (lift t) = m</code></li>
<li>Associativity: <code>forall t1 t2 t3 m f g, bind t2 t3 (bind t1 t2 m f) g = bind t1 t3 m (fun x =&gt; bind t2 t3 (f x) g)</code></li>
</ul>
<p>Encode monads in Coq.</p>
<div id="ex-div-4.1" class="solution">
<pre class="coq"><code>Inductive monad : (Type -&gt; Type) -&gt; Prop :=
| Monad_intro
    (F        : Type -&gt; Type)
    (fmap     : forall t1 t2, (t1 -&gt; t2) -&gt; F t1 -&gt; F t2)
    (Fp       : functor F fmap)
    (lift     : forall t, t -&gt; F t)
    (bind     : forall t1 t2, F t1 -&gt; (t1 -&gt; F t2) -&gt; F t2)
    (left_id  : forall t1 t2 a f, bind t1 t2 (lift t1 a) f = f a)
    (right_id : forall t m, bind t t m (lift t) = m)
    (assoc    : forall t1 t2 t3 m f g,
                  bind t2 t3 (bind t1 t2 m f) g = bind t1 t3 m (fun x =&gt; bind t2 t3 (f x) g)) :
    monad F.</code></pre>
</div>
<button id="ex-btn-4.1">
</button>
<p>Now prove that lists form a monad. You need to figure out what functions to use for <code>lift</code> and <code>bind</code>.</p>
<div id="ex-div-4.2" class="solution">
<p><code>lift</code> function:</p>
<pre class="coq"><code>(* I couldn&#39;t find this in stdlib so let&#39;s define *)
Definition singleton (A : Type) (x : A) := [x].</code></pre>
<p>For <code>bind</code>, you can use standard <code>flat_map</code> function, but it’s argument order is reversed. So instead I rolled my own version:</p>
<pre class="coq"><code>Fixpoint concat {A : Type} (l : list (list A)) : list A :=
  match l with
  | []     =&gt; []
  | h :: t =&gt; app h (concat t)
  end.

(* I don&#39;t like argument order of flat_map in stdlib ... *)
Definition concatMap (A : Type) (B : Type) (l : list A) (f : A -&gt; list B) : list B :=
  concat (map f l).</code></pre>
<p>Now most involved proof in this exercises: (still very easy)</p>
<pre class="coq"><code>Theorem list_monad : monad list.
Proof.
  apply Monad_intro with (fmap := map) (lift := singleton) (bind := concatMap).
  + apply list_functor.
  + intros. unfold concatMap. simpl. rewrite app_nil_r. reflexivity.
  + intros. unfold concatMap. induction m.
    - reflexivity.
    - simpl. f_equal. apply IHm.
  + intros. induction m as [|h t].
    - reflexivity.
    - unfold concatMap in *. simpl. rewrite &lt;- IHt. 
      assert (forall A (l1 : list (list A)) (l2 : list (list A)),
                concat l1 ++ concat l2 = concat (l1 ++ l2)) as H.
        intros. induction l1; auto.
          simpl. rewrite &lt;- app_assoc. rewrite IHl1. auto.
      rewrite H. f_equal. rewrite map_app. reflexivity.
Qed.</code></pre>
</div>
<button id="ex-btn-4.2">
</button>
<h1 id="exercise-5">Exercise 5</h1>
<p>Prove that standard <code>option</code> type with some operations form a semigroup, monoid, functor and monad. You need to find relevant operations.</p>
<p>What restrictions do you need on <code>option</code>s type argument? (<code>A</code> in <code>option A</code>) Does it need to form a monoid for <code>option</code> to form a monoid?</p>
<div id="ex-div-5.1" class="solution">
<pre class="coq"><code>Definition map_option (A B : Type) (f : A -&gt; B) (opt : option A) :=
  match opt with
  | None =&gt; None
  | Some t =&gt; Some (f t)
  end.

Definition append_option A OpA (sg : semigroup A OpA) (a b : option A) : option A :=
  match a, b with
  | None, None =&gt; None
  | None, Some b&#39; =&gt; Some b&#39;
  | Some a&#39;, None =&gt; Some a&#39;
  | Some a&#39;, Some b&#39; =&gt; Some (OpA a&#39; b&#39;)
  end.

Theorem option_semigroup : forall A OpA (sg : semigroup A OpA),
  semigroup (option A) (append_option A OpA sg).
Proof.
  intros. apply Semigroup_intro. intros. destruct a1.
  + destruct a2.
    - destruct a3.
      * simpl. f_equal. inversion sg. apply H.
      * simpl. reflexivity.
    - destruct a3; simpl; reflexivity.
  + destruct a2; destruct a3; auto.
Qed.

Theorem option_monoid : forall A OpA (sg : semigroup A OpA),
  monoid (option A) (append_option A OpA sg) (option_semigroup A OpA sg) None.
Proof.
  intros. apply Monoid_intro. apply option_semigroup.
  intros. split. auto. destruct a; auto.
Qed.

Definition option_map A B (f : A -&gt; B) (o : option A) : option B :=
  match o with
  | None =&gt; None
  | Some a =&gt; Some (f a)
  end.

Theorem option_functor : functor option option_map.
Proof.
  apply Functor_intro; intros; destruct f; auto.
Qed.

Definition option_bind A B (o1 : option A) (f : A -&gt; option B) : option B :=
  match o1 with
  | None =&gt; None
  | Some a =&gt; f a
  end.

Theorem option_monad : monad option.
Proof.
  apply Monad_intro with (fmap := option_map) (lift := Some) (bind := option_bind).
  + apply option_functor.
  + intros. auto.
  + intros. destruct m; auto.
  + intros. destruct m; auto.
Qed.</code></pre>
</div>
<button id="ex-btn-5.1">
</button>
<h1 id="exercise-6">Exercise 6</h1>
<p>I only have a partial solution to this one and it’s not strictly a Coq exercise, but it’s still fun :)</p>
<p>A <a href="http://en.wikipedia.org/wiki/Group_(mathematics)">group</a> is a monoid with inverse element of every element. In Coq syntax:</p>
<pre class="coq"><code>forall e, exists e_i -&gt; op e e_1 = U</code></pre>
<p>where <code>op</code> is monoid operation and <code>U</code> is unit of monoid.</p>
<p>Can you come up with a data structure that forms a group?</p>
<div id="ex-div-6.1" class="solution">
<p>Someone at Coq IRC channel suggested diffs. Do you think a diff could form a group? What would associative operation, unit element, and inverse elements be?</p>
<div id="ex-div-6.2" class="solution">
<p>Diffs don’t form a group. Composing two diffs is the merge operation, which is partial function. We can’t always merge two diffs. (merge conflicts)</p>
<p>So I don’t have an answer to this exercise, If you know examples to this one, please write at comments :)</p>
</div>
<button id="ex-btn-6.2">
</button>
</div>
<button id="ex-btn-6.1">
</button>
<script>
function showMsg(exNum) {
  return "Show solution (ex. " + exNum + ")";
}

function hideMsg(exNum) {
  return "Hide solution (ex. " + exNum + ")";
}

function setUpShowHide(exNum) {
  var div = document.getElementById("ex-div-" + exNum);
  var btn = document.getElementById("ex-btn-" + exNum);
  btn.innerHTML = showMsg(exNum);
  btn.onclick = function() {
    if (div.style.display !== 'none') {
      div.style.display = 'none';
      btn.innerHTML = showMsg(exNum);
    } else {
      div.style.display = 'block';
      btn.innerHTML = hideMsg(exNum);
    }
  }
  div.style.display = 'none';
}

setUpShowHide(1.1);
setUpShowHide(1.2);
setUpShowHide(2.1);
setUpShowHide(2.2);
setUpShowHide(3.1);
setUpShowHide(3.2);
setUpShowHide(4.1);
setUpShowHide(4.2);
setUpShowHide(5.1);
setUpShowHide(6.2);
setUpShowHide(6.1);
</script>]]></summary>
</entry>
<entry>
    <title>A complicated proof of a simple theorem</title>
    <link href="http://osa1.net/posts/2014-07-10-complicated-proof-simple-thm.html" />
    <id>http://osa1.net/posts/2014-07-10-complicated-proof-simple-thm.html</id>
    <published>2014-07-10T00:00:00Z</published>
    <updated>2014-07-10T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>It’s that time of the year again in which I’m suddenly very interested about verification and interactive theorem proving. I’m currently working on proving properties of some well-known algorithms.</p>
<p>At least in the beginning even the simplest properties may turn out to be very hard to prove. In this post I’m going to give an example of a very simple theorem which turns out to be hard to prove for beginners like me.</p>
<p>Let’s say I want to divide peano nats by 10. I can define <code>div10</code> in two ways: 1) I can implement a function that directly divides by 10 2) I can implement <code>div2</code> and <code>div5</code> and use those two to implement <code>div10</code>:</p>
<pre class="coq"><code>(* first definition *)
Fixpoint div10 (n : nat) : nat :=
  match n with
  | S (S (S (S (S (S (S (S (S (S n&#39;))))))))) =&gt; S (div10 n&#39;)
  | _ =&gt; 0
  end.

(* second definition *)
Fixpoint div5 (n : nat) : nat :=
  match n with
  | S (S (S (S (S n&#39;)))) =&gt; S (div5 n&#39;)
  | _ =&gt; 0
  end.

Fixpoint div2 (n : nat) : nat :=
  match n with
  | S (S n&#39;) =&gt; S (div2 n&#39;)
  | _ =&gt; 0
  end.

Definition div10&#39; (n : nat) : nat := div5 (div2 n).</code></pre>
<p>It may be obvious enough that those two definitions are equal(in the sense that they always terminate and given same arguments they return same values) but as an exercise let’s try to prove it:</p>
<pre class="coq"><code>Theorem divs_eq : forall n, div10 n = div10&#39; n.</code></pre>
<p>It’s obvious that we need to do induction on n, but unfortunately that leads to very complicated induction hypothesis and I can’t make any use of them:</p>
<pre class="coq"><code>Proof.
  unfold div10&#39;. intro.
  do 10 (induction n; auto). simpl. f_equal. 
Abort.</code></pre>
<p>At this point goal is same as our original goal, but induction hypothesis are about a hundred lines long so I can’t make any use of it. (and I don’t understand why induction hypothesis getting that big)</p>
<p>It turns out that by using a different induction scheme we can easily prove this. Instead of using standard induction scheme of nats “prove P 0, assume P n and prove P (S n)” we can use “prove P 0, P 1, … P 9, assume P n and prove P (10 + n)”:</p>
<pre class="coq"><code>Definition nat10ind : forall (P : nat -&gt; Prop),
  P 0 -&gt; P 1 -&gt; P 2 -&gt; P 3 -&gt; P 4 -&gt; P 5 -&gt; P 6 -&gt; P 7 -&gt; P 8 -&gt; P 9 -&gt;
  (forall n, P n -&gt; P (10 + n)) -&gt; forall n, P n.

  intros.
  assert (P n /\ P (1 + n) /\ P (2 + n) /\ P (3 + n) /\ P (4 + n) /\ P (5 + n) /\
          P (6 + n) /\ P (7 + n) /\ P (8 + n) /\ P (9 + n)).
  + induction n.
    - simpl. repeat split; assumption.
    - repeat (match goal with H : _ /\ _ |- _ =&gt; destruct H end).
      repeat split; try assumption.
      apply H9. assumption.
  + destruct H10. assumption.
Defined.</code></pre>
<p>Now our main theorem is very easy to prove:</p>
<pre class="coq"><code>Theorem divs_eq : forall n, div10 n = div10&#39; n.
Proof.
  unfold div10&#39;. intro.
  induction n using nat10ind; auto.
  simpl. f_equal. assumption.
Qed.</code></pre>
<p>One thing that I found weird in <code>nat10ind</code> definition is that the expression <code>match goal with ...</code> is kind of special in that <code>goal</code> is not an identifier that represents some part of the context. Rather, <code>match goal with ...</code> is a special syntax to pattern match against the whole context, with hypothesis and goals<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. <code>H : _ /\ _ |- _</code> part is then matching for any hypothesis with form <code>_ /\ _</code> and binding it to <code>H</code>, ignoring the current goal(RHS of turnstile shown as <code>|-</code>).</p>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>See <a href="http://coq.inria.fr/distrib/current/refman/Reference-Manual011.html#sec447">tacexpr_1 production</a> of Coq expression syntax in reference manual for details.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></summary>
</entry>
<entry>
    <title>Proving soundness of simply typed multi-staged lambda-calculus</title>
    <link href="http://osa1.net/posts/2014-03-06-proving-simply-typed-multi-staged-lc.html" />
    <id>http://osa1.net/posts/2014-03-06-proving-simply-typed-multi-staged-lc.html</id>
    <published>2014-03-06T00:00:00Z</published>
    <updated>2014-03-06T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>First part of my first non-trivial(e.g. something other than a Software Foundations exercise) Coq program is finally done. I learned Coq from Software Foundations, but I didn’t finish to book. I still know only very basic tactics, which I think is one of the reasons why my proofs are so long. You can see the source <a href="https://github.com/osa1/StagedLambda/blob/master/Lc.v">here</a>.</p>
<p>Some random notes about implementation:</p>
<ul>
<li>I’m looking to improve proofs. I have a lot more to implement for the rest of the program until deadline, so I may not be able to refactor current proofs very much, but at one point I want to simplify the proofs and use more advanced tactics.</li>
<li>Currently only <code>inversion</code>, <code>destruct</code>, <code>induction</code>, <code>rewrite</code>, <code>assumption</code>, <code>assert</code>, <code>constructor</code>, <code>auro</code>, <code>simpl</code>, <code>intro</code>/<code>intros</code>, <code>apply</code>, <code>right</code>/<code>left</code>, <code>exists</code>, <code>unfold</code>, <code>subst</code>, <code>reflexivity</code>, <code>remember</code> and <code>generalize</code> tactics are used.</li>
<li>Language definition is almost the same as in papers. There is one difference, we implemented substitutions as a function. In reality, substitution in multi-staged lambda-calculus is not a function. I believe this doesn’t effect correctness of theorems. At one point I’ll refactor the code and define substitution as a relation.</li>
<li>I used <code>Case</code>, <code>SCase</code>, <code>SSCase</code> … constructs from SFlib extensively.</li>
</ul>
<p>Now I’m going to implement lambda-calculus with row-polymorphic records. I expect this to be at least 2x harder, since polymorphism is involved. Let’s see how it goes …</p>]]></summary>
</entry>
<entry>
    <title>On proof automations -- part 3</title>
    <link href="http://osa1.net/posts/2013-09-23-proof-automation-3.html" />
    <id>http://osa1.net/posts/2013-09-23-proof-automation-3.html</id>
    <published>2013-09-23T00:00:00Z</published>
    <updated>2013-09-23T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>Over the past several days I learned few things that changed my ideas about proof automation. See my first two posts about proof automation here: <a href="/posts/2013-09-11-proof-automation.html">#1</a> <a href="/posts/2013-09-17-proof-automation-2.html">#2</a></p>
<p>First, it’s really possible to use proof automation without first writing the whole proof. After started working on theorems defined on more complex languages, I realized one thing: Most of the cases of inductions/inversions are trivial to solve. Generally you only interested in one or two cases, while you have in total 20 cases. Why not use automation to eliminate 18 cases automatically? Proving that parts manually only makes it more obscure.</p>
<p>Second thing I learned is that automations like <code>auto</code>, <code>eauto</code>, <code>iauto</code> never perform <code>rewrite</code>, <code>subst</code>, case analysis(<code>inversion</code>, <code>destruct</code>, maybe others?) and induction. I think this is very important point, because this means that your proofs still have to resemble the structure of your non-automated proofs(i.e. structure of your terms). Which prevents your proofs from being too much magical.</p>
<p>Now let’s work on an example to see how some automation is good thing. I’ll prove progress property of STLC(simply-typed lambda calculus), as defined in <a href="www.cis.upenn.edu/~bcpierce/sf/">Software Foundations</a> chapter <a href="http://www.cis.upenn.edu/~bcpierce/sf/Stlc.html">Stlc</a>.</p>
<p>Here’s an initial version without using any kind of automation.</p>
<pre class="coq"><code>Theorem progress_no_auto : forall t T,
     empty |- t \in T -&gt;
     value t \/ exists t&#39;, t ==&gt; t&#39;.
Proof.
  intros t T TD. remember (@empty ty).
  (* induction on typing derivation *)
  has_type_cases (induction TD) Case.
  Case &quot;T_Var&quot;.
    (* this case is impossible because vars are not well-typed in empty env,
       find a contradiction *)
    rewrite Heqp in H. inversion H.
  Case &quot;T_Abs&quot;.
    (* lambdas are values *)
    left. apply v_abs.
  Case &quot;T_App&quot;.
    (* well-typed applications should take a step *)
    right. destruct IHTD1. assumption.
    SCase &quot;t1 is a value&quot;. destruct IHTD2. assumption.
      SSCase &quot;t2 is a value&quot;.
        (* t1 should be a lambda *)
        inversion TD1; subst; inversion H. (* inversion on typing derivation of t1,
                                              most cases can be eliminated with inversion H,
                                              because t1 can only be lambda expression to be well-typed *)
        SSSCase &quot;t1 is tabs&quot;. eexists. apply ST_AppAbs. assumption.
      SSCase &quot;t2 can take a step&quot;.
        inversion H0. eexists. apply ST_App2. assumption. apply H1.
    SCase &quot;t1 can take a step&quot;. inversion H. eexists. apply ST_App1. apply H0.
  Case &quot;T_True&quot;. left. constructor.
  Case &quot;T_False&quot;. left. constructor.
  Case &quot;T_If&quot;.
    (* well-typed T_If can always take a step *)
    destruct IHTD1. assumption.
    SCase &quot;t1 is value&quot;. inversion H; subst.
      SSCase &quot;t1 is a lambda&quot;. inversion TD1.
      SSCase &quot;t1 is ttrue&quot;. right. exists t2. constructor.
      SSCase &quot;t1 is tfalse&quot;. right. exists t3. constructor.
    SCase &quot;t1 can take a step&quot;. inversion H; subst. right.
      exists (tif x0 t2 t3). constructor. assumption.
Qed.</code></pre>
<p>I leave comments to explain what’s going on. The point here is not to get shortest proof, but to show with some automation proofs could be more readable.</p>
<p>When you look to this proof you can realize that our interesting cases are:</p>
<ul>
<li><code>T_Var</code> case, in which we need to show that it’s impossible.</li>
<li><code>T_App</code> and <code>T_If</code> cases, in which we need to use induction hypothesis.</li>
</ul>
<p><code>T_Abs</code>, <code>T_True</code> and <code>T_False</code> cases are completely trivial and we don’t want to make our proof harder to write, read and maintain more.</p>
<p>Second thing to realize that, if you count <code>;</code> operator as a automation, then it makes me using automation even in that proof. In the case of <code>T_App</code>, I’m eliminating all cases of <code>t1</code>(because for <code>T_App</code> to be well-typed, <code>t1</code> can only be a lambda abstraction – other cases should be eliminated with contradictions) with <code>inversion H; subst.</code>. Without <code>;</code> operator, I would have to write this:</p>
<pre class="coq"><code>inversion TD1; subst.
SSSCase &quot;t1 is tvar&quot;. inversion H.
SSSCase &quot;t1 is tabs&quot;. eexists. apply ST_AppAbs. assumption.
SSSCase &quot;t1 is tapp&quot;. inversion H.
SSSCase &quot;t1 is tif&quot;. inversion H.</code></pre>
<p>and when the language gets bigger, more cases with just <code>inversion H</code> would follow.</p>
<p>Now here’s an automated version:</p>
<pre class="coq"><code>Theorem progress : forall t T,
     empty |- t \in T -&gt;
     value t \/ exists t&#39;, t ==&gt; t&#39;.
Proof with eauto.
  intros t T TD. remember (@empty ty). has_type_cases (induction TD) Case; auto.
  Case &quot;T_Var&quot;. rewrite Heqp in H. inversion H.
  Case &quot;T_App&quot;. right. destruct IHTD1...
    SCase &quot;t1 is a value&quot;. destruct IHTD2...
      SSCase &quot;t2 is a value&quot;. inversion TD1; subst; try (solve by inversion)...
      SSCase &quot;t2 can take a step&quot;. inversion H0; subst...
    SCase &quot;t1 can take a step&quot;. inversion H; subst...
  Case &quot;T_If&quot;. right. destruct IHTD1. assumption.
    SCase &quot;t1 is a value&quot;. inversion H; subst; eauto.
      SSCase &quot;t1 is tabs&quot;. solve by inversion.
    SCase &quot;t1 can take a step&quot;. inversion H...
Qed.</code></pre>
<p>IMO this proof is a lot more simple and readable. It eliminates all trivial cases and only leave us with interesting ones. And even in that cases we simply finished with <code>...</code> or explicitly running <code>auto</code>/<code>eauto</code>.</p>]]></summary>
</entry>
<entry>
    <title>On proof automations -- part 2</title>
    <link href="http://osa1.net/posts/2013-09-17-proof-automation-2.html" />
    <id>http://osa1.net/posts/2013-09-17-proof-automation-2.html</id>
    <published>2013-09-17T00:00:00Z</published>
    <updated>2013-09-17T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p><a href="http://adam.chlipala.net/">Adam Chilpala’s</a> <a href="http://adam.chlipala.net/cpdt/">Certified Programming with Dependent Types</a> has a great discussion on writing automated proofs and readability of proofs. Highly recommended. Chapter 16(page 319 on current revision).</p>
<p>I think one key thing to realize about interactive theorem proving is that there is no perfect, or maybe even good, solution to the problem of proof representation. Tactics help, but without automation tactic based proofs are just too fragile(every change in definitions implies changes in proofs) and verbose. But automated proofs are very hard to read, and it’s very hard to modify automated proofs for changes. CPDT book has a great example of this. Let’s think of this proof(I’ll skip the definitions, just think some simple arithmetic expressions language just like you could find in any elementary programming language theory book)</p>
<pre class="coq"><code>Theorem eval_times : forall k e,
    eval (times k e) = k * eval e.
Proof.
  induction e as [| ? IHe1 ? IHe 2]; [
    trivial
    | simpl; rewrite IHe1; rewrite IHe2; rewrite mult_plus_distr_l; trivial ].
Qed.</code></pre>
<p>And now let’s say you added one more constructor to arithmetic expression syntax. How to change this proof to adopt the changes? I don’t think there’s a way to do that without first writing the version without automation. And that brings us back to my first post.</p>
<p>I think the reason why I think too much about this stuff instead of writing some proofs is that I look for elegance and simplicity in my programs. And only if necessary I try to make my programs efficient(fast, small, whatever). After some simple trivial proofs that I did for learning I never found my proofs satisfactory. They are either very very long and repetitive or very hard to understand and modify. It’s very hard to find a sweet-spot in proofs that is not very repetitive and long and still possible to read and understand.</p>
<p>You can always apply methods you learned to make your programs easier to read and modify, like moving some code to a new function with a useful name and formal parameters etc. but in the case of proofs it’s also very hard to find what pieces are considered worthy to be a lemma and what would be useful name for it.</p>
<p>My ideas about proof automation is changed from “no way I use them” to “okay they can be very useful for eliminating repetition without adding obscurity to the proof” in one day. I’ll continue writing Coq proofs for while and probably add new posts on this topic later.</p>]]></summary>
</entry>
<entry>
    <title>On proof automations</title>
    <link href="http://osa1.net/posts/2013-09-11-proof-automation.html" />
    <id>http://osa1.net/posts/2013-09-11-proof-automation.html</id>
    <published>2013-09-11T00:00:00Z</published>
    <updated>2013-09-11T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>While working with constructive systems, at the lowest level you can directly construct proof objects by writing programs. But for realistic scenarios, this is probably too hard to do. Coq and Idris like languages provide an alternative way to construct proof objects: tactics. Tactics provide a way to derive proofs in a way that is somewhat like informal proofs given in spoken languages. But theorems you’re trying to prove is already too complex, and in some parts, very repetitive.</p>
<p>That <em>tactics</em> Coq like theorem provers provide also help you construct proofs with some automation commands. For instance, you can apply same steps several times, until you prove some subgoals. Or you can search for a proof by applying some steps in different combinations.</p>
<p>What doesn’t make sense to me is that in general you cannot really use that automation methods before manually writing long and hard proof by hand and showing that your theorem is really a theorem and it indeed has some repetitive parts.</p>
<p>And at this point you’re probably better off leaving the proof as-is because replacing explicit steps with some magic commands(I’ll come to this point later) only makes the proof harder to read, without any other advantages.</p>
<p>I have similar ideas for <code>auto</code> tactic. What it does is that it searches for a proof by applying different combinations of <code>apply</code> and <code>intros</code> tactics(with some limitations – it only uses apply for hypothesis and hint database which you generate while defining inductive definitions).</p>
<p>Now my problem with <code>auto</code> is that you can only use it when you absolutely sure that your theorem is indeed a theorem – that is, it has a proof. And how can you know that your theorem is correct? The only way to know this is to write a proof for it. Now let’s say <code>auto</code> tactic fails, and when this happens it can’t really say anything. Why did it fail? It may be because your proof is actually too long and it searched for a while but couldn’t find a proof(you can specify search depth as an optional parameter of <code>auto</code> tactic). Or maybe your hint base is not good enough. Or maybe your theorem is not actually a theorem and you cannot prove it.</p>
<p>So both proof search tactics and repetition elimination tactics have this same common problem: you cannot know that they work before writing the proof itself. And automation generally makes the proof unreadable.</p>
<p>Adam Chilpala’s “Certified Programming with Dependent Types” book encourages that automated way of proving. You have that complicated theorem that you don’t even understand what it is saying? No problem, <code>crush</code> tactic(which is not standard, distributed with CPDT book) will prove it for you in one command.</p>
<p>I’m not saying that <code>crush</code> like tactics are necessarily bad and you shouldn’t use it. I’d probably use it in real world when I need a proof that <code>crush</code> can generate and I don’t have time or motivation to do it myself.</p>
<p>And repetition elimination tactics are useful when you have a syntax tree with 15 constructors and you’re proving some theorem that’s relevant with only one of that constructors – 14 subgoals can be easily proved by some clever use of <code>;</code> operator in Coq.</p>
<p>What I’m saying that is several things:</p>
<ul>
<li>Automated proofs can make proofs harder to read and most of the time you already need to have written the long proof to see that it has repetitive parts or at least it is provable. (certainly there are cases where you’ll find yourself trying to prove something wrong)</li>
<li>Once a long proof is written, there is little or no need to remove that proof and replace it with an automated version. (one reason to do that is to have same proof working even after some inductive definitions is expanded/changed)</li>
<li>Most importantly, <code>crush</code> like automation tactics are pure black magic and doesn’t really teach you anthing. As a hobbyist who self-study all this, I prefer learning the principles instead of scripts and magic commands and black boxes.</li>
</ul>
<p>For this reasons I’m trying to use proof automation as little as possible.</p>]]></summary>
</entry>

</feed>
