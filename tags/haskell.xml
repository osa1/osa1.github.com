<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>osa1.net - Posts tagged haskell</title>
    <link href="http://osa1.net/tags/haskell.xml" rel="self" />
    <link href="http://osa1.net" />
    <id>http://osa1.net/tags/haskell.xml</id>
    <author>
        <name>Ömer Sinan Ağacan</name>
        <email>omeragaca@gmail.com</email>
    </author>
    <updated>2018-03-16T00:00:00Z</updated>
    <entry>
    <title>Three runtime optimizations done by GHC's GC</title>
    <link href="http://osa1.net/posts/2018-03-16-gc-optimizations.html" />
    <id>http://osa1.net/posts/2018-03-16-gc-optimizations.html</id>
    <published>2018-03-16T00:00:00Z</published>
    <updated>2018-03-16T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>While working on GHC’s GC code I realized that it does some runtime optimizations. One of those I already knew from another language, but the other two were quite interesting to me because they’re related with laziness. I wouldn’t think consequences of laziness reach this far into the runtime system. It turns out it does; disabling those optimizations make programs run significantly slower.</p>
<p>Because I almost read the whole code line by line, I believe this list is exhaustive. The code is taken from the source code but significantly simplified.</p>
<p>If you’re not familiar with GHC’s heap object layout and info tables etc., I suggest reading <a href="https://ghc.haskell.org/trac/ghc/wiki/Commentary/Rts/Storage/HeapObjects">the wiki page</a> before moving on the rest of the post.</p>
<h1 id="replacing-small-int-and-char-closures-with-statically-initialized-shared-closures">1. Replacing small Int and Char closures with statically initialized, shared closures</h1>
<p><code>Int</code> and <code>Char</code> closures have one non-pointer field for the actual integer and character values, as can be seen in GHCi:</p>
<pre><code>λ&gt; :info Int
data Int = GHC.Types.I# GHC.Prim.Int#     -- Defined in ‘GHC.Types’
λ&gt; :info Char
data Char = GHC.Types.C# GHC.Prim.Char#   -- Defined in ‘GHC.Types’</code></pre>
<p>The corresponding closure type for closures with one non-pointer and no pointers is <a href="https://github.com/ghc/ghc/blob/cb6d8589c83247ec96d5faa82df3e93f419bbfe0/includes/rts/storage/ClosureTypes.h#L25"><code>CONSTR_0_1</code></a>. The garbage collector <a href="https://github.com/ghc/ghc/blob/cb6d8589c83247ec96d5faa82df3e93f419bbfe0/rts/sm/Evac.c#L656">needs to check closure type before copying an object</a> to decide how many bytes to copy (and also to decide what pointers to follow and copy the pointed object, but this happens in a later stage). When it finds a <code>CONSTR_0_1</code> it checks if it’s actually an <code>Int</code> or <code>Char</code> closure, if it is, it checks if the payload (the actual <code>Int</code> and <code>Char</code> values) is within a range. If it is then we know that we have statically-allocated <code>Int</code> or <code>Char</code> closure what is identical to the one we’re copying, so we return address to the statically allocated one rather than copying the closure and returning the new address of the copied closure. This way we avoid having multiple closures for <code>1 :: Int</code>, for example. The code (simplified, some comments by me):</p>
<pre class="sourceCode c"><code class="sourceCode c"><span class="kw">case</span> CONSTR_0_1:
{
    <span class="co">// Constructor with one non-pointer field. Read the field.</span>
    StgWord w = (StgWord)q-&gt;payload[<span class="dv">0</span>];

    <span class="kw">if</span> (<span class="co">// is it a Char?</span>
        info == Czh_con_info &amp;&amp;
        <span class="co">// is the value in range?</span>
        (StgChar)w &lt;= MAX_CHARLIKE)
    {
        <span class="co">// return address to statically allocated Char closure</span>
        *p =  TAG_CLOSURE(tag, (StgClosure *)CHARLIKE_CLOSURE((StgChar)w));
    }
    <span class="kw">else</span> <span class="kw">if</span> (<span class="co">// is it an Int?</span>
             info == Izh_con_info &amp;&amp;
             <span class="co">// is the value in range?</span>
             (StgInt)w &gt;= MIN_INTLIKE &amp;&amp; (StgInt)w &lt;= MAX_INTLIKE)
    {
        <span class="co">// return address to statically allocated Int closure</span>
        *p = TAG_CLOSURE(tag, (StgClosure *)INTLIKE_CLOSURE((StgInt)w));
    }
    <span class="co">// otherwise copy the object</span>
    <span class="kw">else</span>
    {
        copy_tag_nolock(p,info,q,sizeofW(StgHeader)+<span class="dv">1</span>,gen_no,tag);
    }
    <span class="kw">return</span>;
}</code></pre>
<p>What are the ranges here? Looking at the <a href="https://github.com/ghc/ghc/blob/cb6d8589c83247ec96d5faa82df3e93f419bbfe0/rts/StgMiscClosures.cmm#L679-L974">definition</a>, we see that integers in range [-16, 16] and the whole ASCII character set is covered.</p>
<p>Here’s a small program that shows the effect of this optimization:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">module</span> <span class="dt">Main</span> <span class="kw">where</span>

<span class="kw">import </span><span class="dt">GHC.Stats</span>
<span class="kw">import </span><span class="dt">System.Mem</span> (performMajorGC)

<span class="ot">seqIntList ::</span> [<span class="dt">Int</span>] <span class="ot">-&gt;</span> a <span class="ot">-&gt;</span> a
seqIntList []       a <span class="fu">=</span> a
seqIntList (i <span class="fu">:</span> is) a <span class="fu">=</span> i <span class="ot">`seq`</span> is <span class="ot">`seqIntList`</span> a

<span class="ot">main ::</span> <span class="dt">IO</span> ()
main <span class="fu">=</span> <span class="kw">do</span>
    <span class="kw">let</span> lst <span class="fu">=</span> [ <span class="dv">0</span> <span class="fu">..</span> <span class="dv">15</span> ]
    <span class="co">-- let lst = [ 17 .. 32 ] -- enable this on the second run</span>

    <span class="co">-- evaluate the list</span>
    lst <span class="ot">`seqIntList`</span> return ()

    <span class="co">-- collect any thunks, do the optimization if possible, update stats</span>
    performMajorGC

    rts_stats <span class="ot">&lt;-</span> getRTSStats
    putStrLn (<span class="st">&quot;Live data: &quot;</span> <span class="fu">++</span> show (gcdetails_live_bytes (gc rts_stats)) <span class="fu">++</span> <span class="st">&quot; bytes&quot;</span>)

    <span class="co">-- to make sure our list won&#39;t be collected</span>
    lst <span class="ot">`seqIntList`</span> return ()</code></pre>
<p>Run it with:</p>
<pre><code>ghc eq.hs -rtsopts -O0 &amp;&amp; ./eq +RTS -T</code></pre>
<p>On the second run, disable the first list and enable the second one. You’ll see this output:</p>
<pre><code>$ ghc eq.hs -rtsopts -O0 &amp;&amp; ./eq +RTS -T
[1 of 1] Compiling Main             ( eq.hs, eq.o )
Linking eq ...
Live data: 2224 bytes

$ ghc eq.hs -rtsopts -O0 &amp;&amp; ./eq +RTS -T
[1 of 1] Compiling Main             ( eq.hs, eq.o )
Linking eq ...
Live data: 2480 bytes</code></pre>
<p>So second program has 256 bytes more live data. Let’s check if that makes sense. The first program doesn’t have any heap-allocated <code>Int</code> closures, because all of the <code>Int</code> in the program are within the range of statically allocated <code>Int</code> closures. Second one has 16 <code>Int</code> closures. An <code>Int</code> closure is two words: a pointer to the <code>I#</code> info table, and an actual integer value in the payload, so that’s 16 bytes. 16 (number of <code>Int</code> closures) * 16 (<code>Int</code> closure size) = 256.</p>
<p>I know at least one another language, Python, does this as well:</p>
<pre><code>Python 3.5.2 (default, Nov 23 2017, 16:37:01)
[GCC 5.4.0 20160609] on linux
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&gt;&gt;&gt; x = 1
&gt;&gt;&gt; y = 1
&gt;&gt;&gt; x is y
True
&gt;&gt;&gt; x = 100000000000
&gt;&gt;&gt; y = 100000000000
&gt;&gt;&gt; x is y
False</code></pre>
<p>Although I’m not sure if it does this during garbage collection.</p>
<h1 id="shorting-out-indirections">2. Shorting out indirections</h1>
<p>This is related with how lazy evaluation is implemented so we’ll first take a look at the generated code for a simple thunk update. When we compile the following program: (to keep things simple we disable optimizations)</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">fib ::</span> <span class="dt">Int</span> <span class="ot">-&gt;</span> <span class="dt">Int</span>
fib <span class="dv">0</span> <span class="fu">=</span> <span class="dv">0</span>
fib <span class="dv">1</span> <span class="fu">=</span> <span class="dv">1</span>
fib n <span class="fu">=</span> fib (n<span class="fu">-</span><span class="dv">1</span>) <span class="fu">+</span> fib (n<span class="fu">-</span><span class="dv">2</span>)

<span class="ot">main ::</span> <span class="dt">IO</span> ()
main <span class="fu">=</span> <span class="kw">do</span>
    i <span class="ot">&lt;-</span> readLn
    print (fib i)</code></pre>
<p>in STG level we get this function:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell">sat_s31Q <span class="fu">=</span>
    \r [i_s31O]
        <span class="kw">let</span> { sat_s31P <span class="fu">=</span> \u [] fib_rqh i_s31O;
        } <span class="kw">in</span>  print <span class="fu">$</span>fShowInt sat_s31P;</code></pre>
<p>Here <code>fib_rqh</code> is the <code>fib</code> function, and <code>sat_s31P</code> is the thunk for <code>fib i</code>. First let’s take a look at how this thunk is evaluated in the use site: (Cmm syntax)</p>
<pre><code>I64[Sp - 16] = stg_upd_frame_info;
P64[Sp - 8] = _s31P::P64;
_s31O::P64 = P64[_s31P::P64 + 16];
R2 = _s31O::P64;
Sp = Sp - 16;
call fib_rqh_info(R2) args: 24, res: 0, upd: 24;</code></pre>
<p>So we push the thunk (<code>_s31P</code>), then <code>stg_upd_frame_info</code> to the stack, and jump to the code for the <code>fib</code> function, passing the argument in <code>R2</code>.</p>
<p>I won’t show the code (because it’s large and complex), but the code for <code>fib</code> puts the return value in <code>R1</code>, pops the stack, and jump to the code for the popped stack frame, which is <code>stg_upd_frame_info</code>.</p>
<p>At this point we have the return value of <code>fib</code> in <code>R1</code>, and thunk to update at the bottom of the stack.</p>
<p>The code for <code>stg_upd_frame_info</code> is as follows: (simplified, see the original version <a href="https://github.com/ghc/ghc/blob/cb6d8589c83247ec96d5faa82df3e93f419bbfe0/rts/Updates.cmm#L28-L38">here</a>)</p>
<pre><code>INFO_TABLE_RET ( stg_upd_frame, // label
                 UPDATE_FRAME,  // frame type
                 w_ info_ptr,   // info ptr
                 p_ updatee )   // thunk to update at the bottom of the stack
    return (P_ ret) // in R1 we expect the value to update the thunk with
{
    StgInd_indirectee(updatee) = ret;       // (1)
    SET_INFO(updatee, stg_BLACKHOLE_info);  // (2)
    ...
    return (ret);
}</code></pre>
<p>This basically replaces the thunk’s (<code>_s31P</code>) info table pointer with <code>stg_BLACKHOLE_info</code> in line (2) (effectively making the thunk an indirection), and writes pointer to the evaluated object to the payload in line (1).</p>
<p>Now any code that uses this value needs to follow the pointer written to what was originally a thunk in line (1). This is done by the <a href="https://github.com/ghc/ghc/blob/cb6d8589c83247ec96d5faa82df3e93f419bbfe0/rts/StgMiscClosures.cmm#L295">entry code of <code>stg_BLACKHOLE_info</code></a>.</p>
<p>Now, because the GC copies objects from one heap to another, and updates any references to these moved objects in thread stacks (and in other roots), we can follow any indirections when copying blackhole objects, and replace references in thread stacks to the blackhole object with a reference to the object pointed to by the blackhole object. <a href="https://github.com/ghc/ghc/blob/cb6d8589c83247ec96d5faa82df3e93f419bbfe0/rts/sm/Evac.c#L732-L755">The code</a>:</p>
<pre class="sourceCode c"><code class="sourceCode c"><span class="kw">case</span> BLACKHOLE:
{
    StgClosure *r;
    <span class="dt">const</span> StgInfoTable *i;
    r = ((StgInd*)q)-&gt;indirectee;
    <span class="kw">if</span> (GET_CLOSURE_TAG(r) == <span class="dv">0</span>) {
        i = r-&gt;header.info;
        <span class="kw">if</span> (IS_FORWARDING_PTR(i)) {
            r = (StgClosure *)UN_FORWARDING_PTR(i);
            i = r-&gt;header.info;
        }
        <span class="kw">if</span> (i == &amp;stg_TSO_info
            || i == &amp;stg_WHITEHOLE_info
            || i == &amp;stg_BLOCKING_QUEUE_CLEAN_info
            || i == &amp;stg_BLOCKING_QUEUE_DIRTY_info) {
            copy(p,info,q,sizeofW(StgInd),gen_no);
            <span class="kw">return</span>;
        }
        ASSERT(i != &amp;stg_IND_info);
    }
    q = r;
    *p = r;
    <span class="kw">goto</span> loop;
}</code></pre>
<p>I don’t understand all the details in this code, but I think the important bits are the <code>q-&gt;indirectee</code> line which follows the pointer written in line (1) above, and <code>goto loop</code> which makes the garbage collector copy and return the object pointed by the blackhole.</p>
<p>After this we no longer have to follow a pointer to our evaluated thunk. Instead references to the thunk become references to the evaluated object.</p>
<h1 id="selector-thunk-evaluation">3. Selector thunk evaluation</h1>
<p>A selector thunk is a thunk of this form: (<a href="https://github.com/ghc/ghc/blob/cb6d8589c83247ec96d5faa82df3e93f419bbfe0/compiler/codeGen/StgCmmBind.hs#L267-L297">code</a>)</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">case</span> x <span class="kw">of</span>
  <span class="dt">C</span> x1 <span class="fu">...</span> xn <span class="ot">-&gt;</span> xm</code></pre>
<p>where <code>1 &gt;= m &gt;= n</code>, and <code>x</code> is a variable. The problem with such a thunk is that it keeps all of the fields of <code>x</code> live until the selector thunk is evaluated, even when <code>x</code> is evaluated by some other code. As an example where this happens, suppose we have this record:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">data</span> <span class="dt">R</span> <span class="fu">=</span> <span class="dt">R</span> { _<span class="ot">i1 ::</span> <span class="dt">Int</span>, _<span class="ot">i2 ::</span> <span class="dt">Int</span>, <span class="fu">...</span> other fields <span class="fu">...</span> }</code></pre>
<p>then in a function we take <code>R</code> as parameter, and use the fields:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">f ::</span> <span class="dt">R</span> <span class="ot">-&gt;</span> <span class="dt">Int</span>
f r <span class="fu">=</span> _i1 r <span class="fu">+</span> _i2 r</code></pre>
<p>Here <code>_i1 r</code> and <code>_i2 r</code> are selector thunks. Now suppose that the parameter to this function was already evaluated before the function is called. In this case the thunk that holds the this function application will keep all of <code>r</code> live even though only <code>_i1</code> and <code>_i2</code> are needed.</p>
<p>It turns out this problem was known since around 1985. To my knowledge, Wadler was the first one to suggest solving these kind of “leaks” in the garbage collector <a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>. The idea is that while the GC copies these thunks it checks if the “selectee” is already evaluated. If so the GC evaluates the selector thunk during copying, and copies the evaluated form. Because selectors thunks are so simple (the exact shape of a selector thunk is well specified and it can’t do anything other than accessing a field) evaluation of these are just a matter of indexing the selectee’s payload. The function that does this is <a href="https://github.com/ghc/ghc/blob/cb6d8589c83247ec96d5faa82df3e93f419bbfe0/rts/sm/Evac.c#L1002">here</a>. The whole story is complicated because of concurrency concerns (e.g. another GC thread can also evaluate the thunk at the same time), but the actual optimization starts around line 1104 by looking at info table at the selectee. If it’s a constructor, then we access to the field and return it. Otherwise it’s a thunk and we copy it as usual.</p>
<h1 id="conclusion">Conclusion</h1>
<p>In each cycle a copying garbage collector copies live data in a heap to another heap and abandons the old heap. It turns out this kind of garbage collection is really convenient for implementing optimizations described above. The code that traverses all live data, copies it, and updates the roots is already there. Doing updates on objects while copying is just a matter of adding a few more lines in the copying function.</p>
<p>In a non-copying collector this is much trickier, because the collector doesn’t actually need to update roots or the data. For example, to implement optimizations (2) in a mark-sweep collector we have to somehow keep track of the location where we found the pointer to the object we’re currently marking. Then, if the object became an indirection, we have to update the source location and should not mark the indirection object, because some other object may have a reference to it, and we have to update that reference too. In short, it’s certainly possible, but much trickier. Mark phase gets more complicated.</p>
<p>In summary,</p>
<ul>
<li><p>Generational copying collectors are known to be a good fit for functional languages. It turns out if your language is also lazy they’re even better fit.</p></li>
<li><p>Laziness have far-reaching consequences. The optimizations (2) and (3) are really essential to get good performance out of lazy programs (try commenting out those lines in the GC!), and they require support from the GC.</p></li>
</ul>
<hr />
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>“Fixing some space leaks with a garbage collector”, Wadler, 1987.<a href="#fnref1">↩</a></p></li>
</ol>
</div>]]></summary>
</entry>
<entry>
    <title>Fuzzy module loading in GHCi</title>
    <link href="http://osa1.net/posts/2017-10-26-ghc-fuzzy-module-loading.html" />
    <id>http://osa1.net/posts/2017-10-26-ghc-fuzzy-module-loading.html</id>
    <published>2017-10-26T00:00:00Z</published>
    <updated>2017-10-26T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p><a href="https://downloads.haskell.org/~ghc/8.2.1/docs/html/users_guide/ghci.html#ghci-cmd-:def">A GHCi macro</a> is just a function of type <code>String -&gt; IO String</code>. The argument is parameter of the macro, and return value is evaluated by GHCi as a command.</p>
<p>Using this and the <a href="http://hackage.haskell.org/package/process"><code>process</code></a> library we can implement a fuzzy module loader that works inside ghci:</p>
<div class="figure">
<img src="/images/fuzzy_ghci_load.gif" />

</div>
<p>(sorry for the gif quality)</p>
<p>Here I’m using <a href="https://github.com/junegunn/fzf">fzf</a> as the fuzzy file finder. Code for defining the macro:</p>
<pre><code>import System.IO (withFile, IOMode (WriteMode))
import System.Process (runProcess, waitForProcess)

:{
let loadFuzzy _ = do
      let f = &quot;/tmp/fzf_out&quot;
      withFile f WriteMode $ \h -&gt; do
        p &lt;- runProcess &quot;fzf&quot; [] Nothing Nothing Nothing (Just h) Nothing
        _ &lt;- waitForProcess p
        out &lt;- readFile f
        return (&quot;:load &quot; ++ init out)
:}

:def l loadFuzzy</code></pre>
<p>Add this code to your global ghci config file (<code>~/.ghci</code>) or your project-wide <code>.ghci</code> (at the project root).</p>
<p>Only problem here is the <code>process</code> dependency: when you use <code>stack repl</code> or <code>cabal repl</code>, <code>process</code> won’t be importable in GHCi unless the project you’re loading into GHCi already has it as a dependency. One solution is to pass <code>-package process</code> to <code>cabal repl</code> or <code>--ghci-options=&quot;-package process&quot;</code> to <code>stack repl</code>. Because <code>process</code> is distributed with GHC this will always work.</p>]]></summary>
</entry>
<entry>
    <title>A parallel scheduler in 50 lines of Haskell</title>
    <link href="http://osa1.net/posts/2017-10-16-a-parallel-scheduler.html" />
    <id>http://osa1.net/posts/2017-10-16-a-parallel-scheduler.html</id>
    <published>2017-10-16T00:00:00Z</published>
    <updated>2017-10-16T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>Here’s a problem:</p>
<ul>
<li>You have N resources.</li>
<li>You have M tasks.</li>
<li>Each task requires exclusive access to a subset of the resources.</li>
<li>Tasks can be run in parallel.</li>
</ul>
<p>Implement a scheduler that runs these tasks in parallel, utilizing available resources as much as possible.</p>
<p>The code I’ll show here piggybacks on GHC RTS for scheduling. But for that we first have to implement our resources and tasks in a way that exposes necessary information to GHC’s scheduler. The idea is simple and fun to implement, but I can’t recommended using it in production :-) Scheduling is a hard problem, with many variations, and I’ve only recently started reading about it. This solution is a fun one than anything else.</p>
<hr />
<p>The idea is simple; we implement resources as <code>MVar</code>s and tasks as threads. Threads (tasks) take the <code>MVar</code>s before performing the operation. Because threads are scheduled by GHC RTS, GHC handles scheduling of our tasks. Because of fairness properties of <code>MVar</code>s, our threads are scheduled “fairly”, e.g. all tasks eventually finish even when we have infinitely many tasks.</p>
<p>A resource is an abstract object with a lock and unique identifier:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">data</span> <span class="dt">Resource</span> <span class="fu">=</span> <span class="dt">Resource</span>
  { _<span class="ot">resourceName ::</span> <span class="dt">T.Text</span>
  , _<span class="ot">resourceId   ::</span> <span class="dt">Unique</span>
  , _<span class="ot">resourceLock ::</span> <span class="dt">MVar</span> ()
  }

<span class="kw">instance</span> <span class="dt">Show</span> <span class="dt">Resource</span> <span class="kw">where</span>
  show <span class="fu">=</span> T.unpack <span class="fu">.</span> _resourceName</code></pre>
<p><code>_resourceName</code> is just a string to be used for tracing program execution.</p>
<p>A <code>Unique</code> is an integer that can be used in at most one resource:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">newtype</span> <span class="dt">Unique</span> <span class="fu">=</span> <span class="dt">Unique</span> <span class="dt">Int</span>
  <span class="kw">deriving</span> (<span class="dt">Eq</span>, <span class="dt">Ord</span>)</code></pre>
<p>Using <code>Unique</code> we can define a total order for <code>Resource</code>:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">instance</span> <span class="dt">Eq</span> <span class="dt">Resource</span> <span class="kw">where</span>
  (<span class="fu">==</span>) <span class="fu">=</span> (<span class="fu">==</span>) <span class="ot">`on`</span> _resourceId

<span class="kw">instance</span> <span class="dt">Ord</span> <span class="dt">Resource</span> <span class="kw">where</span>
  compare <span class="fu">=</span> comparing _resourceId</code></pre>
<p>A task that requires exclusive access to a subset of all resources can be implemented using <code>withResources</code>:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">withResources ::</span> (<span class="dt">MonadLogger</span> m, <span class="dt">MonadBaseControl</span> <span class="dt">IO</span> m) <span class="ot">=&gt;</span> <span class="dt">S.Set</span> <span class="dt">Resource</span> <span class="ot">-&gt;</span> m () <span class="ot">-&gt;</span> m ()
withResources locks a <span class="fu">=</span> acquire_locks (S.toList locks)
  <span class="kw">where</span>
    acquire_locks ls <span class="fu">=</span> <span class="kw">case</span> ls <span class="kw">of</span>
      [] <span class="ot">-&gt;</span>
        a
      l <span class="fu">:</span> ls&#39; <span class="ot">-&gt;</span> <span class="kw">do</span>
        logDebug (<span class="st">&quot;taking lock &quot;</span> <span class="fu">&lt;&gt;</span> (_resourceName l))
        withMVar (_resourceLock l) <span class="fu">$</span> \() <span class="ot">-&gt;</span>
          acquire_locks ls&#39;</code></pre>
<p>Note that when all tasks are implemented using this function a deadlock won’t occur: resources are ordered, and <code>S.toList</code> generates a sorted list, which in turn causes <code>acquire_locks</code> to take locks in order, effectively implementing <a href="https://en.wikipedia.org/wiki/Dining_philosophers_problem">Dijkstra’s resource hierarchy solution to the dining philosophers problem</a>.</p>
<p>Here are three task generators:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">newtype</span> <span class="dt">Task</span> <span class="fu">=</span> <span class="dt">Task</span>
  {<span class="ot"> runTask ::</span> forall m <span class="fu">.</span> (<span class="dt">MonadLogger</span> m, <span class="dt">MonadBaseControl</span> <span class="dt">IO</span> m) <span class="ot">=&gt;</span> m () }

<span class="ot">mkFastTask ::</span> <span class="dt">Int</span> <span class="ot">-&gt;</span> <span class="dt">S.Set</span> <span class="dt">Resource</span> <span class="ot">-&gt;</span> <span class="dt">Task</span>
mkFastTask i res <span class="fu">=</span>
    <span class="dt">Task</span> <span class="fu">$</span> withResources res <span class="fu">$</span> <span class="kw">do</span>
      logDebug (<span class="st">&quot;Performing &quot;</span> <span class="fu">&lt;&gt;</span> T.pack (show i))
      threadDelay (<span class="dv">500</span><span class="ot"> ::</span> <span class="dt">Milliseconds</span>)
      logDebug (<span class="st">&quot;Fast task done (&quot;</span> <span class="fu">&lt;&gt;</span> T.pack (show i) <span class="fu">&lt;&gt;</span> <span class="st">&quot;)&quot;</span>)

<span class="ot">mkSlowTask ::</span> <span class="dt">Int</span> <span class="ot">-&gt;</span> <span class="dt">S.Set</span> <span class="dt">Resource</span> <span class="ot">-&gt;</span> <span class="dt">Task</span>
mkSlowTask i res <span class="fu">=</span>
    <span class="dt">Task</span> <span class="fu">$</span> withResources res <span class="fu">$</span> <span class="kw">do</span>
      logDebug (<span class="st">&quot;Performing &quot;</span> <span class="fu">&lt;&gt;</span> T.pack (show i))
      threadDelay (<span class="dv">3</span><span class="ot"> ::</span> <span class="dt">Seconds</span>)
      logDebug (<span class="st">&quot;Slow task done (&quot;</span> <span class="fu">&lt;&gt;</span> T.pack (show i) <span class="fu">&lt;&gt;</span> <span class="st">&quot;)&quot;</span>)

<span class="ot">mkCrashingTask ::</span> <span class="dt">Int</span> <span class="ot">-&gt;</span> <span class="dt">S.Set</span> <span class="dt">Resource</span> <span class="ot">-&gt;</span> <span class="dt">Task</span>
mkCrashingTask i res <span class="fu">=</span>
    <span class="dt">Task</span> <span class="fu">$</span> withResources res <span class="fu">$</span> <span class="kw">do</span>
      logDebug (<span class="st">&quot;Performing &quot;</span> <span class="fu">&lt;&gt;</span> T.pack (show i))
      error <span class="st">&quot;task failed&quot;</span></code></pre>
<p>Integer arguments are just for tracing task execution in program output. <code>mkFastTask</code> generates a task that takes 500 milliseconds to run. <code>mkSlowTask</code> generates a task that takes 3 seconds. <code>mkCrashingTask</code> makes a task that throws an exception, demonstrating that we release resources properly on exceptions.</p>
<p>Finally, the scheduler just spawns tasks using <code>forkIO</code> or <code>async</code>:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">schedule ::</span> (<span class="dt">MonadLogger</span> m, <span class="dt">MonadBaseControl</span> <span class="dt">IO</span> m, <span class="dt">Forall</span> (<span class="dt">Pure</span> m)) <span class="ot">=&gt;</span> [<span class="dt">Task</span>] <span class="ot">-&gt;</span> m ()
schedule tasks <span class="fu">=</span> <span class="kw">do</span>
    thrs <span class="ot">&lt;-</span> forM tasks <span class="fu">$</span> \(<span class="dt">Task</span> task) <span class="ot">-&gt;</span>
              async (task <span class="ot">`catch`</span> (\(<span class="ot">e ::</span> <span class="dt">SomeException</span>) <span class="ot">-&gt;</span> logDebug <span class="st">&quot;Task failed&quot;</span>))
    forM_ thrs wait</code></pre>
<p>Here’s an example run</p>
<pre><code>taking lock resource5
Performing 0
taking lock resource0
Performing 1
taking lock resource2
taking lock resource6
taking lock resource7
Performing 2
Task failed
taking lock resource6
Performing 3
taking lock resource8
Performing 4
taking lock resource1
taking lock resource2
Performing 5
Task failed
taking lock resource2
taking lock resource3
taking lock resource8
taking lock resource0
taking lock resource3
taking lock resource4
Performing 9
Fast task done (3)
Fast task done (9)
Fast task done (0)
Slow task done (1)
taking lock resource4
taking lock resource8
Slow task done (4)
Performing 6
Fast task done (6)
taking lock resource7
Performing 8
Task failed
Performing 7
Slow task done (7)</code></pre>
<p>The whole code that randomly generates resources and tasks and then runs them is <a href="https://gist.github.com/osa1/e7416f6a0f299f88f275bb8d56a31da3">here</a>. It uses quite a lot of dependencies because it was extracted from a larger program, and I’m too lazy to make it smaller and simpler. I provided a <code>stack.yaml</code> so hopefully it’s still not too hard to run.</p>]]></summary>
</entry>
<entry>
    <title>Enable these two flags in GHC 8.2</title>
    <link href="http://osa1.net/posts/2017-07-15-two-flags.html" />
    <id>http://osa1.net/posts/2017-07-15-two-flags.html</id>
    <published>2017-07-15T00:00:00Z</published>
    <updated>2017-07-15T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>As usual, the next major GHC release will be <a href="https://ghc.haskell.org/trac/ghc/wiki/Status/GHC-8.2.1#Releasehighlights">pretty great</a>. It’ll come with a bunch of new features that I can’t wait to start using, and I’ve contributed to three of the new features<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>, but what excites me the most is not any of these features. I’m most excited about the improved <code>-Werror</code> flag.</p>
<p>The summary is that with GHC 8.2 we’ll be able to promote some warnings into errors, without making <em>all warnings</em> errors (which is how <code>-Werror</code> worked pre-8.2). With this we can finally fix some of the Haskell 2010 warts.</p>
<hr />
<p>By the beginning of this year I moved from academia to industry. I was writing Haskell in academia, and I’m still writing Haskell, but the environment, tasks, and constraints are quite different, so the way I write Haskell changed quite a lot during this transition.</p>
<p>What I realized that some of the problems with Haskell 2010 are actually worse than I had previously thought.</p>
<h2 id="problem-1-initializing-records-without-initializing-all-of-its-fields">Problem 1: Initializing records without initializing all of its fields</h2>
<p><a href="https://www.haskell.org/onlinereport/haskell2010/haskellch3.html#x8-520003.15.2">Haskell 2010</a> says that when initializing records using labels, fields not mentioned are initialized as bottoms.</p>
<p>I just can’t fathom how Haskell 2010 people thought this is a good idea. In Haskell we constantly rely on compile-time errors to refactor our code. A common workflow is this: you update your data types, and follow the type errors to adapt your code to the changes. Quite often your program works as expected after this. I did this countless times during my career as a Haskell programmer, and I’m trying to improve GHC to <a href="https://github.com/ghc-proposals/ghc-proposals/pull/43">make this workflow even more efficient</a>.</p>
<p>The problem is this “feature” breaks this workflow, because adding a new field to a type no longer generates a compile error. It generates a warning, but that’s not good enough because (1) not all projects have <code>-Wall</code> enabled (2) not all projects are warning-free, which means new warnings sometimes go unnoticed (this happens in our code base all the time).</p>
<p>Indeed, even very experienced Haskellers release buggy code because of this. For example, warp-3.2.10 added a new field (<code>connFree</code>) to one of its types (<code>Connection</code>), and for some reason only the minor version was bumped (3.2.9 to 3.2.10, which is probably wrong according to <a href="https://pvp.haskell.org/">PVP</a> because the type was exported). The problem was warp-tls-3.2.2 had 3.3 as warp upper bound, so it compiled fine against warp-3.2.10, even though it didn’t initialize the field. This caused bugs in our system, which we thankfully discovered in our test environment rather than on production. The fix was <a href="https://github.com/yesodweb/wai/commit/b63ec0e865cf91af4143416adaf430969ba0ebb5#diff-44ce89cb2a54be5e525d74b83901f561R348">easy</a>, but the damage was done (the buggy warp-tls-3.2.2 is still on Hackage).</p>
<h2 id="problem-2-non-exhaustive-pattern-matching">Problem 2: Non-exhaustive pattern matching</h2>
<p>While I can’t find any mention to exhaustiveness of pattern matching in Haskell 2010, it clearly covers the case where patterns do not cover all values when defining <a href="https://www.haskell.org/onlinereport/haskell2010/haskellch3.html#x8-610003.17.3">formal semantics of pattern matching</a> (see case (b)). You only realize that this is a bad idea when (1) your code is not warning-free so new warnings sometimes go unnoticed and (2) you can’t promote individual warnings to errors. This again breaks the workflow I mentioned above, and makes code reviews much harder.</p>
<hr />
<p>The solution that GHC 8.2 brings is we can now make these two warnings errors, using <code>-Werror=missing-fields -Werror=incomplete-patterns</code>. There’s still one problem though, the error message is not good enough. Suppose we had this code:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">module</span> <span class="dt">Lib</span> <span class="kw">where</span>

<span class="kw">data</span> <span class="dt">Rec</span> <span class="fu">=</span> <span class="dt">Rec</span> {<span class="ot"> f1 ::</span> <span class="dt">Int</span>,<span class="ot"> f2 ::</span> <span class="dt">Int</span> }

<span class="kw">data</span> <span class="dt">S</span> <span class="fu">=</span> <span class="dt">C1</span> <span class="dt">Int</span> <span class="fu">|</span> <span class="dt">C2</span> <span class="dt">Int</span>

<span class="co">-- incomplete pattern</span>
sInt s <span class="fu">=</span> <span class="kw">case</span> s <span class="kw">of</span>
           <span class="dt">C1</span> i <span class="ot">-&gt;</span> i

<span class="co">-- missing field</span>
initRec <span class="fu">=</span> <span class="dt">Rec</span> { f1 <span class="fu">=</span> <span class="dv">1</span> }</code></pre>
<p>Compile this with <code>ghc -Wall</code> and you get:</p>
<pre><code>[1 of 1] Compiling Lib              ( test.hs, test.o )

test.hs:11:1: warning: [-Wmissing-signatures]
    Top-level binding with no type signature: sInt :: S -&gt; Int
   |
11 | sInt s = case s of
   | ^^^^

test.hs:11:10: warning: [-Wincomplete-patterns]
    Pattern match(es) are non-exhaustive
    In a case alternative: Patterns not matched: (C2 _)
   |
11 | sInt s = case s of
   |          ^^^^^^^^^...

test.hs:15:1: warning: [-Wmissing-signatures]
    Top-level binding with no type signature: initRec :: Rec
   |
15 | initRec = Rec { f1 = 1 }
   | ^^^^^^^

test.hs:15:11: warning: [-Wmissing-fields]
    • Fields of ‘Rec’ not initialised: f2
    • In the expression: Rec {f1 = 1}
      In an equation for ‘initRec’: initRec = Rec {f1 = 1}
   |
15 | initRec = Rec { f1 = 1 }
   |           ^^^^^^^^^^^^^^</code></pre>
<p>We only care about missing fields and incomplete patterns, so with GHC 8.2 we compile this with <code>ghc -Wall -Werror=missing-fields -Werror=incomplete-patterns</code>, which generates the same warnings, but the process exits with non-zero, and prints these extra lines:</p>
<pre><code>&lt;no location info&gt;: error:
Failing due to -Werror.</code></pre>
<p>This is not too useful, because if you get dozens of warnings there’s basically no way of knowing which of those warnings caused this error. One alternative is to disable <code>-Wall</code> and only use <code>-Werror</code>s. That way you know that the warnings you’re seeing are actually errors.</p>
<p>Still, this is not entirely satisfactory, because even though we don’t cause our build to fail when we have warnings, they’re still sometimes useful to see (for example, name shadowing warnings often catches accidental loops). So to improve this I recently <a href="https://phabricator.haskell.org/D3709">submitted a patch</a>, which is merged, but unfortunately won’t make it to GHC 8.2 (hopefully we’ll see it in GHC 8.4). With that patch when you have both <code>-Wall</code> and some <code>-Werror</code>s, you see this instead:</p>
<pre><code>[1 of 1] Compiling Lib              ( test.hs, test.o )

test.hs:11:1: warning: [-Wmissing-signatures]
    Top-level binding with no type signature: sInt :: S -&gt; Int
   |
11 | sInt s = case s of
   | ^^^^

test.hs:11:10: error: [-Wincomplete-patterns, -Werror=incomplete-patterns]
    Pattern match(es) are non-exhaustive
    In a case alternative: Patterns not matched: (C2 _)
   |
11 | sInt s = case s of
   |          ^^^^^^^^^...

test.hs:15:1: warning: [-Wmissing-signatures]
    Top-level binding with no type signature: initRec :: Rec
   |
15 | initRec = Rec { f1 = 1 }
   | ^^^^^^^

test.hs:15:11: error: [-Wmissing-fields, -Werror=missing-fields]
    • Fields of ‘Rec’ not initialised: f2
    • In the expression: Rec {f1 = 1}
      In an equation for ‘initRec’: initRec = Rec {f1 = 1}
   |
15 | initRec = Rec { f1 = 1 }
   |           ^^^^^^^^^^^^^^</code></pre>
<p>Much better!</p>
<p>This is probably not as exciting to many people as, say, new features like compact regions or join points, but I think this will significantly improve “refactor types, folow type error, repeat” style workflows and make code reviews much easier.</p>
<hr />
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>I discovered and reported <a href="https://ghc.haskell.org/trac/ghc/ticket/10598">#10598</a> two years ago, which led to <code>-XDerivingStrategies</code> work, I was involved in the <a href="http://ezyang.com/papers/ezyang15-cnf.pdf">Compact regions paper</a>, and I implemented <a href="https://phabricator.haskell.org/D2259">unboxed sums</a> during my time at MSR Cambridge last summer.<a href="#fnref1">↩</a></p></li>
</ol>
</div>]]></summary>
</entry>
<entry>
    <title>IORef and STRef under the hood</title>
    <link href="http://osa1.net/posts/2016-07-25-IORef-STRef-exposed.html" />
    <id>http://osa1.net/posts/2016-07-25-IORef-STRef-exposed.html</id>
    <published>2016-07-25T00:00:00Z</published>
    <updated>2016-07-25T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>In this post we’ll take a look at internals of GHC’s mutable variables, and how they’re used by <a href="http://hackage.haskell.org/package/base-4.9.0.0/docs/Data-IORef.html"><code>IORef</code></a> and <a href="http://hackage.haskell.org/package/base-4.9.0.0/docs/Data-STRef.html"><code>STRef</code></a>. The code is copied from GHC, with minor changes for clarity.</p>
<hr />
<pre><code>λ&gt; :m + Data.IORef
λ&gt; :info IORef
newtype IORef a
  = GHC.IORef.IORef (GHC.STRef.STRef GHC.Prim.RealWorld a)
        -- Defined in ‘GHC.IORef’
instance Eq (IORef a) -- Defined in ‘GHC.IORef’</code></pre>
<p><code>GHC.IORef</code> is defined in <code>libraries/base/GHC/IORef.hs</code>:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="co">-- | A mutable variable in the &#39;IO&#39; monad</span>
<span class="kw">newtype</span> <span class="dt">IORef</span> a <span class="fu">=</span> <span class="dt">IORef</span> (<span class="dt">STRef</span> <span class="dt">RealWorld</span> a)</code></pre>
<p>We’ll look at 3 operations: read, write, and atomic modify.</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="co">-- | Read the value of an &#39;IORef&#39;</span>
<span class="ot">readIORef   ::</span> <span class="dt">IORef</span> a <span class="ot">-&gt;</span> <span class="dt">IO</span> a
readIORef  (<span class="dt">IORef</span> var) <span class="fu">=</span> stToIO (readSTRef var)

<span class="co">-- | Write a new value into an &#39;IORef&#39;</span>
<span class="ot">writeIORef  ::</span> <span class="dt">IORef</span> a <span class="ot">-&gt;</span> a <span class="ot">-&gt;</span> <span class="dt">IO</span> ()
writeIORef (<span class="dt">IORef</span> var) v <span class="fu">=</span> stToIO (writeSTRef var v)

<span class="ot">atomicModifyIORef ::</span> <span class="dt">IORef</span> a <span class="ot">-&gt;</span> (a <span class="ot">-&gt;</span> (a,b)) <span class="ot">-&gt;</span> <span class="dt">IO</span> b
atomicModifyIORef (<span class="dt">IORef</span> (<span class="dt">STRef</span> r<span class="st">#)) f = IO $ \s -&gt; atomicModifyMutVar# r# f s</span></code></pre>
<p><code>STRef</code> is defined in <code>libraries/base/GHC/STRef.hs</code>:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="co">-- | A value of type `STRef s a` is a mutable variable in state thread `s`,</span>
<span class="co">-- containing a value of type `a`</span>
<span class="kw">data</span> <span class="dt">STRef</span> s a <span class="fu">=</span> <span class="dt">STRef</span> (<span class="dt">MutVar</span><span class="st"># s a)</span>

<span class="co">-- | Read the value of an &#39;STRef&#39;</span>
<span class="ot">readSTRef ::</span> <span class="dt">STRef</span> s a <span class="ot">-&gt;</span> <span class="dt">ST</span> s a
readSTRef (<span class="dt">STRef</span> var<span class="st">#) = ST $ \s1# -&gt; readMutVar# var# s1#</span>

<span class="co">-- | Write a new value into an &#39;STRef&#39;</span>
<span class="ot">writeSTRef ::</span> <span class="dt">STRef</span> s a <span class="ot">-&gt;</span> a <span class="ot">-&gt;</span> <span class="dt">ST</span> s ()
writeSTRef (<span class="dt">STRef</span> var<span class="st">#) val = ST $ \s1# -&gt;</span>
    <span class="kw">case</span> writeMutVar<span class="st"># var# val s1# of</span>
      s2<span class="st"># -&gt; (# s2#, () #)</span></code></pre>
<p>Note that there’s no <code>atomicModifySTRef</code>, because that only makes sense in IO context. So <code>atomicModifyIORef</code> directly calls the primop.</p>
<p>In summary:</p>
<ul>
<li>IORef: <code>MutVar#</code>, wrapped with <code>STRef</code>. When we unpack an <code>IORef</code> in data constructor fields, internally we store a <code>MutVar#</code>.</li>
<li>writeIORef, writeSTRef: <code>writeMutVar#</code></li>
<li>readIORef, readSTRef: <code>readMutVar#</code></li>
<li>atomicModifyIORef: <code>atomicModifyMutVar#</code></li>
</ul>
<h1 id="readmutvar">readMutVar#</h1>
<p>Primop definition:</p>
<pre><code>primop  ReadMutVarOp &quot;readMutVar#&quot; GenPrimOp
   MutVar# s a -&gt; State# s -&gt; (# State# s, a #)
   {Read contents of {\tt MutVar\#}. Result is not yet evaluated.}
   with
   has_side_effects = True
   can_fail         = True</code></pre>
<p>Code generation is handled by <code>StgCmmPrim.emitPrimOp</code>:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">emitPrimOp ::</span> <span class="dt">DynFlags</span>
           <span class="ot">-&gt;</span> [<span class="dt">LocalReg</span>]        <span class="co">-- where to put the results</span>
           <span class="ot">-&gt;</span> <span class="dt">PrimOp</span>            <span class="co">-- the op</span>
           <span class="ot">-&gt;</span> [<span class="dt">CmmExpr</span>]         <span class="co">-- arguments</span>
           <span class="ot">-&gt;</span> <span class="dt">FCode</span> ()

<span class="fu">...</span>

emitPrimOp dflags [res] <span class="dt">ReadMutVarOp</span> [mutv]
   <span class="fu">=</span> emitAssign (<span class="dt">CmmLocal</span> res) (cmmLoadIndexW dflags mutv (fixedHdrSizeW dflags) (gcWord dflags))</code></pre>
<p>This is just relative addressing, base is the <code>MutVar#</code> we’re reading, and we skip the closure header to read the contents.</p>
<h1 id="writemutvar">writeMutVar#</h1>
<p>Primop definition:</p>
<pre><code>primop  WriteMutVarOp &quot;writeMutVar#&quot;  GenPrimOp
   MutVar# s a -&gt; a -&gt; State# s -&gt; State# s
   {Write contents of {\tt MutVar\#}.}
   with
   has_side_effects = True
   code_size        = { primOpCodeSizeForeignCall }
                         -- for the write barrier
   can_fail         = True</code></pre>
<p>Code generation is again implemented in <code>emitPrimOp</code>:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell">emitPrimOp dflags [] <span class="dt">WriteMutVarOp</span> [mutv,var]
   <span class="fu">=</span> <span class="kw">do</span> emitStore (cmmOffsetW dflags mutv (fixedHdrSizeW dflags)) var
        emitCCall [<span class="co">{-no results-}</span>]
                  (<span class="dt">CmmLit</span> (<span class="dt">CmmLabel</span> mkDirty_MUT_VAR_Label))
                  [(<span class="dt">CmmReg</span> (<span class="dt">CmmGlobal</span> <span class="dt">BaseReg</span>), <span class="dt">AddrHint</span>), (mutv,<span class="dt">AddrHint</span>)]</code></pre>
<p>This is more involved than <code>readMutVar#</code>. First, we write the variable to the <code>MutVar#</code> in the first line (<code>emitStore</code>). Then, we call a function, specified by the variable <code>mkDirty_MUT_VAR_Label</code>, passing two arguments: a global called <code>BaseReg</code>, and the <code>MutVar#</code>. <code>mkDirty_MUT_VAR_Label</code> just holds the name of this function: (defined in <code>rts/sm/Storage.c</code>)</p>
<pre class="sourceCode c"><code class="sourceCode c"><span class="co">/*</span>
<span class="co">   This is the write barrier for MUT_VARs, a.k.a. IORefs.  A</span>
<span class="co">   MUT_VAR_CLEAN object is not on the mutable list; a MUT_VAR_DIRTY</span>
<span class="co">   is.  When written to, a MUT_VAR_CLEAN turns into a MUT_VAR_DIRTY</span>
<span class="co">   and is put on the mutable list.</span>
<span class="co">*/</span>
<span class="dt">void</span> dirty_MUT_VAR(StgRegTable *reg, StgClosure *p)
{
    Capability *cap = regTableToCapability(reg);
    <span class="kw">if</span> (p-&gt;header.info == &amp;stg_MUT_VAR_CLEAN_info) {
        p-&gt;header.info = &amp;stg_MUT_VAR_DIRTY_info;
        recordClosureMutated(cap,p);
    }
}</code></pre>
<p>Remember that for the first argument we passed something called <code>BaseReg</code>, and for the second argument we passed the <code>MutVar#</code>.</p>
<p>This function gets a <code>Capability</code> from the register table, and if the <code>MutVar#</code> is “clean”, it marks it as “dirty” and records in the capability that it’s now mutated.</p>
<p><code>Capability</code> lacks documentation, but it’s not too important, so we just skip that and look at <code>recordClsoureMutated</code>.</p>
<pre class="sourceCode c"><code class="sourceCode c"><span class="dt">void</span> recordClosureMutated(Capability *cap, StgClosure *p)
{
    bdescr *bd = Bdescr((StgPtr)p);
    <span class="kw">if</span> (bd-&gt;gen_no != <span class="dv">0</span>) recordMutableCap(p,cap,bd-&gt;gen_no);
}</code></pre>
<p><code>p</code> is our <code>MutVar#</code> here. <code>bdescr</code> stands for “block descriptor”. GHC RTS allocates memory in blocks, and every block belongs to a generation. First generation is special in that if a <code>MutVar#</code> is in the first generation, it can’t point to a younger generation, as the first generation is already the youngest generation. This is from <code>includes/rts/storage/GC.h</code>:</p>
<pre><code>- generation 0 is the allocation area.  It is given a fixed set of blocks
  during initialisation, and these blocks normally stay in G0S0.  In parallel
  execution, each Capability has its own nursery.</code></pre>
<p>This code basically checks if the <code>MutVar#</code> belongs to first generation (generation 0). If that’s not the case, we record the <code>MutVar#</code> in the generation’s “mut list”:</p>
<pre class="sourceCode c"><code class="sourceCode c"><span class="dt">void</span> recordMutableCap(<span class="dt">const</span> StgClosure *p, Capability *cap, <span class="dt">uint32_t</span> gen)
{
    bdescr* bd = cap-&gt;mut_lists[gen];
    <span class="kw">if</span> (bd-&gt;free &gt;= bd-&gt;start + BLOCK_SIZE_W) {
        bdescr *new_bd = allocBlockOnNode_lock(cap-&gt;node);
        new_bd-&gt;link = bd;
        bd = new_bd;
        cap-&gt;mut_lists[gen] = bd;
    }
    *bd-&gt;free++ = (StgWord)p;
}</code></pre>
<p>Garbage collector then checks that list when collecting younger generations, to avoid collecting young objects kept alive by older generations (i.e. pointers from older generations to younger generations, see <code>scavenge_capability_mut_lists</code> in <code>rts/sm/Scav.c</code>).</p>
<p>We saw in <code>dirty_MUT_VAR</code> that the <code>MutVar#</code> is marked as “dirty” when it’s mutated. When is it marked as “clean” again?</p>
<p>When a <code>MutVar#</code> is copied during GC, the object pointed by it is also copied to the same generation, and then the <code>MutVar#</code> becomes clean again, because it no longer points to a younger generation. This is the related code:</p>
<pre class="sourceCode c"><code class="sourceCode c"><span class="dt">static</span> <span class="dt">void</span>
scavenge_block(bdescr *bd)
{
    ...
    <span class="kw">case</span> MUT_VAR_DIRTY:
        gct-&gt;eager_promotion = rtsFalse;
        evacuate(&amp;((StgMutVar *)p)-&gt;var);
        gct-&gt;eager_promotion = saved_eager_promotion;
        <span class="kw">if</span> (gct-&gt;failed_to_evac) {
            ((StgClosure *)q)-&gt;header.info = &amp;stg_MUT_VAR_DIRTY_info;
        } <span class="kw">else</span> {
            ((StgClosure *)q)-&gt;header.info = &amp;stg_MUT_VAR_CLEAN_info;
        }
        p += sizeofW(StgMutVar);
        <span class="kw">break</span>;
    ...
}</code></pre>
<h1 id="atomicmodifymutvar">atomicModifyMutVar#</h1>
<p>Primop definition:</p>
<pre><code>primop  AtomicModifyMutVarOp &quot;atomicModifyMutVar#&quot; GenPrimOp
   MutVar# s a -&gt; (a -&gt; b) -&gt; State# s -&gt; (# State# s, c #)
   with
   out_of_line      = True
   has_side_effects = True
   can_fail         = True</code></pre>
<p><code>out_of_line = True</code> basically tells code generator that this primop is implemented as a function. Code generator then just generates a function call:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">cgOpApp ::</span> <span class="dt">StgOp</span>        <span class="co">-- The op</span>
        <span class="ot">-&gt;</span> [<span class="dt">StgArg</span>]     <span class="co">-- Arguments</span>
        <span class="ot">-&gt;</span> <span class="dt">Type</span>         <span class="co">-- Result type (always an unboxed tuple)</span>
        <span class="ot">-&gt;</span> <span class="dt">FCode</span> <span class="dt">ReturnKind</span>

<span class="fu">...</span>

cgOpApp (<span class="dt">StgPrimOp</span> primop) args res_ty <span class="fu">=</span> <span class="kw">do</span>
    dflags <span class="ot">&lt;-</span> getDynFlags
    cmm_args <span class="ot">&lt;-</span> getNonVoidArgAmodes args
    <span class="kw">case</span> shouldInlinePrimOp dflags primop cmm_args <span class="kw">of</span>
        <span class="dt">Nothing</span> <span class="ot">-&gt;</span> <span class="kw">do</span>  <span class="co">-- out-of-line</span>
          <span class="kw">let</span> fun <span class="fu">=</span> <span class="dt">CmmLit</span> (<span class="dt">CmmLabel</span> (mkRtsPrimOpLabel primop))
          emitCall (<span class="dt">NativeNodeCall</span>, <span class="dt">NativeReturn</span>) fun cmm_args
        <span class="fu">...</span></code></pre>
<p>The primop is implemented in Cmm, in <code>rts/PrimOps.cmm</code>. The code is a mess, but here’s the important part:</p>
<pre class="sourceCode c"><code class="sourceCode c">stg_atomicModifyMutVarzh ( gcptr mv, gcptr f )
{
  ...
  retry:
    x = StgMutVar_var(mv);
    StgThunk_payload(z,<span class="dv">1</span>) = x;
<span class="ot">#ifdef THREADED_RTS</span>
    (h) = ccall cas(mv + SIZEOF_StgHeader + OFFSET_StgMutVar_var, x, y);
    <span class="kw">if</span> (h != x) { <span class="kw">goto</span> retry; }
<span class="ot">#else</span>
    StgMutVar_var(mv) = y;
<span class="ot">#endif</span>

    <span class="kw">if</span> (GET_INFO(mv) == stg_MUT_VAR_CLEAN_info) {
        ccall dirty_MUT_VAR(BaseReg <span class="st">&quot;ptr&quot;</span>, mv <span class="st">&quot;ptr&quot;</span>);
    }

    <span class="kw">return</span> (r);
}</code></pre>
<p>It’s basically a compare-and-swap loop, and in the end it marks the <code>MutVar#</code> as “dirty”, using the same <code>dirty_MUT_VAR</code> function used by the code generated for <code>writeMutVar#</code>.</p>
<h1 id="the-mutvar-struct">The <code>MutVar#</code> struct</h1>
<p>As the last thing, we look at the definition of <code>MutVar#</code>: (in <code>includes/rts/storage/Closures.h</code>)</p>
<pre><code>typedef struct {
    StgHeader   header;
    StgClosure *var;
} StgMutVar;</code></pre>
<p>Nothing interesting here. See <a href="https://ghc.haskell.org/trac/ghc/wiki/Commentary/Rts/Storage/HeapObjects?redirectedfrom=Commentary/Rts/HeapObjects">this Wiki page</a> for GHC’s heap object layout. In our case, payload contains a single closure.</p>
<hr />
<p>This concludes our <code>MutVar#</code> (which is used under the hood for <code>IORef</code> and <code>STRef</code>) tour. I guess lessons here are:</p>
<ol style="list-style-type: decimal">
<li><p><code>readIORef</code> is fast, but <code>writeIORef</code> is one function call in the best case. In the worst case, it does an expensive allocation (this is not just a heap pointer bump). If you have a tight loop with some state variables, prefer parameter passing instead.</p></li>
<li><p>Unpacking an <code>IORef</code> in a data constructor field does not really make the constructor mutable. Instead, it inlines the <code>MutVar#</code>, which has a mutable pointer field.</p></li>
</ol>
<p>If you think about it a little bit, you may realize that optimizing (2) is actually quite tricky. Imagine having something like this:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">data</span> <span class="dt">D</span> <span class="fu">=</span> <span class="dt">D</span> {<span class="ot"> f1 ::</span> <span class="ot">{-# UNPACK #-}</span> <span class="fu">!</span>(<span class="dt">IORef</span> <span class="dt">Int</span>)
           ,<span class="ot"> f2 ::</span> <span class="dt">Int</span>
           }

<span class="ot">bumpf1 ::</span> <span class="dt">D</span> <span class="ot">-&gt;</span> <span class="dt">IO</span> ()
bumpf1 (<span class="dt">D</span> f1 _) <span class="fu">=</span> modifyIORef f1 (<span class="fu">+</span> <span class="dv">1</span>)

<span class="ot">bumpf2 ::</span> <span class="dt">D</span> <span class="ot">-&gt;</span> <span class="dt">D</span>
bumpf2 (<span class="dt">D</span> f1 f2) <span class="fu">=</span> <span class="dt">D</span> f1 (f2 <span class="fu">+</span> <span class="dv">1</span>)</code></pre>
<p>You’d expect this to print <code>True</code>:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">do</span> ref <span class="ot">&lt;-</span> newIORef <span class="dv">0</span>
   <span class="kw">let</span> d1 <span class="fu">=</span> <span class="dt">D</span> ref <span class="dv">0</span>
       d2 <span class="fu">=</span> bumpD2 d1
   bumpf1 d1
   rv1 <span class="ot">&lt;-</span> readIORef (f1 d1)
   rv2 <span class="ot">&lt;-</span> readIORef (f1 d2)
   print (rv1 <span class="fu">==</span> rv2)</code></pre>
<p>If <code>D</code> becomes mutable after the <code>UNPACK</code>, this code doesn’t work anymore, because we lose sharing after the functional update in line <code>bumpD2 d1</code>.</p>
<p>See also <a href="https://ghc.haskell.org/trac/ghc/ticket/7662#comment:3">this discussion</a> for how other compilers improve this.</p>]]></summary>
</entry>
<entry>
    <title>Unboxed sums FAQ</title>
    <link href="http://osa1.net/posts/2016-07-22-unboxed-sums-faq.html" />
    <id>http://osa1.net/posts/2016-07-22-unboxed-sums-faq.html</id>
    <published>2016-07-22T00:00:00Z</published>
    <updated>2016-07-22T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>The unboxed sums patch that implements unlifted, unboxed sum types (as described in <a href="https://ghc.haskell.org/trac/ghc/wiki/UnpackedSumTypes">this Wiki page</a>) was merged yesterday, and a <a href="https://www.reddit.com/r/haskell/comments/4txuo7/unboxed_sum_types_with_unpack_support_will_be_in/">/r/haskell discussion</a> emerged shortly after. As the implementor, I tried to answer questions there, but to keep answers more organized I wanted to write a blog post about it.</p>
<p>The reason I’m not writing this to the Wiki page is because this is about current plans and status of the feature. The wiki page may be updated in the future as the feature evolves and/or may be edited by others. This page reflects the current status as of today, future plans, and my own ideas.</p>
<hr />
<h2 id="syntax-is-awful-why">Syntax is awful, why?</h2>
<p>This feature is designed to complement the similar feature for product types (tuples), called <a href="https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#unboxed-tuples">“unboxed tuples”</a>. The syntax is thus chosen to reflect this idea. Instead of commas in the unboxed tuple syntax, we used bars (similar to how bars used in sum type declarations). The syntax looks bad for several reasons:</p>
<ul>
<li><p>Type argument of an alternative have to be a single type. If we want multiple types in an alternative, we have to use an unboxed tuple. For example, unboxed sum version of the type <code>data T = T1 Int | T2 String Bool</code> is <code>(# Int | (# String, Bool #) #)</code>. That’s a lot of parens and hashes.</p></li>
<li><p>Similarly, for nullary alternatives (alternatives/constructors with no arguments) we have to use empty unboxed tuples. So a bool-like type looks like <code>(# (# #) | (# #) #)</code>.</p></li>
<li><p>Data constructors use the same syntax, except we have to put spaces between bars. For example, if you have a type with 10 alternatives, you do something like <code>(# | | | | value | | | | | #)</code>. Space between bars is optional in the type syntax, but not optional in the term syntax. The reason is because otherwise we’d have to steal some existing syntax. For example, <code>(# ||| a #)</code> can be parsed as singleton unboxed tuple of <code>Control.Arrow.|||</code> applied to an argument, or an unboxed sum with 4 alternatives.</p></li>
</ul>
<p>Note that the original Wiki page for unboxed sums included a “design questions” section that discussed some alterantive syntax (see <a href="https://ghc.haskell.org/trac/ghc/wiki/UnpackedSumTypes?version=32">this version</a>). Nobody made any progress to flesh out the ideas, and I updated the Wiki page to reflect the implementation. So it was known that the syntax is not good, but it just wasn’t a major concern.</p>
<p>Answer to the second question is also an answer to this question.</p>
<h2 id="how-is-this-supposed-to-be-used-by-users">How is this supposed to be used by users?</h2>
<p>We’re not expecting users to use this type extensively. It’ll mostly be used by the compiler, for optimizations. In fact, we could have skipped the front-end syntax entirely, and it’d be OK for the most part. If you haven’t used unboxed tuples before, you probably won’t be using unboxed sums.</p>
<p>The only place you may want to use this syntax is when you’re writing a high-performance library or program, and you have a sum type that’s used strictly and can take advantage of removing a level of indirection.</p>
<h2 id="how-is-this-used-by-the-compiler">How is this used by the compiler?</h2>
<p>A detailed answer would take too long, but here’s a summary:</p>
<ul>
<li><p><a href="research.microsoft.com/en-us/um/people/simonpj/Papers/cpr/cpr.ps.gz">Constructed product analysis</a> can now be used for returning sums efficiently. Note that this feature was left as “future work” in the paper (which is from 2004. See section 3.2). The high-level idea is that if a function returns a value that <em>it constructs</em>, then instead of boxing the components of the value and returning a boxed object, it can just return the components instead. In the case where the function result is directly scrutinized (i.e. case expressions), this usually reduces allocations. In other cases, it moves the allocation from the callee to the call site, which in turn leads to stack allocation is some cases (when the object doesn’t escape from the scope).</p>
<p>For product types, unboxed tuples are used for returning the value without heap allocation. For sum types, we use unboxed sums.</p></li>
<li><p>Result of strictness (or “demand”) analysis can now be used to pass sums efficiently. As a result worker/wrapper transformations can now be done for functions that take sum arguments. See <a href="https://ghc.haskell.org/trac/ghc/wiki/Commentary/Compiler/Demand">this Wiki page for demand analysis</a> and <a href="http://research.microsoft.com/en-us/um/people/simonpj/papers/usage-types/cardinality-popl14.pdf">this 2014 paper</a>.</p></li>
<li><p><code>{-# UNPACK #-}</code> pragmas now work on sum types, using unboxed sums under the hood.</p></li>
</ul>
<p>Note that none of these need a concrete syntax for unboxed sums.</p>
<hr />
<p>Hopefully this clarifies some questions and concerns, especially about the syntax. We have plenty of time until the first RC for 8.2 (<a href="https://ghc.haskell.org/trac/ghc/wiki/Status/GHC-8.2.1">mid-February 2017</a>), so it’s certainly possible to improve the syntax, and I’ll be working on that part once I’m done with the optimizations.</p>]]></summary>
</entry>
<entry>
    <title>On matching bang patterns</title>
    <link href="http://osa1.net/posts/2016-06-27-matching-bang-pattern.html" />
    <id>http://osa1.net/posts/2016-06-27-matching-bang-pattern.html</id>
    <published>2016-06-27T00:00:00Z</published>
    <updated>2016-06-27T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>I thought a bang pattern is all about <code>seq</code>. That may actually be true, but when that <code>seq</code> is happening may not be obvious. Even after ~5 years of Haskell I was initially very confused by this, and in fact at first I thought it was a bug:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">data</span> <span class="dt">T</span> <span class="fu">=</span> <span class="dt">A</span> <span class="fu">|</span> <span class="dt">B</span> <span class="fu">|</span> <span class="dt">C</span>

<span class="ot">fn5 ::</span> <span class="dt">Int</span> <span class="ot">-&gt;</span> [<span class="dt">T</span>] <span class="ot">-&gt;</span> <span class="dt">Int</span>
fn5  i []       <span class="fu">=</span> i
fn5  i (<span class="dt">A</span> <span class="fu">:</span> ts) <span class="fu">=</span> fn5 (i <span class="fu">+</span> <span class="dv">1</span>) ts
fn5 <span class="fu">!</span>i (<span class="dt">B</span> <span class="fu">:</span> ts) <span class="fu">=</span> fn5 (i <span class="fu">+</span> <span class="dv">2</span>) ts
fn5  i (<span class="dt">C</span> <span class="fu">:</span> ts) <span class="fu">=</span> fn5 <span class="dv">0</span> ts</code></pre>
<p>The question is, given these definitions, what does this evaluate to, and why:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell">fn5 undefined [<span class="dt">C</span>]</code></pre>
<p>This question is basically a <code>BangPatterns</code> question. The key point is that a bang pattern <em>first evaluates the value</em> to match, then looks at the pattern. This is from GHC 8.0.1 user manual, <a href="https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#bang-patterns-informal">section 9.28.1</a>:</p>
<blockquote>
<p>Matching an expression e against a pattern !p is done by first evaluating e (to WHNF) and then matching the result against p.</p>
</blockquote>
<p>My initial thought was that this example would not crash because the pattern <code>i</code> always matches, and since second argument is only matched by last case of this definition, which doesn’t evaluate <code>i</code>, <code>i</code> would not get evaluated.</p>
<p>Or in other words, I thought all this bang pattern does is to add a <code>seq</code>, <em>to the RHS</em>:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell">fn5 i (<span class="dt">B</span> <span class="fu">:</span> ts) <span class="fu">=</span> i <span class="ot">`seq`</span> fn5 (i <span class="fu">+</span> <span class="dv">2</span>) ts</code></pre>
<p>which is not what it really does!</p>
<hr />
<p>Before bang patterns, I think this pattern was considered as the standard way of forcing a function argument (mostly used for accumulator arguments):</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell">f acc _ <span class="fu">|</span> acc <span class="ot">`seq`</span> <span class="dt">False</span> <span class="fu">=</span> undefined
f acc arg <span class="fu">=</span> <span class="fu">...</span></code></pre>
<p>The guard in first equation always fails, but it forces the <code>acc</code> by the time it fails. While this looks bad<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>, it compiles to nice code after a case-of-case transformation, and it evaluates <code>acc</code> as first thing to do whenever it’s applied to two arguments.</p>
<p>Now, with <code>BangPatterns</code>, we get to do this:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell">f <span class="fu">!</span>acc arg <span class="fu">=</span> <span class="fu">...</span></code></pre>
<p>Which is good when we have one equation only, but when we have multiple equations like in <code>fn5</code> above, we need add bang patterns to every equation, or we risk having bugs<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a>.</p>
<p>So in short, we don’t have a good solution for saying a function is strict on some arguments, without risking bugs (by trying to add minimum number of bangs) or adding a lot of them.</p>
<hr />
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>I don’t like seeing <code>undefined</code>s like this, because I reserve them for code that’s not yet implemented but needs to be implemented. Using <code>undefined</code> as a proxy is also not OK these days, as we have <a href="https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#visible-type-application">visible type applications in GHC 8.0.1</a> and <a href="http://hackage.haskell.org/package/base-4.9.0.0/docs/Data-Proxy.html"><code>Proxy</code> in base</a> since <code>base-4.7</code>.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>I don’t mean semantic bugs, rather, unexpected memory usage etc. caused by not forcing thunks on time.<a href="#fnref2">↩</a></p></li>
</ol>
</div>]]></summary>
</entry>
<entry>
    <title>On -XStrict</title>
    <link href="http://osa1.net/posts/2015-11-16-XStrict-faq.html" />
    <id>http://osa1.net/posts/2015-11-16-XStrict-faq.html</id>
    <published>2015-11-16T00:00:00Z</published>
    <updated>2015-11-16T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p><code>-XStrict</code> has <a href="https://phabricator.haskell.org/D1142">landed in HEAD</a> a couple of days ago, and judged from the <a href="https://www.reddit.com/r/haskell/comments/3sts2t/strict_haskell_xstrict_has_landed/">upvotes</a> it seems like <a href="https://www.reddit.com/r/haskell">/r/haskell</a> was quite excited about it.</p>
<p>In the thread I tried to answer questions about <code>-XStrict</code>s effects on programs’ semantics. Does it make the language effectively call-by-value? Do I still have bottoms in my values? Do I lose infinite lists(streams)? In this post I’ll try to give a more organized answer, with some answers to the questions asked in the Reddit thread.</p>
<hr />
<p>Let’s think about how to create a thunk in Haskell:</p>
<ul>
<li><p>Create a let-binding. The RHS of let-binding is a thunk until actually use it.</p></li>
<li><p>Create a where-binding. This is just a syntactic sugar for a let-binding, so I won’t consider this as a different case.</p></li>
<li><p>Pass an argument to a function or a data constructor. The argument will only be evaluated when it’s actually “used”.</p></li>
</ul>
<p>Here I deliberately don’t define what I mean by “used”, because it’ll complicate the discussion a lot.</p>
<p>Now, with <code>-XStrict</code>, we have a bang pattern in every binder. This means that:</p>
<ol style="list-style-type: decimal">
<li><p>Let-bindings are now strict. E.g. if we have something like:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">let</span> a <span class="fu">=</span> <span class="fu">...</span> <span class="kw">in</span> <span class="fu">&lt;</span>body<span class="fu">&gt;</span></code></pre>
<p><code>a</code> is now evaluated before <code>&lt;body&gt;</code>, and so we can be sure that it won’t be bottom in <code>&lt;body&gt;</code>. Note however that this isn’t to say that <code>a</code> can’t <em>contain</em> bottoms. Here I’m just saying that <code>a</code> can’t be bottom in <code>&lt;body&gt;</code>.</p></li>
<li><p>Function arguments are now strict. In a function like this:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell">f a b <span class="fu">=</span> <span class="fu">&lt;</span>body<span class="fu">&gt;</span></code></pre>
<p><code>a</code> and <code>b</code> can’t be bottom in <code>&lt;body&gt;</code>.</p></li>
<li><p>Data constructor arguments(fields) are now strict. If we have this:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">data</span> <span class="dt">List</span> a <span class="fu">=</span> <span class="dt">Nil</span> <span class="fu">|</span> <span class="dt">Cons</span> a (<span class="dt">List</span> a)</code></pre>
<p>The two fields of <code>Cons</code> are now strict. Now, this case may look a bit tricky at first. What we really say here is that once a <code>List</code> is constructed, it can’t contain bottoms. We can still do something like <code>undefined :: List Int</code>, but the program immediately fails, instead of running until you try to pattern match on that <code>undefined</code> value like in the lazy case. This follows from the first two rules. Keep reading for more details.</p></li>
</ol>
<p>When all these combined, it means that our programs are evaluated just like how they would be in a call-by-value language. For example, if we have:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">let</span> a <span class="fu">=</span> <span class="fu">...</span>  <span class="kw">in</span> <span class="fu">&lt;</span>body<span class="fu">&gt;</span></code></pre>
<p>We can be sure that <code>a</code> won’t be bottom in <code>&lt;body&gt;</code>, and it can’t contain bottoms too! This follows from all the rules above. The first rule only says that <code>a</code> can’t be bottom itself. It doesn’t say anything about the fields(subexpressions) of <code>a</code>. Third rule says that it can’t contain bottom fields.</p>
<p>When we make all the binders and fields strict, including all the modules in all the dependencies(<code>base</code> etc.), we guarantee that we our programs evaluate like in a call-by-value language.</p>
<p>Now, call-by-value, call-by-name(and it’s efficient implementation call-by-need) etc. are really about how to evaluate a function application. In our case since we have a strictness annotation in all the function arguments, arguments will be evaluated before being passed to the function. Which really means evaluating the function application in call-by-value semantics.</p>
<p>Below are some questions and answers that I answered in the Reddit thread and in some follow-up threads.</p>
<hr />
<h2 id="what-about-standard-list-tuple-etc.-types">What about standard list, tuple etc. types?</h2>
<p>Unless we compile modules that define those using <code>-XStrict</code>, they’ll stay non-strict. For the standard types, we need <code>base</code> compiled with <code>-XStrict</code>. In practice this will probably never happen. But I think we can have another base, say, <code>base-strict</code>, which is the same <code>base</code>, except compiled with <code>-XStrict</code>. In this case depending on which one we’re using our lists, tuples etc. become strict or lazy.</p>
<h2 id="what-about-monadic-code">What about monadic code?</h2>
<p>Monadic code is really not special in any sense. When talking about the semantics we should see through the syntactic sugar. Say we have this:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">do</span> a <span class="ot">&lt;-</span> m1
   b <span class="ot">&lt;-</span> m2 a
   return (a <span class="fu">+</span> b)</code></pre>
<p>This is really just a syntactic sugar for:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell">m1 <span class="fu">&gt;&gt;=</span> (\a <span class="ot">-&gt;</span> m2 a <span class="fu">&gt;&gt;=</span> (\b <span class="ot">-&gt;</span> return (a <span class="fu">+</span> b)))</code></pre>
<p>With <code>-XStrict</code>, function arguments <code>a</code> and <code>b</code> and all the arguments of <code>&gt;&gt;=</code> and <code>return</code> will be strict. This makes the whole code strict. In this code this means that <code>m1</code> will be evaluated before <code>m2 a</code> is evaluated, and <code>return</code>s return value will be strict etc.</p>
<h2 id="what-about-list-comprehensions-and-infinite-listsstreams">What about list comprehensions and infinite lists(streams)?</h2>
<p>A list comprehension like <code>[1..]</code> is again a syntactic sugar. It’s expanded form is <code>enumFrom 1</code>. <code>enumFrom</code>’s type is <code>Enum a =&gt; a -&gt; [a]</code>. Let’s say we’re using <code>Enum Int</code> here. Since the instance is defined in <code>base</code>, and lists are also defined in <code>base</code>, this code will still work. However, if we compile <code>base</code> with <code>-XStrict</code>, this code loops because the standard list type will become strict.</p>
<p>In practice we would probably define strict and lazy lists separately to have the laziness when we need.</p>
<h2 id="what-about-higher-order-functions">What about higher-order functions?</h2>
<p>Since we don’t distinguish strict and lazy functions in type level, when we have a higher-order functions it may seem like we’d loose the guarantees. But this is not the case, at least not in general. Suppose we have this:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell">map<span class="ot"> ::</span> (a <span class="ot">-&gt;</span> b) <span class="ot">-&gt;</span> [a] <span class="ot">-&gt;</span> [b]
map _ []       <span class="fu">=</span> []
map f (x <span class="fu">:</span> xs) <span class="fu">=</span> f x <span class="fu">:</span> map f xs</code></pre>
<p>This is compiled with <code>-XStrict</code>. Now suppose we pass a function <code>f</code> which is lazy in it’s argument. Since <code>(:)</code> is strict in this code, we’ll still evaluate the <code>f x</code> before returning. Our guarantee that the list won’t be bottom and won’t have bottom still holds.</p>
<p>See also the paper <a href="http://research.microsoft.com/en-us/um/people/simonpj/papers/strict-core/tacc-hs09.pdf">“Types are calling conventions”</a>.</p>
<h2 id="haskells-denotational-semantics-says-that-lifted-types-have-bottoms">Haskell’s denotational semantics says that lifted types have bottoms</h2>
<p>This is true. Even if we have this strict type:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">data</span> <span class="dt">List</span> a <span class="fu">=</span> <span class="dt">Nil</span> <span class="fu">|</span> <span class="dt">Cons</span> <span class="fu">!</span>a <span class="fu">!</span>(<span class="dt">List</span> a)</code></pre>
<p>We can construct bottom values of this type, using, for example, <code>undefined :: List Int</code> or <code>(let x = x in x) :: Blah</code>.</p>
<p>However, if you think about how this value will be evaluated you’ll realize that this is exactly like how it would be evaluated in a call-by-value language. For example, if we try to bind it to a value:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">let</span> a <span class="fu">=</span><span class="ot"> undefined ::</span> <span class="dt">List</span> <span class="dt">Int</span> <span class="kw">in</span> <span class="fu">&lt;</span>body<span class="fu">&gt;</span></code></pre>
<p>This code will fail before <code>&lt;body&gt;</code> is run. In <code>&lt;body&gt;</code> <code>a</code> can’t be bottom and can’t contain bottoms.</p>
<p>(See also <a href="https://mail.haskell.org/pipermail/ghc-devs/2015-September/009799.html">this ghc-devs thread</a> about adding user defined unlifted data types to GHC. With this we could eliminate all the bottoms in some user-defined types.)</p>
<h2 id="but-haskell-will-still-generate-thunks-in-the-rts-level">But Haskell will still generate thunks in the RTS level?</h2>
<p>This is exactly right. <code>-XStrict</code> is really a very simple compiler pass that adds strictness annotations to every binder and field. We don’t have any changes in the GHC RTS to take advantage of additional strictness.</p>
<p>In other words, operational semantics of the language and implementation of this operational semantics in GHC RTS is still the same. We just do a program transformation to generate a program that evaluates like it would in a call-by-value language.</p>
<p>This means that if we have this program:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">let</span> a <span class="fu">=</span><span class="ot"> undefined ::</span> <span class="dt">List</span> <span class="dt">Int</span> <span class="kw">in</span> <span class="fu">&lt;</span>body<span class="fu">&gt;</span></code></pre>
<p>A thunk is still constructed in the runtime, but it’s evaluated <em>before</em> the <code>&lt;body&gt;</code> is evaluated. So this code fails before <code>&lt;body&gt;</code> is evaluated, and if we evaluate <code>&lt;body&gt;</code> it means that <code>a</code> is not bottom and doesn’t have bottoms.</p>
<p>This is another potential improvement over the <code>-XStrict</code>. For more details and some optimizations see <a href="https://mail.haskell.org/pipermail/ghc-devs/2015-October/010175.html">this ghc-devs thread</a>.</p>]]></summary>
</entry>
<entry>
    <title>On data representation in GHC Haskell</title>
    <link href="http://osa1.net/posts/2015-11-13-data-repr-1.html" />
    <id>http://osa1.net/posts/2015-11-13-data-repr-1.html</id>
    <published>2015-11-13T00:00:00Z</published>
    <updated>2015-11-13T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>It’s been a while since last time I wrote a blog post. This is not because I don’t have anything to write, rather, I have too much to write, and I was afraid that if I start writing it’d take too long.</p>
<p>But now that I started writing stuff for different applications(fellowships, internships etc.) I thought maybe this is a good time to write some blog posts too.</p>
<hr />
<p>At ICFP this year<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> we initiated a discussion about data representation in GHC Haskell. <a href="http://www.cs.indiana.edu/~rrnewton/homepage.html">My advisor</a> gave <a href="https://youtu.be/TT4poCUSf3A?t=59s">lightning talk at HIW</a>(Haskell Implementors Workshop). It’s a 5-minute talk and I recommend everyone reading this blog post to watch it. In the presentation, he showed this plot: (click on it to maximize)</p>
<p><a href="/images/data_repr_1/plot_old.png"><img src="/images/data_repr_1/plot_old_small.png" /></a></p>
<p>I put that here as a reference, but I’ll actually use some more detailed plots and correct a mistake in that plot. You can generate all the plots I show here using my benchmark programs <a href="https://github.com/osa1/spec_bench">here</a>.</p>
<p>In this post I want to make a point that is similar to Ryan’s point in the lightning talk: In Haskell, we’re not doing good job in data layouts. Our data is lazy by default, and laziness implies indirections(pointers). Updating a lazy record field means first generating a thunk and pointing to that thunk from the record. When the thunk is evaluated, we get one more indirection: A new pointer pointing to the actual data<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a>. This means that we need two pointer dereferencing just to read a single <code>Int</code> from a record.</p>
<p>At this point garbage collector helps to eliminate one level of indirection, and updates the field to point to the data directly. But this waits until the next garbage collection.</p>
<p>GHC has some support for “unpacking” fields of ADTs and records. When a field is “unpacked”, it means that 1) the field is strict 2) the value is not allocated separately and pointed to by a pointer, it’s part of the data constructor/record<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a>.</p>
<p>To illustrate how important unpacking is, I implemented two benchmarks. This is the data types I use in the benchmarks:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">data</span> <span class="dt">GL</span> a <span class="fu">=</span> <span class="dt">GLNil</span> <span class="fu">|</span> <span class="dt">GLCons</span> a (<span class="dt">GL</span> a)

<span class="kw">data</span> <span class="dt">SGL</span> a <span class="fu">=</span> <span class="dt">SGLNil</span> <span class="fu">|</span> <span class="dt">SGLCons</span> <span class="fu">!</span>a (<span class="dt">SGL</span> a)

<span class="kw">data</span> <span class="dt">IntList</span> <span class="fu">=</span> <span class="dt">ILNil</span> <span class="fu">|</span> <span class="dt">ILCons</span> <span class="ot">{-# UNPACK #-}</span> <span class="fu">!</span><span class="dt">Int</span> <span class="dt">IntList</span></code></pre>
<p>The first type, <code>GL</code>, is the exactly the same as GHC’s standard list type. Second one is mostly the same, only difference is I have a strictness annotation(a <a href="https://downloads.haskell.org/~ghc/7.10.2/docs/html/users_guide/bang-patterns.html"><code>BangPattern</code></a>) before in the head(or <code>car</code>) part of the list. Third one is similar to second one, except I also unpack the <code>car</code>.</p>
<p>Note that if you look at the types, first two of these types are parametric on the list elements, while the third one is specialized to integers. This is essentially monomorphization, and some compilers can do that automatically in some cases(<a href="http://mlton.org/Monomorphise">1</a>, <a href="http://www.impredicative.com/ur/">2</a>, if you know other compilers that do this, please write a comment), but in the presence of higher-order functions(and probably some other features), it’s in general not possible to monomorphise everything<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a>. GHC has some limited support for this with the <a href="https://downloads.haskell.org/~ghc/7.10.2/docs/html/users_guide/pragmas.html#specialize-pragma"><code>SPECIALIZE</code> pragma</a>, but it only works on functions and it doesn’t specialize data types(and maybe it can’t do this even in theory).</p>
<p>Now, I’m going to implement two functions on these data types. First function is for summing up all the elements:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">glSum ::</span> <span class="dt">GL</span> <span class="dt">Int</span> <span class="ot">-&gt;</span> <span class="dt">Int</span>
glSum <span class="fu">=</span> glSumAcc <span class="dv">0</span>

<span class="ot">glSumAcc ::</span> <span class="dt">Int</span> <span class="ot">-&gt;</span> <span class="dt">GL</span> <span class="dt">Int</span> <span class="ot">-&gt;</span> <span class="dt">Int</span>
glSumAcc <span class="fu">!</span>acc <span class="dt">GLNil</span>        <span class="fu">=</span> acc
glSumAcc <span class="fu">!</span>acc (<span class="dt">GLCons</span> h t) <span class="fu">=</span> glSumAcc (acc <span class="fu">+</span> h) t</code></pre>
<p>I implement exactly the same function for other types too.</p>
<p>Second function is the length function:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">glLength ::</span> <span class="dt">GL</span> a <span class="ot">-&gt;</span> <span class="dt">Int</span>
glLength <span class="fu">=</span> glLengthAcc <span class="dv">0</span>

<span class="ot">glLengthAcc ::</span> <span class="dt">Int</span> <span class="ot">-&gt;</span> <span class="dt">GL</span> a <span class="ot">-&gt;</span> <span class="dt">Int</span>
glLengthAcc <span class="fu">!</span>acc <span class="dt">GLNil</span>        <span class="fu">=</span> acc
glLengthAcc <span class="fu">!</span>acc (<span class="dt">GLCons</span> _ l) <span class="fu">=</span> glLengthAcc (acc <span class="fu">+</span> <span class="dv">1</span>) l</code></pre>
<p>(Similarly for other two types..)</p>
<p>Now I’m going to benchmark these two function for all three variants, but let’s just speculate about what would be the expected results.</p>
<p>Clearly the third one should be faster for <code>sum</code>, because we don’t need to follow pointers for reading the integer. But should the second type(parametric but strict) be any faster? I’d say yes. The reason is because the field is strict, and so when we do pattern matching on <code>Int</code> to add integers, we don’t need to enter any thunks, we know that <code>Int</code> is already in WHNF. We should be able to just read the field.</p>
<p>Here’s the result: (click on it to maximize)</p>
<p><a href="/images/data_repr_1/plot_sum.png"><img src="/images/data_repr_1/plot_sum_small.png" /></a></p>
<p>As you can see I have an extra line in this plot: I added GHC’s standard lists and used <a href="http://hackage.haskell.org/package/base-4.8.1.0/docs/Data-List.html#v:sum">standard <code>sum</code> function</a>. It’s ridiculously slow, it’s almost two orders of magnitude slower on a list with length 10^7. Before moving on and interpreting rest of the lines let’s just talk a bit about why this is slower. We only need to look at the type of <code>sum</code> function:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell">sum<span class="ot"> ::</span> (<span class="dt">Foldable</span> t, <span class="dt">Num</span> a) <span class="ot">=&gt;</span> t a <span class="ot">-&gt;</span> a</code></pre>
<p>This function is very, very general. There’s no way for this function to run fast, unless GHC is smart enough to generate a specialized version of this for <code>[Int]</code>. It turns out GHC is not smart enough, and in our case this is causing trouble. Note that we need two specializations here. First is to eliminate <code>Foldable t</code> part(we statically know that <code>t</code> is <code>[]</code>), and second is to eliminate <code>Num a</code> part(we statically know that <code>a</code> is <code>Int</code>).</p>
<p>Now that we have this out of the way, let’s look at the other 3 lines. We can see that unboxed version is really faster(0.039 seconds other lists vs. 0.018 seconds unboxed list), which was expected. The interesting part is strict version is exactly the same as lazy version. Now, I don’t have a good explanation for this. The generated Core and STG for these two variants of <code>sum</code> are exactly the same. The Cmm or assembly code should be different. The problem is, I really can’t make any sense of generated Cmm code. I should study Cmm more for this.</p>
<p>But I have an idea: Pattern matching means entering the thunk to reveal the WHNF structure. Since our integers are boxed, we need to pattern match on them to read the primitive <code>Int#</code>s. This means entering the thunks, even if the field is strict.</p>
<p>In my benchmarks, I only used completely normalized lists. This means that in the lazy case we enter thunks, only to return immediately, because the <code>Int</code> is already in WHNF. There’s no difference in lazy and strict variants in pattern matching.</p>
<p>Only difference is when we update the field, in which case generated code should be different if the field is strict.</p>
<p>To validate the second part of this claim, I wrote another benchmark. In this benchmark I again create a list of length 10^7, but every cell has an integer calculated using a function. I then measure list generation and consumption(sum) times. The idea is that in the case of strict list, list generation should be slower, but consumption should be faster, and the opposite in the lazy list case. Indeed we can observe this in the output:</p>
<pre><code>Performing major GC
Generating generic list
Took 0.737606563 seconds.
Performing major GC
Summing generic list
Took 0.490998969 seconds.
Performing major GC
Generating strict list
Took 0.870686580 seconds.
Performing major GC
Summing strict list
Took 0.035886157 seconds.</code></pre>
<p>The program is <a href="https://github.com/osa1/spec_bench/commit/b63322eb1edd32792837b58853c00ba0effad0a6">here</a>. We can see that summing strict list is 10x faster, but producing is slower, because instead of generating thunks we’re actually doing the work while producing cons cells.</p>
<p>(One thing to note here is that if the computation in thunks is not expensive enough, strict lists are faster in both production and consumption. I think the reason is because thunking overhead is bigger than actually doing the work in some cases)</p>
<p>OK, I hope this explains the story with <code>sum</code>. The second part of the benchmark is even more interesting. It runs <code>length</code>. Here’s the plot:</p>
<p><a href="/images/data_repr_1/plot_len.png"><img src="/images/data_repr_1/plot_len_small.png" /></a></p>
<p>We again have a line for standard Haskell list here. Good news is that even though standard <a href="http://hackage.haskell.org/package/base-4.8.1.0/docs/Data-List.html#v:length">length</a>’s type is again very general, this time GHC was able to optimize:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell">length<span class="ot"> ::</span> <span class="dt">Foldable</span> t <span class="ot">=&gt;</span> t a <span class="ot">-&gt;</span> <span class="dt">Int</span></code></pre>
<p>Interesting part is that unboxed list is again faster! But why? We’re not using head fields, whether it’s a pointer or not should not matter, right?</p>
<p>Here I again have an idea: Even though we never use head parts, the garbage collector has to traverse all the data, and copy them from one generation to other.</p>
<p>(We also allocate more, but I’m not measuring that in the benchmarks)</p>
<p>To again back my claims, I have another set of benchmark programs that generate some GC stats. Here’s the output for “generic” case:</p>
<pre><code>   800,051,568 bytes allocated in the heap
 1,138,797,344 bytes copied during GC
   310,234,360 bytes maximum residency (12 sample(s))
    68,472,584 bytes maximum slop
           705 MB total memory in use (0 MB lost due to fragmentation)

                                   Tot time (elapsed)  Avg pause  Max pause
Gen  0      1520 colls,     0 par    0.207s   0.215s     0.0001s    0.0009s
Gen  1        12 colls,     0 par    0.348s   0.528s     0.0440s    0.1931s

INIT    time    0.000s  (  0.000s elapsed)
MUT     time    0.126s  (  0.082s elapsed)
GC      time    0.555s  (  0.743s elapsed)
EXIT    time    0.004s  (  0.050s elapsed)
Total   time    0.685s  (  0.875s elapsed)

%GC     time      81.0%  (84.9% elapsed)

Alloc rate    6,349,615,619 bytes per MUT second

Productivity  19.0% of total user, 14.9% of total elapsed</code></pre>
<p>“Generic strict” case:</p>
<pre><code>   800,051,568 bytes allocated in the heap
 1,138,797,344 bytes copied during GC
   310,234,360 bytes maximum residency (12 sample(s))
    68,472,584 bytes maximum slop
           705 MB total memory in use (0 MB lost due to fragmentation)

                                   Tot time (elapsed)  Avg pause  Max pause
Gen  0      1520 colls,     0 par    0.211s   0.213s     0.0001s    0.0008s
Gen  1        12 colls,     0 par    0.367s   0.531s     0.0442s    0.1990s

INIT    time    0.000s  (  0.000s elapsed)
MUT     time    0.114s  (  0.080s elapsed)
GC      time    0.578s  (  0.744s elapsed)
EXIT    time    0.003s  (  0.045s elapsed)
Total   time    0.698s  (  0.869s elapsed)

%GC     time      82.8%  (85.6% elapsed)

Alloc rate    7,017,996,210 bytes per MUT second

Productivity  17.2% of total user, 13.8% of total elapsed</code></pre>
<p>“Unboxed” case:</p>
<pre><code>   560,051,552 bytes allocated in the heap
   486,232,928 bytes copied during GC
   123,589,752 bytes maximum residency (9 sample(s))
     3,815,304 bytes maximum slop
           244 MB total memory in use (0 MB lost due to fragmentation)

                                   Tot time (elapsed)  Avg pause  Max pause
Gen  0      1062 colls,     0 par    0.117s   0.123s     0.0001s    0.0018s
Gen  1         9 colls,     0 par    0.116s   0.179s     0.0199s    0.0771s

INIT    time    0.000s  (  0.001s elapsed)
MUT     time    0.070s  (  0.054s elapsed)
GC      time    0.233s  (  0.302s elapsed)
EXIT    time    0.002s  (  0.019s elapsed)
Total   time    0.306s  (  0.376s elapsed)

%GC     time      76.1%  (80.4% elapsed)

Alloc rate    8,000,736,457 bytes per MUT second

Productivity  23.9% of total user, 19.4% of total elapsed</code></pre>
<p>Interesting parts are productivity rates and total bytes allocated. We can see that unboxed version is a lot better in both.</p>
<p>The reason why productivities are too bad in all cases is, I think, that because this is purely an allocation benchmark, all we do is to allocate and then we do one pass on the whole thing. In this type of programs it makes sense to increase the initial heap a little bit to increase the productivity. For example, if I use <code>-H1G</code> productivity increases to 64% in generic list case and to 97% in unboxed list case.</p>
<p>So what’s the lesson here?</p>
<p>The data layout matters a lot. Even if the data is not used in some hot code path, GC needs to traverse all the live data and in the case of GHC Haskell it needs to copy them in each GC cycle. Also, lazy-by-default is bad for performance.</p>
<h1 id="an-improvement">An improvement</h1>
<p>I recently finished implementing hard parts of a project. I don’t want to give too much detail here for now but let’s just say we’re improving the unboxing story in GHC. With our new patch, you will be able to <code>{-# UNPACK #-}</code> some of the types that you can’t right now. When/if the patch lands I’m going to announce it here.</p>
<p>With <a href="http://downloads.haskell.org/~ghc/master/users-guide/glasgow_exts.html?highlight=strictdata#strict-haskell"><code>-XStrictData</code></a>, <a href="https://phabricator.haskell.org/D1142"><code>-XStrict</code></a> and flags like <code>-funbox-strict-fields</code>(search for it <a href="https://downloads.haskell.org/~ghc/7.10.2/docs/html/users_guide/flag-reference.html">here</a>) we’ll have a lot better story about data layout than we have today. But there are still some missing pieces. Our patch will hopefully implement one more missing piece and we’ll have even better data layout story.</p>
<p>I think the next step then will be <a href="https://mail.haskell.org/pipermail/ghc-devs/2015-October/010175.html">better calling conventions for strict functions</a> and some other tweaks in the runtime system for better strict code support overall. Then maybe we can officially declare Haskell as a language with lazy and strict evaluation ;-) .</p>
<hr />
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>I realized that I forgot to announce it here, but <a href="/papers/cnf.pdf">we had a paper at ICFP</a>, and I gave a <a href="https://www.youtube.com/watch?v=gkx-D-7Y1EU">talk at Haskell Implementors Workshop</a> this year.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>That data is still not totally normalized, it’s in weak head normal form. The new heap object that we get when we evaluate a thunk is called an “indirection”. See more details <a href="https://ghc.haskell.org/trac/ghc/wiki/Commentary/Rts/Storage/HeapObjects#Indirections">here</a>.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>See the <a href="https://downloads.haskell.org/~ghc/7.10.2/docs/html/users_guide/pragmas.html#unpack-pragma">GHC user manual entry</a> for more info.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>I’m hoping to make this a subject to another blog post. One thing to note here is that even when monomorphization is possible, it may cause a code explosion.<a href="#fnref4">↩</a></p></li>
</ol>
</div>]]></summary>
</entry>
<entry>
    <title>The issue with work sharing (common subexpression elimination)</title>
    <link href="http://osa1.net/posts/2015-08-13-the-issue-with-work-sharing.html" />
    <id>http://osa1.net/posts/2015-08-13-the-issue-with-work-sharing.html</id>
    <published>2015-08-13T00:00:00Z</published>
    <updated>2015-08-13T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>I’d expect more work sharing to be always more beneficial. But apparently this is not the case, as pointed out in (Chitil, 1997)<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>.</p>
<p>Here’s an example from the paper: (slightly changed)</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell">sum [<span class="dv">1</span> <span class="fu">..</span> <span class="dv">1000</span>] <span class="fu">+</span> sum [<span class="fu">-</span><span class="dv">1000</span> <span class="fu">..</span> <span class="fu">-</span><span class="dv">1</span>] <span class="fu">+</span> prod [<span class="dv">1</span> <span class="fu">..</span> <span class="dv">1000</span>]</code></pre>
<p>We can evaluate this expression to WHNF using heap space enough for a single list(to be more specific, we only need a single cons cell at any time). After evaluating a subexpression, we can deallocate and allocate for the next list etc.</p>
<p>However, if we eliminate common subexpressions, and generate this code:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">let</span> v <span class="fu">=</span> [<span class="dv">1</span> <span class="fu">..</span> <span class="dv">1000</span>]
 <span class="kw">in</span> sum v <span class="fu">+</span> sum [<span class="fu">-</span><span class="dv">1000</span> <span class="fu">..</span> <span class="fu">-</span><span class="dv">1</span>] <span class="fu">+</span> prod v</code></pre>
<p>Now <code>v</code> has to live until the let body is evaluated to a value. We win in allocation/deallocation side, but we lose in residency side. In paper’s words: “Whereas the transformation always decreases total heap usage, it may considerably influence heap residency.”</p>
<p>In general, we can’t do this transformation, without risking increased residency:</p>
<p>\[ e’[e,e] \leadsto \texttt{let}\; x = e\; \texttt{in}\; e’[x,x] \]</p>
<p>As a solution, the paper suggests this:</p>
<ol style="list-style-type: decimal">
<li>We always do CSE if the subexpressions’ WHNF == NF(i.e. if it’s a “safe type” in paper’s terms). According to the paper, “a partially evaluated expression is certain to require only a small, fixed amount of space if it’s not a function, whose environment may refer to arbitrary large data structures, and its WHNF is already its normal form”.</li>
<li>We always do CSE when a named expression is syntactically dominating another equal expression:</li>
</ol>
<p>\[ \texttt{let}\; x = e\; \texttt{in}\; e’[e] \leadsto \texttt{let}\; x = e\; \texttt{in}\; e’[x] \]</p>
<hr />
<p>Note that (1) is not always true, assume an expression with type <code>ForeignPtr a</code> where <code>a</code> is a huge FFI object. This has WHNF == NF property, but it may increase residency significantly. Maybe GHC didn’t have FFI at the time the paper is written.</p>
<p>Also, I’m wondering how is CSE is handled in current GHC.</p>
<hr />
<p>In supercompilation, we want to avoid evaluating same expressions in a loop forever, so we keep some kind of “history”, and when we come across a term that we evaluated before, we fold the process tree and avoid evaluating same term again.</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell">(fib <span class="dv">1000</span>, fib <span class="dv">1000</span>)</code></pre>
<p>Unless we make sure to split it in a way that branches of the process tree are unaware of each other, we may end up eliminating common subexpressions. However, since there are lots of cases where we may want CSE, a splitter that always prevents it is not always desirable. We should instead allow CSE in a controlled way.</p>
<hr />
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Olaf Chitil, “Common Subexpression Elimination in a Lazy Functional Language”, section 3.5.<a href="#fnref1">↩</a></p></li>
</ol>
</div>]]></summary>
</entry>

</feed>
