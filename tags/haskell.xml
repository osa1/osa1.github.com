<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>osa1.net - Posts tagged haskell</title>
    <link href="http://osa1.net/tags/haskell.xml" rel="self" />
    <link href="http://osa1.net" />
    <id>http://osa1.net/tags/haskell.xml</id>
    <author>
        <name>Ömer Sinan Ağacan</name>
        <email>omeragaca@gmail.com</email>
    </author>
    <updated>2016-07-22T00:00:00Z</updated>
    <entry>
    <title>Unboxed sums FAQ</title>
    <link href="http://osa1.net/posts/2016-07-22-unboxed-sums-faq.html" />
    <id>http://osa1.net/posts/2016-07-22-unboxed-sums-faq.html</id>
    <published>2016-07-22T00:00:00Z</published>
    <updated>2016-07-22T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>The unboxed sums patch that implements unlifted, unboxed sum types (as described in <a href="https://ghc.haskell.org/trac/ghc/wiki/UnpackedSumTypes">this Wiki page</a>) was merged yesterday, and a <a href="https://www.reddit.com/r/haskell/comments/4txuo7/unboxed_sum_types_with_unpack_support_will_be_in/">/r/haskell discussion</a> emerged shortly after. As the implementor, I tried to answer questions there, but to keep answers more organized I wanted to write a blog post about it.</p>
<p>The reason I’m not writing this to the Wiki page is because this is about current plans and status of the feature. The wiki page may be updated in the future as the feature evolves and/or may be edited by others. This page reflects the current status as of today, future plans, and my own ideas.</p>
<hr />
<h2 id="syntax-is-awful-why">Syntax is awful, why?</h2>
<p>This feature is designed to complement the similar feature for product types (tuples), called <a href="https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#unboxed-tuples">“unboxed tuples”</a>. The syntax is thus chosen to reflect this idea. Instead of commas in the unboxed tuple syntax, we used bars (similar to how bars used in sum type declarations). The syntax looks bad for several reasons:</p>
<ul>
<li><p>Type argument of an alternative have to be a single type. If we want multiple types in an alternative, we have to use an unboxed tuple. For example, unboxed sum version of the type <code>data T = T1 Int | T2 String Bool</code> is <code>(# Int | (# String, Bool #) #)</code>. That’s a lot of parens and hashes.</p></li>
<li><p>Similarly, for nullary alternatives (alternatives/constructors with no arguments) we have to use empty unboxed tuples. So a bool-like type looks like <code>(# (# #) | (# #) #)</code>.</p></li>
<li><p>Data constructors use the same syntax, except we have to put spaces between bars. For example, if you have a type with 10 alternatives, you do something like <code>(# | | | | value | | | | | #)</code>. Space between bars is optional in the type syntax, but not optional in the term syntax. The reason is because otherwise we’d have to steal some existing syntax. For example, <code>(# ||| a #)</code> can be parsed as singleton unboxed tuple of <code>Control.Arrow.|||</code> applied to an argument, or an unboxed sum with 4 alternatives.</p></li>
</ul>
<p>Note that the original Wiki page for unboxed sums included a “design questions” section that discussed some alterantive syntax (see <a href="https://ghc.haskell.org/trac/ghc/wiki/UnpackedSumTypes?version=32">this version</a>). Nobody made any progress to flesh out the ideas, and I updated the Wiki page to reflect the implementation. So it was known that the syntax is not good, but it just wasn’t a major concern.</p>
<p>Answer to the second question is also an answer to this question.</p>
<h2 id="how-is-this-supposed-to-be-used-by-users">How is this supposed to be used by users?</h2>
<p>We’re not expecting users to use this type extensively. It’ll mostly be used by the compiler, for optimizations. In fact, we could have skipped the front-end syntax entirely, and it’d be OK for the most part. If you haven’t used unboxed tuples before, you probably won’t be using unboxed sums.</p>
<p>The only place you may want to use this syntax is when you’re writing a high-performance library or program, and you have a sum type that’s used strictly and can take advantage of removing a level of indirection.</p>
<h2 id="how-is-this-used-by-the-compiler">How is this used by the compiler?</h2>
<p>A detailed answer would take too long, but here’s a summary:</p>
<ul>
<li><p><a href="research.microsoft.com/en-us/um/people/simonpj/Papers/cpr/cpr.ps.gz">Constructed product analysis</a> can now be used for returning sums efficiently. Note that this feature was left as “future work” in the paper (which is from 2004. See section 3.2). The high-level idea is that if a function returns a value that <em>it constructs</em>, then instead of boxing the components of the value and returning a boxed object, it can just return the components instead. In the case where the function result is directly scrutinized (i.e. case expressions), this usually reduces allocations. In other cases, it moves the allocation from the callee to the call site, which in turn leads to stack allocation is some cases (when the object doesn’t escape from the scope).</p>
<p>For product types, unboxed tuples are used for returning the value without heap allocation. For sum types, we use unboxed sums.</p></li>
<li><p>Result of strictness (or “demand”) analysis can now be used to pass sums efficiently. As a result worker/wrapper transformations can now be done for functions that take sum arguments. See <a href="https://ghc.haskell.org/trac/ghc/wiki/Commentary/Compiler/Demand">this Wiki page for demand analysis</a> and <a href="http://research.microsoft.com/en-us/um/people/simonpj/papers/usage-types/cardinality-popl14.pdf">this 2014 paper</a>.</p></li>
<li><p><code>{-# UNPACK #-}</code> pragmas now work on sum types, using unboxed sums under the hood.</p></li>
</ul>
<p>Note that none of these need a concrete syntax for unboxed sums.</p>
<hr />
<p>Hopefully this clarifies some questions and concerns, especially about the syntax. We have plenty of time until the first RC for 8.2 (<a href="https://ghc.haskell.org/trac/ghc/wiki/Status/GHC-8.2.1">mid-February 2017</a>), so it’s certainly possible to improve the syntax, and I’ll be working on that part once I’m done with the optimizations.</p>]]></summary>
</entry>
<entry>
    <title>On matching bang patterns</title>
    <link href="http://osa1.net/posts/2016-06-27-matching-bang-pattern.html" />
    <id>http://osa1.net/posts/2016-06-27-matching-bang-pattern.html</id>
    <published>2016-06-27T00:00:00Z</published>
    <updated>2016-06-27T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>I thought a bang pattern is all about <code>seq</code>. That may actually be true, but when that <code>seq</code> is happening may not be obvious. Even after ~5 years of Haskell I was initially very confused by this, and in fact at first I thought it was a bug:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">data</span> <span class="dt">T</span> <span class="fu">=</span> <span class="dt">A</span> <span class="fu">|</span> <span class="dt">B</span> <span class="fu">|</span> <span class="dt">C</span>

<span class="ot">fn5 ::</span> <span class="dt">Int</span> <span class="ot">-&gt;</span> [<span class="dt">T</span>] <span class="ot">-&gt;</span> <span class="dt">Int</span>
fn5  i []       <span class="fu">=</span> i
fn5  i (<span class="dt">A</span> <span class="fu">:</span> ts) <span class="fu">=</span> fn5 (i <span class="fu">+</span> <span class="dv">1</span>) ts
fn5 <span class="fu">!</span>i (<span class="dt">B</span> <span class="fu">:</span> ts) <span class="fu">=</span> fn5 (i <span class="fu">+</span> <span class="dv">2</span>) ts
fn5  i (<span class="dt">C</span> <span class="fu">:</span> ts) <span class="fu">=</span> fn5 <span class="dv">0</span> ts</code></pre>
<p>The question is, given these definitions, what does this evaluate to, and why:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell">fn5 undefined [<span class="dt">C</span>]</code></pre>
<p>This question is basically a <code>BangPatterns</code> question. The key point is that a bang pattern <em>first evaluates the value</em> to match, then looks at the pattern. This is from GHC 8.0.1 user manual, <a href="https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#bang-patterns-informal">section 9.28.1</a>:</p>
<blockquote>
<p>Matching an expression e against a pattern !p is done by first evaluating e (to WHNF) and then matching the result against p.</p>
</blockquote>
<p>My initial thought was that this example would not crash because the pattern <code>i</code> always matches, and since second argument is only matched by last case of this definition, which doesn’t evaluate <code>i</code>, <code>i</code> would not get evaluated.</p>
<p>Or in other words, I thought all this bang pattern does is to add a <code>seq</code>, <em>to the RHS</em>:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell">fn5 i (<span class="dt">B</span> <span class="fu">:</span> ts) <span class="fu">=</span> i <span class="ot">`seq`</span> fn5 (i <span class="fu">+</span> <span class="dv">2</span>) ts</code></pre>
<p>which is not what it really does!</p>
<hr />
<p>Before bang patterns, I think this pattern was considered as the standard way of forcing a function argument (mostly used for accumulator arguments):</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell">f acc _ <span class="fu">|</span> acc <span class="ot">`seq`</span> <span class="dt">False</span> <span class="fu">=</span> undefined
f acc arg <span class="fu">=</span> <span class="fu">...</span></code></pre>
<p>The guard in first equation always fails, but it forces the <code>acc</code> by the time it fails. While this looks bad<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>, it compiles to nice code after a case-of-case transformation, and it evaluates <code>acc</code> as first thing to do whenever it’s applied to two arguments.</p>
<p>Now, with <code>BangPatterns</code>, we get to do this:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell">f <span class="fu">!</span>acc arg <span class="fu">=</span> <span class="fu">...</span></code></pre>
<p>Which is good when we have one equation only, but when we have multiple equations like in <code>fn5</code> above, we need add bang patterns to every equation, or we risk having bugs<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a>.</p>
<p>So in short, we don’t have a good solution for saying a function is strict on some arguments, without risking bugs (by trying to add minimum number of bangs) or adding a lot of them.</p>
<hr />
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>I don’t like seeing <code>undefined</code>s like this, because I reserve them for code that’s not yet implemented but needs to be implemented. Using <code>undefined</code> as a proxy is also not OK these days, as we have <a href="https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#visible-type-application">visible type applications in GHC 8.0.1</a> and <a href="http://hackage.haskell.org/package/base-4.9.0.0/docs/Data-Proxy.html"><code>Proxy</code> in base</a> since <code>base-4.7</code>.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>I don’t mean semantic bugs, rather, unexpected memory usage etc. caused by not forcing thunks on time.<a href="#fnref2">↩</a></p></li>
</ol>
</div>]]></summary>
</entry>
<entry>
    <title>On -XStrict</title>
    <link href="http://osa1.net/posts/2015-11-16-XStrict-faq.html" />
    <id>http://osa1.net/posts/2015-11-16-XStrict-faq.html</id>
    <published>2015-11-16T00:00:00Z</published>
    <updated>2015-11-16T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p><code>-XStrict</code> has <a href="https://phabricator.haskell.org/D1142">landed in HEAD</a> a couple of days ago, and judged from the <a href="https://www.reddit.com/r/haskell/comments/3sts2t/strict_haskell_xstrict_has_landed/">upvotes</a> it seems like <a href="https://www.reddit.com/r/haskell">/r/haskell</a> was quite excited about it.</p>
<p>In the thread I tried to answer questions about <code>-XStrict</code>s effects on programs’ semantics. Does it make the language effectively call-by-value? Do I still have bottoms in my values? Do I lose infinite lists(streams)? In this post I’ll try to give a more organized answer, with some answers to the questions asked in the Reddit thread.</p>
<hr />
<p>Let’s think about how to create a thunk in Haskell:</p>
<ul>
<li><p>Create a let-binding. The RHS of let-binding is a thunk until actually use it.</p></li>
<li><p>Create a where-binding. This is just a syntactic sugar for a let-binding, so I won’t consider this as a different case.</p></li>
<li><p>Pass an argument to a function or a data constructor. The argument will only be evaluated when it’s actually “used”.</p></li>
</ul>
<p>Here I deliberately don’t define what I mean by “used”, because it’ll complicate the discussion a lot.</p>
<p>Now, with <code>-XStrict</code>, we have a bang pattern in every binder. This means that:</p>
<ol style="list-style-type: decimal">
<li><p>Let-bindings are now strict. E.g. if we have something like:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">let</span> a <span class="fu">=</span> <span class="fu">...</span> <span class="kw">in</span> <span class="fu">&lt;</span>body<span class="fu">&gt;</span></code></pre>
<p><code>a</code> is now evaluated before <code>&lt;body&gt;</code>, and so we can be sure that it won’t be bottom in <code>&lt;body&gt;</code>. Note however that this isn’t to say that <code>a</code> can’t <em>contain</em> bottoms. Here I’m just saying that <code>a</code> can’t be bottom in <code>&lt;body&gt;</code>.</p></li>
<li><p>Function arguments are now strict. In a function like this:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell">f a b <span class="fu">=</span> <span class="fu">&lt;</span>body<span class="fu">&gt;</span></code></pre>
<p><code>a</code> and <code>b</code> can’t be bottom in <code>&lt;body&gt;</code>.</p></li>
<li><p>Data constructor arguments(fields) are now strict. If we have this:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">data</span> <span class="dt">List</span> a <span class="fu">=</span> <span class="dt">Nil</span> <span class="fu">|</span> <span class="dt">Cons</span> a (<span class="dt">List</span> a)</code></pre>
<p>The two fields of <code>Cons</code> are now strict. Now, this case may look a bit tricky at first. What we really say here is that once a <code>List</code> is constructed, it can’t contain bottoms. We can still do something like <code>undefined :: List Int</code>, but the program immediately fails, instead of running until you try to pattern match on that <code>undefined</code> value like in the lazy case. This follows from the first two rules. Keep reading for more details.</p></li>
</ol>
<p>When all these combined, it means that our programs are evaluated just like how they would be in a call-by-value language. For example, if we have:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">let</span> a <span class="fu">=</span> <span class="fu">...</span>  <span class="kw">in</span> <span class="fu">&lt;</span>body<span class="fu">&gt;</span></code></pre>
<p>We can be sure that <code>a</code> won’t be bottom in <code>&lt;body&gt;</code>, and it can’t contain bottoms too! This follows from all the rules above. The first rule only says that <code>a</code> can’t be bottom itself. It doesn’t say anything about the fields(subexpressions) of <code>a</code>. Third rule says that it can’t contain bottom fields.</p>
<p>When we make all the binders and fields strict, including all the modules in all the dependencies(<code>base</code> etc.), we guarantee that we our programs evaluate like in a call-by-value language.</p>
<p>Now, call-by-value, call-by-name(and it’s efficient implementation call-by-need) etc. are really about how to evaluate a function application. In our case since we have a strictness annotation in all the function arguments, arguments will be evaluated before being passed to the function. Which really means evaluating the function application in call-by-value semantics.</p>
<p>Below are some questions and answers that I answered in the Reddit thread and in some follow-up threads.</p>
<hr />
<h2 id="what-about-standard-list-tuple-etc.-types">What about standard list, tuple etc. types?</h2>
<p>Unless we compile modules that define those using <code>-XStrict</code>, they’ll stay non-strict. For the standard types, we need <code>base</code> compiled with <code>-XStrict</code>. In practice this will probably never happen. But I think we can have another base, say, <code>base-strict</code>, which is the same <code>base</code>, except compiled with <code>-XStrict</code>. In this case depending on which one we’re using our lists, tuples etc. become strict or lazy.</p>
<h2 id="what-about-monadic-code">What about monadic code?</h2>
<p>Monadic code is really not special in any sense. When talking about the semantics we should see through the syntactic sugar. Say we have this:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">do</span> a <span class="ot">&lt;-</span> m1
   b <span class="ot">&lt;-</span> m2 a
   return (a <span class="fu">+</span> b)</code></pre>
<p>This is really just a syntactic sugar for:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell">m1 <span class="fu">&gt;&gt;=</span> (\a <span class="ot">-&gt;</span> m2 a <span class="fu">&gt;&gt;=</span> (\b <span class="ot">-&gt;</span> return (a <span class="fu">+</span> b)))</code></pre>
<p>With <code>-XStrict</code>, function arguments <code>a</code> and <code>b</code> and all the arguments of <code>&gt;&gt;=</code> and <code>return</code> will be strict. This makes the whole code strict. In this code this means that <code>m1</code> will be evaluated before <code>m2 a</code> is evaluated, and <code>return</code>s return value will be strict etc.</p>
<h2 id="what-about-list-comprehensions-and-infinite-listsstreams">What about list comprehensions and infinite lists(streams)?</h2>
<p>A list comprehension like <code>[1..]</code> is again a syntactic sugar. It’s expanded form is <code>enumFrom 1</code>. <code>enumFrom</code>’s type is <code>Enum a =&gt; a -&gt; [a]</code>. Let’s say we’re using <code>Enum Int</code> here. Since the instance is defined in <code>base</code>, and lists are also defined in <code>base</code>, this code will still work. However, if we compile <code>base</code> with <code>-XStrict</code>, this code loops because the standard list type will become strict.</p>
<p>In practice we would probably define strict and lazy lists separately to have the laziness when we need.</p>
<h2 id="what-about-higher-order-functions">What about higher-order functions?</h2>
<p>Since we don’t distinguish strict and lazy functions in type level, when we have a higher-order functions it may seem like we’d loose the guarantees. But this is not the case, at least not in general. Suppose we have this:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell">map<span class="ot"> ::</span> (a <span class="ot">-&gt;</span> b) <span class="ot">-&gt;</span> [a] <span class="ot">-&gt;</span> [b]
map _ []       <span class="fu">=</span> []
map f (x <span class="fu">:</span> xs) <span class="fu">=</span> f x <span class="fu">:</span> map f xs</code></pre>
<p>This is compiled with <code>-XStrict</code>. Now suppose we pass a function <code>f</code> which is lazy in it’s argument. Since <code>(:)</code> is strict in this code, we’ll still evaluate the <code>f x</code> before returning. Our guarantee that the list won’t be bottom and won’t have bottom still holds.</p>
<p>See also the paper <a href="http://research.microsoft.com/en-us/um/people/simonpj/papers/strict-core/tacc-hs09.pdf">“Types are calling conventions”</a>.</p>
<h2 id="haskells-denotational-semantics-says-that-lifted-types-have-bottoms">Haskell’s denotational semantics says that lifted types have bottoms</h2>
<p>This is true. Even if we have this strict type:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">data</span> <span class="dt">List</span> a <span class="fu">=</span> <span class="dt">Nil</span> <span class="fu">|</span> <span class="dt">Cons</span> <span class="fu">!</span>a <span class="fu">!</span>(<span class="dt">List</span> a)</code></pre>
<p>We can construct bottom values of this type, using, for example, <code>undefined :: List Int</code> or <code>(let x = x in x) :: Blah</code>.</p>
<p>However, if you think about how this value will be evaluated you’ll realize that this is exactly like how it would be evaluated in a call-by-value language. For example, if we try to bind it to a value:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">let</span> a <span class="fu">=</span><span class="ot"> undefined ::</span> <span class="dt">List</span> <span class="dt">Int</span> <span class="kw">in</span> <span class="fu">&lt;</span>body<span class="fu">&gt;</span></code></pre>
<p>This code will fail before <code>&lt;body&gt;</code> is run. In <code>&lt;body&gt;</code> <code>a</code> can’t be bottom and can’t contain bottoms.</p>
<p>(See also <a href="https://mail.haskell.org/pipermail/ghc-devs/2015-September/009799.html">this ghc-devs thread</a> about adding user defined unlifted data types to GHC. With this we could eliminate all the bottoms in some user-defined types.)</p>
<h2 id="but-haskell-will-still-generate-thunks-in-the-rts-level">But Haskell will still generate thunks in the RTS level?</h2>
<p>This is exactly right. <code>-XStrict</code> is really a very simple compiler pass that adds strictness annotations to every binder and field. We don’t have any changes in the GHC RTS to take advantage of additional strictness.</p>
<p>In other words, operational semantics of the language and implementation of this operational semantics in GHC RTS is still the same. We just do a program transformation to generate a program that evaluates like it would in a call-by-value language.</p>
<p>This means that if we have this program:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">let</span> a <span class="fu">=</span><span class="ot"> undefined ::</span> <span class="dt">List</span> <span class="dt">Int</span> <span class="kw">in</span> <span class="fu">&lt;</span>body<span class="fu">&gt;</span></code></pre>
<p>A thunk is still constructed in the runtime, but it’s evaluated <em>before</em> the <code>&lt;body&gt;</code> is evaluated. So this code fails before <code>&lt;body&gt;</code> is evaluated, and if we evaluate <code>&lt;body&gt;</code> it means that <code>a</code> is not bottom and doesn’t have bottoms.</p>
<p>This is another potential improvement over the <code>-XStrict</code>. For more details and some optimizations see <a href="https://mail.haskell.org/pipermail/ghc-devs/2015-October/010175.html">this ghc-devs thread</a>.</p>]]></summary>
</entry>
<entry>
    <title>On data representation in GHC Haskell</title>
    <link href="http://osa1.net/posts/2015-11-13-data-repr-1.html" />
    <id>http://osa1.net/posts/2015-11-13-data-repr-1.html</id>
    <published>2015-11-13T00:00:00Z</published>
    <updated>2015-11-13T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>It’s been a while since last time I wrote a blog post. This is not because I don’t have anything to write, rather, I have too much to write, and I was afraid that if I start writing it’d take too long.</p>
<p>But now that I started writing stuff for different applications(fellowships, internships etc.) I thought maybe this is a good time to write some blog posts too.</p>
<hr />
<p>At ICFP this year<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> we initiated a discussion about data representation in GHC Haskell. <a href="http://www.cs.indiana.edu/~rrnewton/homepage.html">My advisor</a> gave <a href="https://youtu.be/TT4poCUSf3A?t=59s">lightning talk at HIW</a>(Haskell Implementors Workshop). It’s a 5-minute talk and I recommend everyone reading this blog post to watch it. In the presentation, he showed this plot: (click on it to maximize)</p>
<p><a href="/images/data_repr_1/plot_old.png"><img src="/images/data_repr_1/plot_old_small.png" /></a></p>
<p>I put that here as a reference, but I’ll actually use some more detailed plots and correct a mistake in that plot. You can generate all the plots I show here using my benchmark programs <a href="https://github.com/osa1/spec_bench">here</a>.</p>
<p>In this post I want to make a point that is similar to Ryan’s point in the lightning talk: In Haskell, we’re not doing good job in data layouts. Our data is lazy by default, and laziness implies indirections(pointers). Updating a lazy record field means first generating a thunk and pointing to that thunk from the record. When the thunk is evaluated, we get one more indirection: A new pointer pointing to the actual data<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a>. This means that we need two pointer dereferencing just to read a single <code>Int</code> from a record.</p>
<p>At this point garbage collector helps to eliminate one level of indirection, and updates the field to point to the data directly. But this waits until the next garbage collection.</p>
<p>GHC has some support for “unpacking” fields of ADTs and records. When a field is “unpacked”, it means that 1) the field is strict 2) the value is not allocated separately and pointed to by a pointer, it’s part of the data constructor/record<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a>.</p>
<p>To illustrate how important unpacking is, I implemented two benchmarks. This is the data types I use in the benchmarks:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">data</span> <span class="dt">GL</span> a <span class="fu">=</span> <span class="dt">GLNil</span> <span class="fu">|</span> <span class="dt">GLCons</span> a (<span class="dt">GL</span> a)

<span class="kw">data</span> <span class="dt">SGL</span> a <span class="fu">=</span> <span class="dt">SGLNil</span> <span class="fu">|</span> <span class="dt">SGLCons</span> <span class="fu">!</span>a (<span class="dt">SGL</span> a)

<span class="kw">data</span> <span class="dt">IntList</span> <span class="fu">=</span> <span class="dt">ILNil</span> <span class="fu">|</span> <span class="dt">ILCons</span> <span class="ot">{-# UNPACK #-}</span> <span class="fu">!</span><span class="dt">Int</span> <span class="dt">IntList</span></code></pre>
<p>The first type, <code>GL</code>, is the exactly the same as GHC’s standard list type. Second one is mostly the same, only difference is I have a strictness annotation(a <a href="https://downloads.haskell.org/~ghc/7.10.2/docs/html/users_guide/bang-patterns.html"><code>BangPattern</code></a>) before in the head(or <code>car</code>) part of the list. Third one is similar to second one, except I also unpack the <code>car</code>.</p>
<p>Note that if you look at the types, first two of these types are parametric on the list elements, while the third one is specialized to integers. This is essentially monomorphization, and some compilers can do that automatically in some cases(<a href="http://mlton.org/Monomorphise">1</a>, <a href="http://www.impredicative.com/ur/">2</a>, if you know other compilers that do this, please write a comment), but in the presence of higher-order functions(and probably some other features), it’s in general not possible to monomorphise everything<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a>. GHC has some limited support for this with the <a href="https://downloads.haskell.org/~ghc/7.10.2/docs/html/users_guide/pragmas.html#specialize-pragma"><code>SPECIALIZE</code> pragma</a>, but it only works on functions and it doesn’t specialize data types(and maybe it can’t do this even in theory).</p>
<p>Now, I’m going to implement two functions on these data types. First function is for summing up all the elements:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">glSum ::</span> <span class="dt">GL</span> <span class="dt">Int</span> <span class="ot">-&gt;</span> <span class="dt">Int</span>
glSum <span class="fu">=</span> glSumAcc <span class="dv">0</span>

<span class="ot">glSumAcc ::</span> <span class="dt">Int</span> <span class="ot">-&gt;</span> <span class="dt">GL</span> <span class="dt">Int</span> <span class="ot">-&gt;</span> <span class="dt">Int</span>
glSumAcc <span class="fu">!</span>acc <span class="dt">GLNil</span>        <span class="fu">=</span> acc
glSumAcc <span class="fu">!</span>acc (<span class="dt">GLCons</span> h t) <span class="fu">=</span> glSumAcc (acc <span class="fu">+</span> h) t</code></pre>
<p>I implement exactly the same function for other types too.</p>
<p>Second function is the length function:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">glLength ::</span> <span class="dt">GL</span> a <span class="ot">-&gt;</span> <span class="dt">Int</span>
glLength <span class="fu">=</span> glLengthAcc <span class="dv">0</span>

<span class="ot">glLengthAcc ::</span> <span class="dt">Int</span> <span class="ot">-&gt;</span> <span class="dt">GL</span> a <span class="ot">-&gt;</span> <span class="dt">Int</span>
glLengthAcc <span class="fu">!</span>acc <span class="dt">GLNil</span>        <span class="fu">=</span> acc
glLengthAcc <span class="fu">!</span>acc (<span class="dt">GLCons</span> _ l) <span class="fu">=</span> glLengthAcc (acc <span class="fu">+</span> <span class="dv">1</span>) l</code></pre>
<p>(Similarly for other two types..)</p>
<p>Now I’m going to benchmark these two function for all three variants, but let’s just speculate about what would be the expected results.</p>
<p>Clearly the third one should be faster for <code>sum</code>, because we don’t need to follow pointers for reading the integer. But should the second type(parametric but strict) be any faster? I’d say yes. The reason is because the field is strict, and so when we do pattern matching on <code>Int</code> to add integers, we don’t need to enter any thunks, we know that <code>Int</code> is already in WHNF. We should be able to just read the field.</p>
<p>Here’s the result: (click on it to maximize)</p>
<p><a href="/images/data_repr_1/plot_sum.png"><img src="/images/data_repr_1/plot_sum_small.png" /></a></p>
<p>As you can see I have an extra line in this plot: I added GHC’s standard lists and used <a href="http://hackage.haskell.org/package/base-4.8.1.0/docs/Data-List.html#v:sum">standard <code>sum</code> function</a>. It’s ridiculously slow, it’s almost two orders of magnitude slower on a list with length 10^7. Before moving on and interpreting rest of the lines let’s just talk a bit about why this is slower. We only need to look at the type of <code>sum</code> function:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell">sum<span class="ot"> ::</span> (<span class="dt">Foldable</span> t, <span class="dt">Num</span> a) <span class="ot">=&gt;</span> t a <span class="ot">-&gt;</span> a</code></pre>
<p>This function is very, very general. There’s no way for this function to run fast, unless GHC is smart enough to generate a specialized version of this for <code>[Int]</code>. It turns out GHC is not smart enough, and in our case this is causing trouble. Note that we need two specializations here. First is to eliminate <code>Foldable t</code> part(we statically know that <code>t</code> is <code>[]</code>), and second is to eliminate <code>Num a</code> part(we statically know that <code>a</code> is <code>Int</code>).</p>
<p>Now that we have this out of the way, let’s look at the other 3 lines. We can see that unboxed version is really faster(0.039 seconds other lists vs. 0.018 seconds unboxed list), which was expected. The interesting part is strict version is exactly the same as lazy version. Now, I don’t have a good explanation for this. The generated Core and STG for these two variants of <code>sum</code> are exactly the same. The Cmm or assembly code should be different. The problem is, I really can’t make any sense of generated Cmm code. I should study Cmm more for this.</p>
<p>But I have an idea: Pattern matching means entering the thunk to reveal the WHNF structure. Since our integers are boxed, we need to pattern match on them to read the primitive <code>Int#</code>s. This means entering the thunks, even if the field is strict.</p>
<p>In my benchmarks, I only used completely normalized lists. This means that in the lazy case we enter thunks, only to return immediately, because the <code>Int</code> is already in WHNF. There’s no difference in lazy and strict variants in pattern matching.</p>
<p>Only difference is when we update the field, in which case generated code should be different if the field is strict.</p>
<p>To validate the second part of this claim, I wrote another benchmark. In this benchmark I again create a list of length 10^7, but every cell has an integer calculated using a function. I then measure list generation and consumption(sum) times. The idea is that in the case of strict list, list generation should be slower, but consumption should be faster, and the opposite in the lazy list case. Indeed we can observe this in the output:</p>
<pre><code>Performing major GC
Generating generic list
Took 0.737606563 seconds.
Performing major GC
Summing generic list
Took 0.490998969 seconds.
Performing major GC
Generating strict list
Took 0.870686580 seconds.
Performing major GC
Summing strict list
Took 0.035886157 seconds.</code></pre>
<p>The program is <a href="https://github.com/osa1/spec_bench/commit/b63322eb1edd32792837b58853c00ba0effad0a6">here</a>. We can see that summing strict list is 10x faster, but producing is slower, because instead of generating thunks we’re actually doing the work while producing cons cells.</p>
<p>(One thing to note here is that if the computation in thunks is not expensive enough, strict lists are faster in both production and consumption. I think the reason is because thunking overhead is bigger than actually doing the work in some cases)</p>
<p>OK, I hope this explains the story with <code>sum</code>. The second part of the benchmark is even more interesting. It runs <code>length</code>. Here’s the plot:</p>
<p><a href="/images/data_repr_1/plot_len.png"><img src="/images/data_repr_1/plot_len_small.png" /></a></p>
<p>We again have a line for standard Haskell list here. Good news is that even though standard <a href="http://hackage.haskell.org/package/base-4.8.1.0/docs/Data-List.html#v:length">length</a>’s type is again very general, this time GHC was able to optimize:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell">length<span class="ot"> ::</span> <span class="dt">Foldable</span> t <span class="ot">=&gt;</span> t a <span class="ot">-&gt;</span> <span class="dt">Int</span></code></pre>
<p>Interesting part is that unboxed list is again faster! But why? We’re not using head fields, whether it’s a pointer or not should not matter, right?</p>
<p>Here I again have an idea: Even though we never use head parts, the garbage collector has to traverse all the data, and copy them from one generation to other.</p>
<p>(We also allocate more, but I’m not measuring that in the benchmarks)</p>
<p>To again back my claims, I have another set of benchmark programs that generate some GC stats. Here’s the output for “generic” case:</p>
<pre><code>   800,051,568 bytes allocated in the heap
 1,138,797,344 bytes copied during GC
   310,234,360 bytes maximum residency (12 sample(s))
    68,472,584 bytes maximum slop
           705 MB total memory in use (0 MB lost due to fragmentation)

                                   Tot time (elapsed)  Avg pause  Max pause
Gen  0      1520 colls,     0 par    0.207s   0.215s     0.0001s    0.0009s
Gen  1        12 colls,     0 par    0.348s   0.528s     0.0440s    0.1931s

INIT    time    0.000s  (  0.000s elapsed)
MUT     time    0.126s  (  0.082s elapsed)
GC      time    0.555s  (  0.743s elapsed)
EXIT    time    0.004s  (  0.050s elapsed)
Total   time    0.685s  (  0.875s elapsed)

%GC     time      81.0%  (84.9% elapsed)

Alloc rate    6,349,615,619 bytes per MUT second

Productivity  19.0% of total user, 14.9% of total elapsed</code></pre>
<p>“Generic strict” case:</p>
<pre><code>   800,051,568 bytes allocated in the heap
 1,138,797,344 bytes copied during GC
   310,234,360 bytes maximum residency (12 sample(s))
    68,472,584 bytes maximum slop
           705 MB total memory in use (0 MB lost due to fragmentation)

                                   Tot time (elapsed)  Avg pause  Max pause
Gen  0      1520 colls,     0 par    0.211s   0.213s     0.0001s    0.0008s
Gen  1        12 colls,     0 par    0.367s   0.531s     0.0442s    0.1990s

INIT    time    0.000s  (  0.000s elapsed)
MUT     time    0.114s  (  0.080s elapsed)
GC      time    0.578s  (  0.744s elapsed)
EXIT    time    0.003s  (  0.045s elapsed)
Total   time    0.698s  (  0.869s elapsed)

%GC     time      82.8%  (85.6% elapsed)

Alloc rate    7,017,996,210 bytes per MUT second

Productivity  17.2% of total user, 13.8% of total elapsed</code></pre>
<p>“Unboxed” case:</p>
<pre><code>   560,051,552 bytes allocated in the heap
   486,232,928 bytes copied during GC
   123,589,752 bytes maximum residency (9 sample(s))
     3,815,304 bytes maximum slop
           244 MB total memory in use (0 MB lost due to fragmentation)

                                   Tot time (elapsed)  Avg pause  Max pause
Gen  0      1062 colls,     0 par    0.117s   0.123s     0.0001s    0.0018s
Gen  1         9 colls,     0 par    0.116s   0.179s     0.0199s    0.0771s

INIT    time    0.000s  (  0.001s elapsed)
MUT     time    0.070s  (  0.054s elapsed)
GC      time    0.233s  (  0.302s elapsed)
EXIT    time    0.002s  (  0.019s elapsed)
Total   time    0.306s  (  0.376s elapsed)

%GC     time      76.1%  (80.4% elapsed)

Alloc rate    8,000,736,457 bytes per MUT second

Productivity  23.9% of total user, 19.4% of total elapsed</code></pre>
<p>Interesting parts are productivity rates and total bytes allocated. We can see that unboxed version is a lot better in both.</p>
<p>The reason why productivities are too bad in all cases is, I think, that because this is purely an allocation benchmark, all we do is to allocate and then we do one pass on the whole thing. In this type of programs it makes sense to increase the initial heap a little bit to increase the productivity. For example, if I use <code>-H1G</code> productivity increases to 64% in generic list case and to 97% in unboxed list case.</p>
<p>So what’s the lesson here?</p>
<p>The data layout matters a lot. Even if the data is not used in some hot code path, GC needs to traverse all the live data and in the case of GHC Haskell it needs to copy them in each GC cycle. Also, lazy-by-default is bad for performance.</p>
<h1 id="an-improvement">An improvement</h1>
<p>I recently finished implementing hard parts of a project. I don’t want to give too much detail here for now but let’s just say we’re improving the unboxing story in GHC. With our new patch, you will be able to <code>{-# UNPACK #-}</code> some of the types that you can’t right now. When/if the patch lands I’m going to announce it here.</p>
<p>With <a href="http://downloads.haskell.org/~ghc/master/users-guide/glasgow_exts.html?highlight=strictdata#strict-haskell"><code>-XStrictData</code></a>, <a href="https://phabricator.haskell.org/D1142"><code>-XStrict</code></a> and flags like <code>-funbox-strict-fields</code>(search for it <a href="https://downloads.haskell.org/~ghc/7.10.2/docs/html/users_guide/flag-reference.html">here</a>) we’ll have a lot better story about data layout than we have today. But there are still some missing pieces. Our patch will hopefully implement one more missing piece and we’ll have even better data layout story.</p>
<p>I think the next step then will be <a href="https://mail.haskell.org/pipermail/ghc-devs/2015-October/010175.html">better calling conventions for strict functions</a> and some other tweaks in the runtime system for better strict code support overall. Then maybe we can officially declare Haskell as a language with lazy and strict evaluation ;-) .</p>
<hr />
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>I realized that I forgot to announce it here, but <a href="/papers/cnf.pdf">we had a paper at ICFP</a>, and I gave a <a href="https://www.youtube.com/watch?v=gkx-D-7Y1EU">talk at Haskell Implementors Workshop</a> this year.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>That data is still not totally normalized, it’s in weak head normal form. The new heap object that we get when we evaluate a thunk is called an “indirection”. See more details <a href="https://ghc.haskell.org/trac/ghc/wiki/Commentary/Rts/Storage/HeapObjects#Indirections">here</a>.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>See the <a href="https://downloads.haskell.org/~ghc/7.10.2/docs/html/users_guide/pragmas.html#unpack-pragma">GHC user manual entry</a> for more info.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>I’m hoping to make this a subject to another blog post. One thing to note here is that even when monomorphization is possible, it may cause a code explosion.<a href="#fnref4">↩</a></p></li>
</ol>
</div>]]></summary>
</entry>
<entry>
    <title>The issue with work sharing (common subexpression elimination)</title>
    <link href="http://osa1.net/posts/2015-08-13-the-issue-with-work-sharing.html" />
    <id>http://osa1.net/posts/2015-08-13-the-issue-with-work-sharing.html</id>
    <published>2015-08-13T00:00:00Z</published>
    <updated>2015-08-13T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>I’d expect more work sharing to be always more beneficial. But apparently this is not the case, as pointed out in (Chitil, 1997)<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>.</p>
<p>Here’s an example from the paper: (slightly changed)</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell">sum [<span class="dv">1</span> <span class="fu">..</span> <span class="dv">1000</span>] <span class="fu">+</span> sum [<span class="fu">-</span><span class="dv">1000</span> <span class="fu">..</span> <span class="fu">-</span><span class="dv">1</span>] <span class="fu">+</span> prod [<span class="dv">1</span> <span class="fu">..</span> <span class="dv">1000</span>]</code></pre>
<p>We can evaluate this expression to WHNF using heap space enough for a single list(to be more specific, we only need a single cons cell at any time). After evaluating a subexpression, we can deallocate and allocate for the next list etc.</p>
<p>However, if we eliminate common subexpressions, and generate this code:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">let</span> v <span class="fu">=</span> [<span class="dv">1</span> <span class="fu">..</span> <span class="dv">1000</span>]
 <span class="kw">in</span> sum v <span class="fu">+</span> sum [<span class="fu">-</span><span class="dv">1000</span> <span class="fu">..</span> <span class="fu">-</span><span class="dv">1</span>] <span class="fu">+</span> prod v</code></pre>
<p>Now <code>v</code> has to live until the let body is evaluated to a value. We win in allocation/deallocation side, but we lose in residency side. In paper’s words: “Whereas the transformation always decreases total heap usage, it may considerably influence heap residency.”</p>
<p>In general, we can’t do this transformation, without risking increased residency:</p>
<p>\[ e’[e,e] \leadsto \texttt{let}\; x = e\; \texttt{in}\; e’[x,x] \]</p>
<p>As a solution, the paper suggests this:</p>
<ol style="list-style-type: decimal">
<li>We always do CSE if the subexpressions’ WHNF == NF(i.e. if it’s a “safe type” in paper’s terms). According to the paper, “a partially evaluated expression is certain to require only a small, fixed amount of space if it’s not a function, whose environment may refer to arbitrary large data structures, and its WHNF is already its normal form”.</li>
<li>We always do CSE when a named expression is syntactically dominating another equal expression:</li>
</ol>
<p>\[ \texttt{let}\; x = e\; \texttt{in}\; e’[e] \leadsto \texttt{let}\; x = e\; \texttt{in}\; e’[x] \]</p>
<hr />
<p>Note that (1) is not always true, assume an expression with type <code>ForeignPtr a</code> where <code>a</code> is a huge FFI object. This has WHNF == NF property, but it may increase residency significantly. Maybe GHC didn’t have FFI at the time the paper is written.</p>
<p>Also, I’m wondering how is CSE is handled in current GHC.</p>
<hr />
<p>In supercompilation, we want to avoid evaluating same expressions in a loop forever, so we keep some kind of “history”, and when we come across a term that we evaluated before, we fold the process tree and avoid evaluating same term again.</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell">(fib <span class="dv">1000</span>, fib <span class="dv">1000</span>)</code></pre>
<p>Unless we make sure to split it in a way that branches of the process tree are unaware of each other, we may end up eliminating common subexpressions. However, since there are lots of cases where we may want CSE, a splitter that always prevents it is not always desirable. We should instead allow CSE in a controlled way.</p>
<hr />
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Olaf Chitil, “Common Subexpression Elimination in a Lazy Functional Language”, section 3.5.<a href="#fnref1">↩</a></p></li>
</ol>
</div>]]></summary>
</entry>
<entry>
    <title>The issue of splitting without work duplication</title>
    <link href="http://osa1.net/posts/2015-08-13-the-issue-of-splitting-wo-duplication.html" />
    <id>http://osa1.net/posts/2015-08-13-the-issue-of-splitting-wo-duplication.html</id>
    <published>2015-08-13T00:00:00Z</published>
    <updated>2015-08-13T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>(I’m starting publishing my long list of unpublished blog posts with this post)</p>
<p>(Examples are from <a href="http://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-835.html">Bolingbroke’s PhD thesis</a>)</p>
<p><em>Example 1:</em></p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">let</span> a  <span class="fu">=</span> id y
    id <span class="fu">=</span> \x <span class="ot">-&gt;</span> x
 <span class="kw">in</span> <span class="dt">Just</span> a</code></pre>
<p><em>Problem:</em> The compiler should know about <code>id</code> while compiling <code>a</code>. This is easy to do, just tell the compiler about every binding when compiling RHSs. However, it causes some other problems:</p>
<p><em>Example 2:</em></p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">let</span> n <span class="fu">=</span> fib <span class="dv">100</span>
    b <span class="fu">=</span> n <span class="fu">+</span> <span class="dv">1</span>
    c <span class="fu">=</span> n <span class="fu">+</span> <span class="dv">2</span>
 <span class="kw">in</span> (b, c)</code></pre>
<p><em>Problem:</em> If we tell about <code>n</code> to the compiler when it’s compiling <code>b</code> and <code>c</code>, we’re taking the risk of work duplication. It may seem like <code>fib 100</code> will be evaluated in compile time and so duplication is not a huge deal, but this is not necessarily the case. First, we can’t know if it’s going to be evaluated to a value in compile time. Second, even if it’s a closed term and we somehow know it’s going to be terminated, termination checker of the evaluator may want to stop it before it’s evaluated to a value. Third, most of the time it’ll be an open term that’ll get stuck in the middle of supercompilation.</p>
<p>And when that happens we will generate a let-binding in residual code. In our case, we’ll be generating two let-bindings, one is for <code>b</code> and one is for <code>c</code>, and those let bindings will be doing same work.</p>
<hr />
<p><em>Question:</em> Can we rely on a post-processsing pass to eliminate common subexpressions? I.e. if we generate a code like this:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">let</span> b <span class="fu">=</span> <span class="kw">let</span> n_supercompiled <span class="fu">=</span> <span class="fu">&lt;</span>supercompiled fib <span class="dv">100</span><span class="fu">&gt;</span>
         <span class="kw">in</span> n_supercompiled <span class="fu">+</span> <span class="dv">1</span>
    c <span class="fu">=</span> <span class="kw">let</span> n_supercompiled <span class="fu">=</span> <span class="fu">&lt;</span>supercompiled fib <span class="dv">100</span><span class="fu">&gt;</span>
         <span class="kw">in</span> n_supercompiled <span class="fu">+</span> <span class="dv">2</span>
 <span class="kw">in</span> (b, c)</code></pre>
<p>It would transform it to obvious residual code that has single <code>n_supercompiled</code> which is in scope of <code>b</code> and <code>c</code>.</p>
<p>What are trade-offs?</p>
<hr />
<p>Finding a good heuristic is hard. Let’s say we try to estimate costs of expressions and decide whether to tell the compiler about them or not. If we decide that <code>ys</code> and <code>xs</code> are expensive in this case:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">let</span> map <span class="fu">=</span> <span class="fu">...</span>
    ys <span class="fu">=</span> map f zs
    xs <span class="fu">=</span> map g ys
 <span class="kw">in</span> <span class="dt">Just</span> xs</code></pre>
<p>We miss a deforestation opportunity, because the compiler won’t know about <code>ys</code> while compiling <code>xs</code>.</p>]]></summary>
</entry>
<entry>
    <title>On sufficiently smart compilers</title>
    <link href="http://osa1.net/posts/2015-08-09-sufficiently-smart-compiler.html" />
    <id>http://osa1.net/posts/2015-08-09-sufficiently-smart-compiler.html</id>
    <published>2015-08-09T00:00:00Z</published>
    <updated>2015-08-09T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>I’ve been thinking about optimizing functional programs recently, for a project that I’m hoping to make my research topic in the near future. You probably already know about <a href="http://c2.com/cgi/wiki?SufficientlySmartCompiler">The Myth of the Sufficiently Smart Compiler</a>. It basically says that the advanced compiler that optimizes your high-level, highly-abstracted programs to efficient low-level code, is basically a myth.</p>
<p>This post is a brain dump on sufficiently smart compilation of functional programs and some compilation techniques. I’ll first make some seemingly-unrelated points, and then hopefully use them to argue that the sufficiently smart compiler is not a myth, it just needs some hard work to be realized.</p>
<h2 id="unreliable-optimizations-and-performance-critical-software">Unreliable optimizations and performance-critical software</h2>
<p>Every once in a while I see some blog posts about optimizing a JIT-compiled program by inspecting JIT trace dumps and generated code carefully, and I find this horrible, for the following reasons:</p>
<ul>
<li><p>It couples your program design with the JIT compiler’s internals. From a software engineering point of view, I think this is really one of the worst things that can happen to a software. You end up structuring your code with the compiler’s convenience in mind. But compilers can’t make sense of high-level, abstracted code (remember the myth?). So you end up with a code that’s low-level, hard to read, understand and maintain. And what happens when a new version of the compiler is released?</p></li>
<li><p>JIT compilers are highly complex, and as a result they’re very hard to reason about and this complex design makes them unpredictable. A seemingly-unrelated change in your program can make the traces go significantly bad, and result in less optimized code, because maybe the change somehow made it to the trace and now you need to refactor your code.</p></li>
<li><p>If you need performance that bad, and you’re willing to read traces and generated assembly output for that, you could probably just write in a language that makes low-level optimizations easy/possible<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>. Or at least write performance-critical parts in a low-level language. Both of these cases eliminate the need for a JIT compiler.</p></li>
</ul>
<p>I think the last point is worth discussing further. Most JIT compilers we use nowadays are for compiling dynamic languages<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a>. By their nature dynamic languages are hard to optimize in compile time, so they rely on runtime knowledge for optimizations. But does that make JIT compilers useless for statically-typed languages that are more amenable to compile-time optimizations? I don’t have a good answer to this, probably because I’m not a JIT expert. I think the fact that <a href="http://openjdk.java.net/groups/hotspot/">HotSpot</a> is doing good job is not an answer to this, because in JVM there’s bytecode interpretation going on, and this is adding some room for runtime optimizations. Namely, you have one level of indirection that you can eliminate using JIT compilation.</p>
<p>In other statically-typed, compiled languages like C++, Haskell, OCaml etc. there’s less room for that kind of optimizations. I think applicability of JIT compilation techniques to these type of languages would make an interesting topic for a research project.</p>
<h2 id="compilers-that-can-learn-your-domain-and-manipulate-your-programs">Compilers that can learn your domain and manipulate your programs</h2>
<p>High-level languages and abstractions make efficient execution of programs harder, but there are a couple of things that they can do to help with the compilation. Namely, you can guide the compiler to optimize your domain-specific code.</p>
<p>One nice and simple example is <a href="https://downloads.haskell.org/~ghc/7.0.1/docs/html/users_guide/rewrite-rules.html">rewrite rules of GHC</a>. They’re used quite heavily in <a href="http://hackage.haskell.org/package/base">base</a> (GHC’s standard library) to eliminate intermediate lists. Other libraries use the same mechanism to tell the compiler how to optimize the code that uses their abstractions<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a>.</p>
<p>But for a compiler to support this kind of program transformations the language has to have some properties. In our case, we should be able to reason about the code in compile time, and locally, i.e. without thinking about runtime execution environment (heap, stack, variables in scope etc.) and the interaction of our code with the rest of the code. This is possible in purely functional languages because they make <a href="http://www.haskellforall.com/2013/12/equational-reasoning.html">equational reasoning</a> possible.</p>
<p>This is a very powerful property. This makes it possible to see programs as terms in an algebra, and we can freely manipulate these terms according to our rules. In the most basic sense, these rules can be the rules that define our language’s operational semantics, because by its very definition these rules are guaranteed to preserve semantics of programs. But we can go even further by adding rewrite rules to these rules. Rewrite rules are a way to say, “trust me, this transformation preserves semantics” and at that point a compiler is free to use these rules.</p>
<p>Furthermore, some properties of the language can give us <a href="http://ttic.uchicago.edu/~dreyer/course/papers/wadler.pdf">free theorems</a>, which in turn can help us with <a href="http://research.microsoft.com/en-us/um/people/simonpj/Papers/deforestation-short-cut.pdf">some optimizations</a><a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a>.</p>
<p>This type of “algebraic manipulation of programs” is a very powerful concept, and it can do great things. A very good example is this <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.48.7721">1997 paper about optimizing Haskell</a>. Most (maybe all?) of the transformations described in that paper are still in use.</p>
<h2 id="compilers-that-preserve-the-semantics">Compilers that preserve the semantics</h2>
<p>You probably wouldn’t want a compiler that compiles your programs to programs that do different things. We expect it to preserve the semantics. But that rule is sometimes too strict, and prevents some optimizations.</p>
<p>For example, if floating points and operations on floating points in your language are defined as they’re defined in IEEE-754, then the compiler can’t assume associativity of floating point operations and you lose some optimization opportunities. GCC’s <code>-ffast-math</code> is for relaxing this restriction by letting the compiler assume this associativity.</p>
<p>Another example is termination properties of programs. For example, would you be OK with this transformation in a purely functional language:</p>
<pre><code>(λx . 1) loop ~&gt; 1</code></pre>
<p>In a call-by-name (or call-by-need, which is an efficient implementation of call-by-name) language, this is a valid transformation. But in call-by-value language this would change the semantics. Previously this program was looping, but now it returns 1.</p>
<p>This example is actually a good demonstration of a problem that we have even in purely functional languages. Namely, there are some programs that don’t map to any values in the domain you use to model your language<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a>. The way these programs are modeled are generally by defining a special value, ⊥ (read “bottom”). Non-terminating and exception/error throwing programs are said to be “bottom” and denoted with this value. Bottom values are said to be “less defined” than non-bottom values.</p>
<p>Using this definition, we can say that the transformation shown above transforms a program to a more defined one. You might want this restriction of preserving definedness of programs for different reasons, and here’s an example reason: Without this restriction, your program may terminate or loop depending on how the compiler performed. A seemingly-unrelated change in your program may cause a different termination behavior.</p>
<p>Now this is a hard problem. There are papers about transforming call-by-value functional languages while preserving termination properties (see <a href="https://www.sics.se/~pj/papers/scp/popl09-scp.pdf">this</a> as an example). In general, we can’t decide if a program is bottom or not. First of all, that would be solving the halting problem. But more specifically, we can’t do this transformation if <code>y</code> depends on a dynamic input here:</p>
<pre><code>(λx . 1) (1 / y) ~&gt; 1</code></pre>
<p>In most cases though, the compiler is simply not able to propagate enough information to this stage to see if <code>y</code> can be <code>0</code> or not<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a>, even if all the necessary information is available in compile time.</p>
<h2 id="making-the-most-out-of-available-input">Making the most out of available input</h2>
<p>There’s an old yet IMHO under-appreciated technique for taking statically known inputs into account while compiling programs. It’s called “partial evaluation” and described in details in this awesome book <a href="http://www.itu.dk/~sestoft/pebook/jonesgomardsestoft-a4.pdf">“Partial Evaluation and Automatic Program Generation”</a> by Neil D. Jones, Carsten K. Gomard and Peter Sestoft. One very interesting but somewhat esoteric application of this idea is <a href="https://cs.au.dk/~hosc/local/HOSC-12-4-pp381-391.pdf">Futamura projections</a><a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a>, but to give a easier to understand example, a C partial evaluator could read your Vim config in compile time and compile Vim to an executable that doesn’t read any Vim files on startup because it’s already specialized to the Vim config it read in compile time<a href="#fn8" class="footnoteRef" id="fnref8"><sup>8</sup></a>. General tools may depend of lots of dynamic input, but in your special case you may fix some of these variables and this is where a partial evaluation comes into the play. See <a href="http://blog.regehr.org/archives/1197">this blog post</a> for another example.</p>
<p>How much further could it propagate this statically known input and specialize rest the code using it? That’s completely different story and comes with some very hard to solve problems. I’ll again come to this later.</p>
<p>The whole point is to generate specialized code for known input. We can shift the stage a little bit and apply this idea in runtime, and that gives us <a href="http://www.cs.rice.edu/~taha/MSP/">multi-stage programming</a>.</p>
<p>MSP allows us to generate code in runtime, link it to the program in a way that the generated code runs in the current execution environment (i.e. the generated code can refer to names in enclosing scope, pretty much like how closures would do).</p>
<p>Traditionally, MSP doesn’t allow code generation in compile-time, and the techniques used for code generation are completely different<a href="#fn9" class="footnoteRef" id="fnref9"><sup>9</sup></a>. But we can generate code specialized to input that is only available in runtime. For example, you can write a game that runs code specialized to the player’s options. Or run a web server that does some optimizations on request dispatch code depending on some analysis on recent requests.</p>
<p>This is again a very powerful concept, and only recently I started to appreciate its potential<a href="#fn10" class="footnoteRef" id="fnref10"><sup>10</sup></a>. IMO, MSP is missing a “killer language”<a href="#fn11" class="footnoteRef" id="fnref11"><sup>11</sup></a> (and also a “killer application” but I think that follows the language) and I’m hoping to make some progress on this front in the future.</p>
<h2 id="finally-a-sufficiently-smart-compiler">Finally, a sufficiently smart compiler</h2>
<p>This post may seem to be going nowhere, so let’s back up a bit and come to the point.</p>
<p>I define a sufficiently smart compiler not as a completely automated program, but as a toolchain. This toolchain has a completely automated compiler, but it also gives programmers tools for runtime code generation, and for teaching the compiler domain-specific optimizations. The compiler knows about the language’s semantics, and when possible it does reductions in compile time to remove abstractions and leave less work to runtime.</p>
<p>While doing reductions in compile time, it takes programmers’ rules into account, and optimizes abstractions accordingly. This allows it to optimize domain-specific abstractions that normally a compiler would have no way of knowing.</p>
<p>By now it should be clear that such a compiler is only possible with a language that allows these optimizations. For example, without a purely functional language, rewrite rules are not easy, if not impossible.</p>
<p>The compiler gradually compiles the language into languages that are more and more close to the machine language that it has to generate in the end. Reductions and user rules are applied in a level where programs are still expressed in a purely functional language. This language should be sufficient for most optimizations that eliminate programmers’ abstractions in compile time.</p>
<p>This way, programmers don’t need to look at ridiculous bytecode traces or instructions written in a highly-complex assembly language to figure out how things are optimized, and rather they stay in the same level of abstraction that their programs are written in. When they want to know about memory allocations, for example, they should be able to look at the next level in the compilation, which should have explicit memory allocation operations and pointers etc. The main point is that they stay in a level where they can observe some particular behavior (e.g. memory allocation) of a program and they don’t have to read assembly, for example, to see if their higher-order <code>map</code> application that uses an increment function to increment integers in a list is compiled to a loop without any function calls.</p>
<p>In this compiler there’s no room for abstraction-breaking, unreliable optimizations or optimizations that cause coupling with the compiler’s internals, like in the case of JIT compilers.</p>
<p>In the beginning I said that I don’t see this as a myth. So how I think this is possible to implement? This is already a long-enough post, and I’ll stop for now. Let me just say that almost all of these things are implemented in different projects:</p>
<ul>
<li><p>MSP does runtime code generation and <a href="http://okmij.org/ftp/ML/MetaOCaml.html">MetaOCaml</a> gives us a nice way to do that in a safe way. Another alternative is <a href="http://terralang.org/">Terra</a>, but in Terra generated code is in a different language, so that’s quite different (also, it’s a dynamically typed language that gives no guarantees about generated code).</p></li>
<li><p>Domain-specific optimizations are possible in Haskell thanks to GHC’s rewrite rules, as mentioned in the related section above.</p></li>
<li><p>GHC’s internal languages Core, STG and Cmm allow programmers to gradually go low level and see the details they’re looking for. Most of the time Core is enough to see if your abstractions are eliminated in compile time and if your rules worked as expected.</p></li>
<li><p>Compile time reduction of programs are done by supercompilers. It was a lesser known technique until recently a couple of papers (<a href="http://dl.acm.org/citation.cfm?id=1863588">1</a>, <a href="http://research.microsoft.com/en-us/um/people/simonpj/papers/supercompilation/supercomp-by-eval.pdf">2</a>) and a <a href="http://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-835.html">PhD thesis</a> explored it in the context of Haskell.</p></li>
</ul>
<p>Some of these features are orthogonal to each other, like MSP and compile-time reductions. But some others are not, for example, we expect a supercompiler to take rewrite rules into account, otherwise it may be impossible to do some optimizations.</p>
<p>The hardest part seems to be compile-time reductions of programs according to operational semantics of the language, which involves some very hard problems, and one of the reasons has to do with preserving semantics. In the next couple of posts I’m hoping to talk about that, and in the meantime you can refer to chapter 9 of the PhD thesis I linked above.</p>
<hr />
<p>This post has made it to <a href="https://www.reddit.com/r/haskell/comments/3wdrv6/on_sufficiently_smart_compilers/">/r/haskell</a>, <a href="https://www.reddit.com/r/compsci/comments/3wdqro/on_sufficiently_smart_compilers/">/r/compsci</a> and <a href="https://news.ycombinator.com/item?id=10733201">Hacker News</a>. Thanks for sharing this!</p>
<hr />
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p><a href="https://github.com/SnabbCo/snabbswitch">Snabb Switch</a> project comes to mind here. It’s a Lua project and they rely on LuaJIT to optimize their code. See this series of blog posts: <a href="https://github.com/lukego/blog/issues/5">1</a>, <a href="https://github.com/lukego/blog/issues/6">2</a>, <a href="https://github.com/lukego/blog/issues/8">3</a>.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p><a href="https://developers.google.com/v8/">V8</a> and <a href="https://wiki.mozilla.org/JavaScript:TraceMonkey">TraceMonkey</a> for JavaScript, <a href="http://luajit.org/">LuaJIT</a> for Lua, <a href="http://pypy.org/">PyPy</a> for Python. There are also research-level JIT compilers, like <a href="https://github.com/higgsjs/Higgs">Higgs</a> for JavaScript and <a href="https://github.com/samth/pycket">Pycket</a> (<a href="https://rpython.readthedocs.org/en/latest/">RPython</a> based, created by colleagues from IU) for Racket.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>One example that I like very much is the <a href="http://hackage.haskell.org/package/pipes">pipes library</a>. You can see some of its rewrite rules <a href="https://github.com/Gabriel439/Haskell-Pipes-Library/blob/d7b1430b1b35abfde98b32cbc4aae02a4e027dd0/src/Pipes/Core.hs#L869">here</a>.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>One very good question to ask here is, what exactly gives us free theorems? I don’t have an answer to that question yet.<a href="#fnref4">↩</a></p></li>
<li id="fn5"><p>This type of giving semantics to languages is called “denotational semantics”. I don’t have very good reading material about this but you may want to have a look at <a href="https://en.wikibooks.org/wiki/Haskell/Denotational_semantics">this</a>.<a href="#fnref5">↩</a></p></li>
<li id="fn6"><p>We’re assuming that it somehow knows that divide-by-zero leads to bottom.<a href="#fnref6">↩</a></p></li>
<li id="fn7"><p>I wrote about this <a href="http://osa1.net/posts/2015-01-11-understanding-futamura-projections.html">previously</a> and I also have <a href="http://osa1.net/posts/2015-05-13-comp-through-interp.html">this related project</a>.<a href="#fnref7">↩</a></p></li>
<li id="fn8"><p>In practice this is probably hard to achieve, and it certainly needs some refactoring in current Vim codebase.<a href="#fnref8">↩</a></p></li>
<li id="fn9"><p>See <a href="http://osa1.net/posts/2015-05-13-comp-through-interp.html">my blog post</a> for a comparison.<a href="#fnref9">↩</a></p></li>
<li id="fn10"><p>Even though I’ve been working on MSP languages for a while know. See my previous work on this: <a href="/posts/2013-04-15-internship.report.html">1</a>, <a href="/posts/2014-03-06-proving-simply-typed-multi-staged-lc.html">2</a>, <a href="/posts/2015-05-13-comp-through-interp.html">3</a>, and here’s a <a href="/posts/2015-05-17-staging-is-not-just-codegen.html">ranty post</a>.<a href="#fnref10">↩</a></p></li>
<li id="fn11"><p><a href="http://terralang.org/">Terra</a> comes quite close, but I have some confusions about it and I’m hoping to write about those in the future.<a href="#fnref11">↩</a></p></li>
</ol>
</div>]]></summary>
</entry>
<entry>
    <title>Top-down expression parsing is easy</title>
    <link href="http://osa1.net/posts/2015-01-29-top-down-expr-parsing-easy.html" />
    <id>http://osa1.net/posts/2015-01-29-top-down-expr-parsing-easy.html</id>
    <published>2015-01-29T00:00:00Z</published>
    <updated>2015-01-29T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>I recently fixed <a href="http://hackage.haskell.org/package/language-lua">language-lua</a>’s 2-years-old expression parsing bug. Previously it was using <a href="http://hackage.haskell.org/package/parsec-3.1.8/docs/Text-Parsec-Expr.html">Parsec’s expression parser</a>, which is actually horrible because it can’t handle chained unary operators.</p>
<p>Two weeks ago I decided to take a look into Lua’s original implementation, and in about an hour or so the algorithm was crystal clear to me. I immediately <a href="https://github.com/osa1/language-lua/commit/b4bebe36e927dcc671dbe6dd19572b83073dc556#diff-630bbd2d118baf109da6ad79d3f168bfR257">implemented it</a> and closed the <a href="https://github.com/osa1/language-lua/issues/2">2-years-old bug report</a>.</p>
<p>This implementation is essentially a port of Lua’s expression parser. Recently I thought about the algorithm and I was wondering if this has a name – the algorithm looked pretty obvious to me once I understand and given how much we know about parsing I thought this should have a name.</p>
<p>I found <a href="http://www.engr.mun.ca/~theo/Misc/exp_parsing.htm#climbing">this algorithm named “precedence climbing”</a>. This is almost the same algorithm, only difference is that instead of using <code>lookahead</code> I’m just consuming the binary operator and returning it to the caller(which is parsing an expression with lower precedence than current parser) if precedence is lower. Associativity handling is also different(I use different left and right precedences to handle associativity) but the idea is really the same.</p>
<p>Now, there is also another algorithm called Pratt, and I can’t read the original paper(paywall), but according to <a href="http://lambda-the-ultimate.org/node/3682">this LtU discussion</a> it should also be similar. Indeed, <a href="http://journal.stuffwithstuff.com/2011/03/19/pratt-parsers-expression-parsing-made-easy/">this explanation of it</a> looks pretty similar, and <a href="http://stackoverflow.com/a/13637731/691032">this StackOverflow answer</a> says that Lua’s implementation is “Pratt style parsing”.</p>
<p>So it seems like we have two, or maybe one since they’re actually very similar, solution(s) to solve top-down expression parsing problem and Haskell implementation using Parsec is possible in only 12 lines of code.</p>
<h1 id="a-challenge">A challenge</h1>
<p>One challenge might be to modify Parsec’s expression parser so that internally it generates a Pratt/precedence climbing parser. I’m hoping to spare some time to work on this.</p>]]></summary>
</entry>
<entry>
    <title>Loading dynamic Haskell libs in Lua</title>
    <link href="http://osa1.net/posts/2015-01-16-haskell-so-lua.html" />
    <id>http://osa1.net/posts/2015-01-16-haskell-so-lua.html</id>
    <published>2015-01-16T00:00:00Z</published>
    <updated>2015-01-16T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>Last year I wrote <a href="http://osa1.net/posts/2014-04-27-calling-haskell-lua.html">a blog post</a> in which I explained how to call Lua from Haskell and Haskell from Lua using <a href="http://hackage.haskell.org/package/hslua">hslua</a> library. At the end of that blog post I mentioned that it should be possible to compile Haskell code to shared library and load that in Lua.</p>
<p>Today a friend in our research group <a href="https://github.com/iu-parfunc">parfunc</a> asked a question about compiling Haskell to shared libraries and loading generated libraries in other programs and I thought while I’m at it I can just update my blog post as well. So in this post I’m going to explain how to compile Haskell functions to shared libraries and load them in Lua.</p>
<p>Before diving into the code, a few remarks:</p>
<ul>
<li>All the code in this blog post is tried on Linux, with Lua 5.1 and latest LuaJIT.</li>
<li>To be able to load our function in Lua and register it, our functions should have C linkage and <a href="http://www.lua.org/manual/5.1/manual.html#lua_CFunction"><code>lua_CFunction</code></a> type. We can either write Haskell functions directly using this type, or write C wrapper functions around our Haskell functions to be able to use them in Lua. In this post I’m going to do first one.</li>
<li>We’ll need some intermediate C code to expose some Haskell RTS functions to Lua, like <code>hs_init</code> to start Haskell runtime and <code>hs_exit</code> to stop it.</li>
<li>To be able to <code>require</code> our shared library in Lua, we need to implement a <code>int luaopen_&lt;ourlibrary&gt;(lua_State *L)</code> function. While in theory it should be possible to implement that function in Haskell, I’ll implement it in C in this post, because I’m not sure how to write Lua wrappers for <code>hs_init</code> and <code>hs_exit</code> in Haskell.</li>
<li>To keep the code as simple as possible, our Haskell function will be a very dumb addition function.</li>
</ul>
<p>Let’s start.</p>
<h1 id="defining-lua-function-in-haskell">Defining Lua function in Haskell</h1>
<p>This is exactly the same as before: We just define a function with type: <code>LuaState -&gt; IO Int</code>. To keep the code simple, we don’t do error handling at all.</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">module</span> <span class="dt">LibArith</span> <span class="kw">where</span>

<span class="kw">import </span><span class="dt">Data.Maybe</span>
<span class="kw">import </span><span class="dt">Scripting.Lua</span> <span class="co">-- this one from hslua</span>

foreign export ccall
<span class="ot">  add ::</span> <span class="dt">LuaState</span> <span class="ot">-&gt;</span> <span class="dt">IO</span> ()

<span class="ot">add ::</span> <span class="dt">LuaState</span> <span class="ot">-&gt;</span> <span class="dt">IO</span> ()
add l <span class="fu">=</span> <span class="kw">do</span>
  i1 <span class="ot">&lt;-</span> fromJust <span class="ot">`fmap`</span> peek l <span class="dv">1</span>
  i2 <span class="ot">&lt;-</span> fromJust <span class="ot">`fmap`</span> peek l <span class="dv">2</span>
  pop l <span class="dv">2</span>
  push l (i1 <span class="fu">+</span><span class="ot"> i2 ::</span> <span class="dt">Int</span>)
  return <span class="dv">1</span></code></pre>
<h1 id="implementing-intermediate-c">Implementing intermediate C</h1>
<p>In our C glue code, we do two things:</p>
<ol style="list-style-type: decimal">
<li>Wrap <code>hs_init</code> and <code>hs_exit</code> Haskell runtime functions.</li>
<li>Implement Lua C module interface in which we register our functions to Lua. (see <a href="http://www.lua.org/manual/5.1/manual.html#pdf-package.loaders">related docs</a> for details)</li>
</ol>
<p>Here’s the code:</p>
<pre class="sourceCode c"><code class="sourceCode c"><span class="ot">#include &quot;LibArith_stub.h&quot;</span>
<span class="ot">#include &quot;lua.h&quot;</span>

<span class="dt">int</span> hs_init_lua(lua_State *L)
{
  hs_init(NULL, NULL);
  <span class="kw">return</span> <span class="dv">0</span>;
}

<span class="dt">int</span> hs_exit_lua(lua_State *L)
{
  hs_exit();
  <span class="kw">return</span> <span class="dv">0</span>;
}

<span class="dt">int</span> luaopen_lualibhelper(lua_State *L)
{
  lua_pushcfunction(L, add);
  lua_setglobal(L, <span class="st">&quot;add_in_haskell&quot;</span>);
  lua_pushcfunction(L, hs_init_lua);
  lua_setglobal(L, <span class="st">&quot;hs_init&quot;</span>);
  lua_pushcfunction(L, hs_exit_lua);
  lua_setglobal(L, <span class="st">&quot;hs_exit&quot;</span>);
  <span class="kw">return</span> <span class="dv">0</span>;
}</code></pre>
<p>Some things to note:</p>
<ul>
<li><code>LibArith_stub.h</code> is generated by GHC. I’ll explain how to compile and link next.</li>
<li>Our Haskell function actually has type <code>HsInt (*)(void *)</code>. While this is not what Lua API expected(it expects <code>int (*)(lua_State *L)</code>), in my x86_64 Linux machine this is working fine. In the worst case, you may need to wrap the Haskell function in C and convert the types using Haskell RTS C API and Lua C API.</li>
</ul>
<h1 id="compiling-and-linking">Compiling and linking</h1>
<p>This is the tricky part, I wasted a good 2 hours trying to figure how to compile to <code>.so</code> and link it with correct set of libraries.</p>
<p>First step is to compile <code>hslua</code> in a sandbox, or at least make it reachable by GHC(by installing globally, using nix environments etc.). I’ll be giving commands assuming that you’re in a sandbox that has <code>hslua</code> installed, if you’re not, then just replace <code>cabal exec ghc --</code> part with <code>ghc</code> and it should just work.</p>
<p>Step 1, compile and link the Haskell code to generate a shared library:</p>
<pre><code>$ cabal exec ghc -- LibArith.hs -shared -dynamic -fPIC -o libarith.so -lHSrts-ghc7.8.3</code></pre>
<p>Note that if you’re using a different version of GHC, you’ll need to modify the last argument to make it link it with corrent GHC RTS library.(alternatively, you can link with debug or profiling versions etc.)</p>
<p>Step 2, compile the Lua module written in C(the C code above) and link it with our shared Haskell library:</p>
<pre><code>$ cabal exec ghc -- libarithhelper.c -no-hs-main -optl -larith -o lualibhelper.so -shared -fPIC -dynamic</code></pre>
<p>Note that you may need to pass extra linker parameters if you have Lua library/headers in non-standard locations. If that’s the case, <code>-optl</code> argument of GHC is used to add linker arguments, just use standard linker arguments with that(<code>-L</code>, <code>-I</code> etc.).</p>
<p>This command should print a warning like this:</p>
<pre><code>/home/omer/opt/luajit_bin/include/luajit-2.0/lua.h:168:16:
     note: expected ‘lua_CFunction’ but argument is of type ‘HsInt (*)(void *)’
     LUA_API void  (lua_pushcclosure) (lua_State *L, lua_CFunction fn, int n);</code></pre>
<p>Like mentioned above, this doesn’t make any difference on my x86_64 Linux machine. If that’s being a problem on your system, just wrap your Haskell function in intermediate C code above using Haskell RTS API.</p>
<p>Now you should have two shared libraries, one for our Haskell code and one for the intermediate C code. One problem is that the shared library generated from C is now depending on the one generated from Haskell. So Haskell library should be in your <code>LD_LIBRARY_PATH</code>.</p>
<p>A good improvement here would be to compile Haskell code to static library, and generate one dynamic library only. (which has Haskell library statically linked to it)</p>
<h1 id="loading-the-code-in-lua">Loading the code in Lua</h1>
<p>Before loading it, make sure that the dynamic linker can really find the shared library generated from Haskell. Run this:</p>
<pre><code>$ ldd lualibhelper.so | grep &quot;not found&quot;</code></pre>
<p>Make sure it’s not printing anything.</p>
<p>Now just run Lua and enjoy the library:</p>
<pre><code>$ luajit-2.0.3
LuaJIT 2.0.3 -- Copyright (C) 2005-2014 Mike Pall. http://luajit.org/
JIT: ON CMOV SSE2 SSE3 SSE4.1 fold cse dce fwd dse narrow loop abc sink fuse
&gt; require &quot;lualibhelper&quot;
&gt; hs_init()
&gt; print(add_in_haskell(1, 2))
3
&gt; print(add_in_haskell(-10, 20))
10</code></pre>
<p>Just for the amusement, let’s crash it by running Haskell function <em>after</em> stopping the Haskell runtime:</p>
<pre><code>&gt; hs_exit()
&gt; add_in_haskell(1, 2)
newBoundTask: RTS is not initialised; call hs_init() first</code></pre>
<p>Fun :)</p>
<h1 id="conclusion">Conclusion</h1>
<p>It turns out that extending Lua using Haskell is almost as easy as the doing it using the technique I explained in my <a href="http://osa1.net/posts/2014-04-27-calling-haskell-lua.html">previous blog post on this topic</a>.</p>
<p>This post also demonstrates one other thing, namely, compiling Haskell libraries to shared libraries and dynamically loading them in different programs. I’m hoping that this post helps fellow Haskellers to extend their programs written in different languages with Haskell.</p>]]></summary>
</entry>
<entry>
    <title>GHC + Cabal installation guide for starters</title>
    <link href="http://osa1.net/posts/2014-12-09-ghc-cabal-installation-guide.html" />
    <id>http://osa1.net/posts/2014-12-09-ghc-cabal-installation-guide.html</id>
    <published>2014-09-12T00:00:00Z</published>
    <updated>2014-09-12T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>I see a lot of starters having problems with installing latest GHC and Cabal, and then keeping their package repository in a sane state(e.g. no thousands of broken packages because of one re-install, no installation problems because of conflicts). I came up with a simple method several years ago, and today when combined with sandboxes, that works really well. I suggest every new Haskeller to do the same. Here’s how I do my GHC/Cabal installation up-to-date and sane:</p>
<p>NEVER USE HASKELL PALTFORM. When I first started, it caused just too much pain. I don’t know how it is today, but I presume same problems should still apply. (globally installing packages)</p>
<p>Most important thing is to keep Cabal and cabal-install up-to-date. You can easily remove a GHC, or install other versions and keep them togerher once you have a working and relatively new(so that it supports sandboxes) Cabal. In fact, I currently have 3 versions of GHC installed, for different projects, and I have no problems at all. Starting from a system with no GHC/Cabal is installed, here’s the way to install latest Cabal and cabal-install:</p>
<ol style="list-style-type: decimal">
<li>Install whatever GHC and Cabal you have in your package manager.</li>
<li>If the GHC you installed from package manager is not latest one, install latest pre-compiled binary from <a href="http://www.haskell.org/ghc/">GHC webpage</a>. Set your <code>$PATH</code>(or move executables to <code>$PATH</code>) and remove the GHC you installed using the package manager.</li>
<li>Run <code>cabal update &amp;&amp; cabal install Cabal cabal-install</code>, then remove Cabal and cabal-install installed using the package manager, and add <code>~/.cabal/bin</code> to $PATH. After doing that, you’ll have latest GHC and Cabal installed. Also, Cabal will be installed locally, so you can update it very easily using <code>cabal udpate &amp;&amp; cabal install Cabal cabal-install</code> whenever you want. Updating for newer GHC is similarly easy, just installed pre-compiled binary from the link above and move it wherever you want. You can use multiple GHC installations at the same time without any problems, Cabal just keeps separate repositories for different GHC versions.</li>
</ol>
<p>The worst thing that can happen is that your package manager may not have Cabal at all. In that case you may need Haskell Platform temporarily, for booting GHC and Cabal. Once you have Haskell Platform(which includes Cabal) and run <code>cabal update &amp;&amp; cabal install Cabal cabal-install</code>, just remove Haskell Platform and download latest GHC as mentioned above and go from there.</p>
<p>Occasionally you may want to remove some directories in <code>~/.cabal/lib</code>. Here’s what I have right now:</p>
<pre><code>➜  lib  pwd
/home/omer/.cabal/lib
➜  lib  ls | xargs du -hs
107M    x86_64-linux-ghc-7.6.3
341M    x86_64-linux-ghc-7.8.3
170M    x86_64-linux-ghcjs-0.1.0_ghc-7.8.2
34M     x86_64-linux-ghcjs-0.1.0_ghc-7.8.3</code></pre>
<p>I have 170M of libraries installed in GHCJS compiled with GHC 7.8.2, but I updated my GHCJS installation and I won’t be using that version anymore, so it’s safe to remove that directory. Similarly you may want to remove versions you won’t be using anymore.</p>
<p>Once you have GHC and Cabal installed, you should be very careful with global installations. Basically all you need to do is to use sandboxes as much as possible. You may want or need to have some programs installed globally, like <code>alex</code> and <code>happy</code> and those are fine since they have almost no dependencies at all.</p>
<p>In all other cases, just create a <code>~/bin</code> directory and add it to your <code>$PATH</code>. Now whenever you need a Haskell program in your path, install it in a sandbox, and symlink it to <code>~/bin</code>.</p>
<p>Another very useful tip: You may still have some installation problems because of dependency conflicts. In that cases always try to install with <code>cabal install --allow-newer</code>. Most of the time this sandbox approach + <code>--allow-newer</code> should solve all your problems.</p>
<p>As a last thing, if you still have problems because you installed some libs globally for some reason, you may want to reset your whole Cabal state. In that case, just copy <code>~/.cabal/bin/cabal</code> to somewhere else and remove <code>~/.cabal</code>. Then using copied <code>cabal</code> executable, run <code>cabal update &amp;&amp; cabal install Cabal cabal-install</code> again. Now you have a fresh Cabal state and you can remove copied <code>cabal</code> executable and go with the one just installed at <code>~/.cabal/bin</code>.</p>
<p>I hope this helps.</p>]]></summary>
</entry>

</feed>
