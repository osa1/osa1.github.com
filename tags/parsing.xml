<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>osa1.net - Posts tagged parsing</title>
    <link href="http://osa1.net/tags/parsing.xml" rel="self" />
    <link href="http://osa1.net" />
    <id>http://osa1.net/tags/parsing.xml</id>
    <author>
        <name>Ömer Sinan Ağacan</name>
        <email>omeragacan@gmail.com</email>
    </author>
    <updated>2024-11-29T00:00:00Z</updated>
    <entry>
    <title>Exploring parsing APIs: the cost of recursion</title>
    <link href="http://osa1.net/posts/2024-11-29-how-to-parse-3.html" />
    <id>http://osa1.net/posts/2024-11-29-how-to-parse-3.html</id>
    <published>2024-11-29T00:00:00Z</published>
    <updated>2024-11-29T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>In the <a href="https://osa1.net/posts/2024-11-22-how-to-parse-1.html">first post</a> of this series we looked at a few different ways of parsing a simple JSON-like language. In the <a href="https://osa1.net/posts/2024-11-28-how-to-parse-2.html">second post</a> we implemented a few lexers, and looked at the performance when the parsers from the first post are combined with the lexers in the second post.</p>
<p>One of the surprising results in these posts is that our recursive descent parser, which parses the input directly to an AST, and in some sense is the simplest possible implementation when we need an AST, actually performs the worst.</p>
<p>(The implementations that collect tokens in a vector before parsing perform worse than recursive descent parsing, but those implementations have other issues as well, and can’t be used in many cases. Maybe I should’ve omitted them entirely.)</p>
<p>To keep things simple, let’s consider these three benchmarks from the last post:</p>
<ul>
<li>Recursive descent: 127 Mb/s</li>
<li>tokenize_iter + events_iter to AST: 138 Mb/s</li>
<li>tokenize_push + events_push to AST: 151 Mb/s</li>
</ul>
<p>To recap, the “iter” variants are <code>Iterator</code>s that return one parse (or lexing) event at a time. The “push” variants take a “listener” argument with callbacks for events. See the first post for details.</p>
<p>In the “iter” benchmark, the code that generates the AST iterates an event parser:</p>
<pre><code>fn event_to_tree&lt;I: Iterator&lt;Item = Result&lt;ParseEvent, ParseError&gt;&gt;&gt;(
    parser: &amp;mut I,
    input: &amp;str,
) -&gt; Result&lt;Json, ParseError&gt; {
  ...
}</code></pre>
<p>And the event parser iterates a lexer:</p>
<pre><code>fn parse_events_iter_using_lexer_iter&lt;I: Iterator&lt;Item = Result&lt;(usize, Token), usize&gt;&gt;&gt;(
    lexer: I,
    input_size: usize,
) -&gt; EventParser&lt;I&gt; {
    ...
}</code></pre>
<p>When the AST generator asks for the next parse event, the parse event generator asks for the next token (maybe multiple times) and returns an event. AST generator consumes all of the events and builds the AST.</p>
<p>In the “push” benchmark, we have a “listener” that handles parse events and builds up an AST:</p>
<pre><code>struct AstBuilderListener&lt;&#39;a&gt; {
    input: &amp;&#39;a str,
    container_stack: Vec&lt;Container&gt;,
    current_container: Option&lt;Container&gt;,
    parsed_object: Option&lt;Json&gt;,
    error: Option&lt;ParseError&gt;,
}

impl&lt;&#39;a&gt; EventListener for AstBuilderListener&lt;&#39;a&gt; {
    ...
}</code></pre>
<p>And another listener that handles tokens:</p>
<pre><code>struct LexerEventListenerImpl&lt;&#39;a, L: EventListener&gt; {
    listener: &amp;&#39;a mut L,
    container_stack: Vec&lt;Container&gt;,
    state: ParserState,
}

impl&lt;&#39;a, L: EventListener&gt; LexerEventListener for LexerEventListenerImpl&lt;&#39;a, L&gt; {
  ...
}</code></pre>
<p>This implementation is driven by the lexer which “pushes” the tokens to the event parser, which (sometimes after handling multiple tokens) “pushes” parse events to the AST builder.</p>
<p>Both of these setups are considerably more complicated than recursive descent, yet they perform better. How?</p>
<p>When we consider what the recursive descent parser does that these don’t, it’s kind of obvious. It’s even in the name: recursion.</p>
<p>Our lexers and event parsers all optimize really well: there is no heap allocation anywhere, the code that “pushes” events are all monomorphised based on the handler type, so the handler calls are direct calls and can be (and probably) inlined. There’s also no recursion anywhere.</p>
<p>The recursive descent parser is basically one function that recursively calls itself for nested objects. It turns out this recursion has a cost. When I eliminate the recursion with some more state:</p>
<pre><code>enum ParserState {
    /// Parse any kind of object, update state based on the current container.
    TopLevel,

    /// Parsing a container, parse another element on &#39;,&#39;, or finish the
    /// container on &#39;]&#39; or &#39;}&#39;.
    ExpectComma,

    /// Parsing an object, parse a key.
    ObjectExpectKeyValue,

    /// Parsing an object, parse a key, or terminate the object.
    ObjectExpectKeyValueTerminate,

    /// Parsing an object and we&#39;ve just parsed a key, expect &#39;:&#39;.
    ObjectExpectColon,
}

fn parse_single(iter: &amp;mut Peekable&lt;CharIndices&gt;, input: &amp;str) -&gt; Result&lt;Json, ParseError&gt; {
    let mut container_stack: Vec&lt;Container&gt; = vec![];
    let mut state = ParserState::TopLevel;

    loop {
      ...
    }
}</code></pre>
<p>It performs better than the recursive descent parser and the iterator based parser, and on par with the “push” based parser: (numbers are slightly different than above as I rerun them together)</p>
<ul>
<li>Recursive descent: 127 Mb/s</li>
<li>tokenize_iter + events_iter to AST: 136 Mb/s</li>
<li>tokenize_push + events_push to AST: 158 Mb/s</li>
<li>Direct parser without recursion: 156 Mb/s</li>
</ul>
<p>I’m not quite sure what about recursion that makes the recursive descent parser perform so much worse, but my guess is that it makes the control flow more complicated to analyze, and in runtime, you have to move things around (in registers and stack locations) based on calling conventions. When moving between registers and stack locations you do memory reads and writes. My guess is that when combined, these cost something.</p>
<h1 id="other-considerations-with-recursion">Other considerations with recursion</h1>
<p>If you checkout the git repo and run the tests with <code>cargo test</code>, you will see that a test fails with a stack overflow.</p>
<p>This is something else to keep in mind when parsing recursively. Stack overflows are a real issue with recursive parsing, and I know some libraries that are <a href="https://github.com/google/protobuf.dart/blob/ccf104dbc36929c0f8708285d5f3a8fae206343e/protobuf/lib/src/protobuf/coded_buffer_reader.dart#L29">explicit about it</a>.</p>
<p>In practice though, I’m not sure if this can be the main reason to avoid recursive parsing. Recursion can happen in other places as well, and in a server application you would probably monitor runtime, memory consumption, and maybe even other resources of a handler, and have some kind of error handler that handles everything else.</p>
<p>Some higher level languages like <a href="https://hackage.haskell.org/package/base-4.20.0.1/docs/GHC-IO-Exception.html#t:AsyncException">Haskell</a> and <a href="https://api.dart.dev/dart-core/StackOverflowError-class.html">Dart</a> make stack overflows exceptions/errors that can be caught and handled, so they can be handled as a part of “unexpected” crashes easily. In Rust, stack overflows can be handled at thread boundaries.</p>
<p>If the application is a command line tool or a compiler, where the input is provided by the user and handled on the user’s computer, it’s less of a problem and you can probably just let the application crash.</p>
<p>So I don’t think we can say that recursion should be avoided at all costs when parsing.</p>
<h1 id="references">References</h1>
<p>As usual, the code is available: <a href="https://github.com/osa1/how-to-parse-3">github.com/osa1/how-to-parse-3</a>.</p>
<p>To work around the stack overflow when testing, test in release mode: <code>cargo test --release</code>.</p>
<p>If you want to profile the code and understand more about why one version is faster than the other, I added 4 executables to the package, one for each benchmark listed above. You can generate a 100M input and run the parsers individually with:</p>
<pre><code>$ cargo build --release
...

$ ./target/release/test_gen 100000000 &gt; input

$ time ./target/release/parse_non_recursive input
./target/release/parse_non_recursive input  0.64s user 0.22s system 99% cpu 0.854 total</code></pre>]]></summary>
</entry>
<entry>
    <title>Exploring parsing APIs: adding a lexer</title>
    <link href="http://osa1.net/posts/2024-11-28-how-to-parse-2.html" />
    <id>http://osa1.net/posts/2024-11-28-how-to-parse-2.html</id>
    <published>2024-11-28T00:00:00Z</published>
    <updated>2024-11-28T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>In the <a href="https://osa1.net/posts/2024-11-22-how-to-parse-1.html">previous post</a> we looked at three different parsing APIs, and compared them for runtime and the use cases they support.</p>
<p>In this post we’ll add a lexer (or “tokenizer”), with two APIs, and for each lexer see how the parsers from the previous post perform when combined with the lexer.</p>
<p><strong>What is a lexer?</strong> A lexer is very similar to the event parsers we saw in the previous post, but it doesn’t try to maintain any structure. It generates “tokens”, which are parts of the program that cannot be split into smaller parts. A lexer doesn’t care about parentheses or other delimiters being balanced, or that values in an array are separated by commas, or anything else. It simply splits the input into tokens.</p>
<p><strong>Why is a lexer useful?</strong> If you already have an event parser, adding a lexer may not allow a lot of new use cases. The main use cases that I’m aware of are:</p>
<ul>
<li><p>Syntax highlighting: when higlighting syntax we don’t care about the tree structure, we care about keywords, punctuation (list separators, dots in paths etc.), delimiters (commas, bracets, brackets), and literals. A lexer gives us exactly these and nothing else.</p></li>
<li><p>Supporting incremental parsing: one way of incrementally update an AST is by starting re-lexing a few (often just one) tokens before the edited token, re-lexing until after the edit location, until generating a token identical to an existing token again. AST nodes of modified tokens are then marked as “modified” and re-parsed.</p>
<p>The details are complicated, I recommend chapter 2 of <a href="https://diekmann.uk/diekmann_phd.pdf">this PhD thesis</a> for an introduction to incremental parsing.</p>
<p>If you need to re-parse code as it gets edited, even if you don’t need or want incremental parsing, incremental lexing is easy, it makes sense to re-lex incrementally and then parse from scratch using the incrementally updated token list, because incremental lexing is so simple.</p></li>
<li><p>For separating complex parsing code into smaller parts: modern languages can have complicated literal syntax, with multiple string literals with varying delimiters (like <code>r#"..."#</code> syntax in Rust, or <code>[=[...]=]</code> in Lua), multiple variants of comments (single line and multi-line, documentation and normal), multiple number syntaxes (with different suffixes like <code>123u32</code> in Rust, underscores to separate digits for readability) and so on.</p>
<p>A lexer separates handling of these from the part of the parser that deals with the program structure.</p></li>
</ul>
<h1 id="the-apis">The APIs</h1>
<p>Similar to the previous post, we will look at three different APIs for lexing:</p>
<ul>
<li>A lexer that generates a list of tokens directly: <code>tokenize_list</code>.</li>
<li>An iterator that generates one token at a time: <code>tokenize_iter</code>.</li>
<li>A “push” API that calls “listener” methods for the tokens: <code>tokenize_push</code>.</li>
</ul>
<p>For our simplified (and enhanced, with comments) JSON, our token type is:</p>
<pre><code>pub enum Token {
    Int(u64),
    Str { size_in_bytes: usize },
    True,
    False,
    Null,
    LBracket,
    RBracket,
    LBrace,
    RBrace,
    Colon,
    Comma,
    Comment { size_in_bytes: usize },
}</code></pre>
<p>Similar to our event type from the previous post, this type needs to be cheap to generate (ideally stack allocated).</p>
<p>The tokens are generated along with byte offsets in the input, also similar to events.</p>
<p>For the push API, the listener interface also directly follows the token type:</p>
<pre><code>pub trait LexerEventListener {
    fn handle_int(&amp;mut self, byte_offset: usize, i: u64);

    fn handle_str(&amp;mut self, byte_offset: usize, size_in_bytes: usize);

    // Similar for other token types.
    ...

    fn handle_error(&amp;mut self, byte_offset: usize);
}</code></pre>
<p>To keep things simple, the event handlers don’t return a <code>bool</code> to stop parsing. It can be added in a few lines of code and it doesn’t affect performance.</p>
<p>Unlike the different types of event parsers from the previous post, implementations of these lexer APIs are almost identical. This is because the lexer has only one state, which is the current position in the input. A <code>next</code> call in the iterator implementation simply continues from the current location in the input, and updates the current location as it reads characters from the input.</p>
<p>The entry points are:</p>
<pre><code>pub fn tokenize_iter&lt;&#39;a&gt;(input: &amp;&#39;a str) -&gt; Lexer&lt;&#39;a&gt; { ... }
  // Lexer implements `Iterator`

pub fn tokenize_push&lt;L: LexerEventListener&gt;(input: &amp;str, listener: &amp;mut L) { ... }

pub fn tokenize_list(input: &amp;str) -&gt; Result&lt;Vec&lt;(usize, Token)&gt; usize&gt; { ... }</code></pre>
<h1 id="combining-with-the-event-parsers">Combining with the event parsers</h1>
<p>We have 3 lexers and 2 event parsers, so 6 combinations in total:</p>
<ol type="1">
<li>tokenize_list + parse_events_iter</li>
<li>tokenize_list + parse_events_push</li>
<li>tokenize_iter + parse_events_iter</li>
<li>tokenize_iter + parse_events_push</li>
<li>tokenize_push + parse_events_iter</li>
<li>tokenize_push + parse_events_push</li>
</ol>
<p>However (5) is not easily possible in Rust. The problem is that a push implementation cannot be converted into an iterator, as it will scan the entire input without ever returning and keep calling the listener methods. To convert a push API into an iterator, we need a language feature that allows us to stop the current thread (or maybe a “fiber”, green thread etc.) and resume it later. In Rust, this is possible with <code>async</code> or threads. Threads are expensive, and <code>async</code> requires a lot of refactoring, and all the call sites to be made <code>async</code> as well.</p>
<p>So in this post we won’t consider this combination.</p>
<h1 id="notes-on-implementations">Notes on implementations</h1>
<p>Implementing these combinations is mostly straightforward. Full code is linked below as usual. The takeaways are:</p>
<ul>
<li>The push API cannot be converted into an iterator API, without language features.</li>
<li>The push API requires state management in the consumer: the consumer will have to save the state that needs to be maintained between the calls to the listener methods.</li>
<li>The iterator API is more flexible as it can be converted into a push API.</li>
<li>The iterator API is also easier to use: the consumer can iterate through the elements in nested loops, and needs less state management. The state can also be function locals, instead of fields of a struct (or class etc.).</li>
<li>The list API (generates an entire vector of tokens) only makes sense when you need to collect all of the tokens in memory. The only use case for this that I’m aware of is incremental parsing.</li>
</ul>
<h1 id="references-and-benchmarks">References and benchmarks</h1>
<p>The code (including benchmarks) is here: <a href="https://github.com/osa1/how-to-parse-2">github.com/osa1/how-to-parse-2</a>.</p>
<p><strong>Token generation benchmarks:</strong> Collect all of the tokens in a <code>Vec</code>.</p>
<ul>
<li>tokenize_list: 305 MB/s</li>
<li>tokenize_push: 303 MB/s</li>
<li>tokenize_iter: 329 MB/s</li>
</ul>
<p>In the event generation benchmarks in the last post, the push implementation is about 10% faster than the iterator. But in the lexer, the iterator is faster when collecting the tokens in a vector. It looks like when the state that the parser manages between the <code>next</code> calls gets simpler, the compiler is able to optimize the code better, and iterator implementation beats the push implementation.</p>
<p>The vector generator and push implementation adding the elements to a vector via the listener perform the same, which shows that when monomorphised, the push implementation optimizes quite well for simple cases (but also in complex cases, as we will see below). In languages without monomorphisation, the push API should be slower.</p>
<p><strong>Tokens to events:</strong> Convert tokens to events.</p>
<ul>
<li>events_iter: 282 MB/s</li>
<li>events_push: 315 MB/s</li>
<li>tokenize_list + events_iter: 181 MB/s</li>
<li>tokenize_list + events_push: 187 MB/s</li>
<li>tokenize_iter + events_iter: 269 MB/s</li>
<li>tokenize_iter + events_push: 275 MB/s</li>
<li>tokenize_push + events_push: 351 MB/s</li>
</ul>
<p>The first two benchmarks are the ones from the previous post that don’t use a lexer, generate events directly. The numbers are slightly different than the numbers from the previous post as I rerun them again.</p>
<p>If you need some kind of incremental implementation, scanning the entire input and collecting the events or tokens in a vector performs bad. There’s no point in combining the list API with push or iterator APIs.</p>
<p>What’s surprising is that the push lexer implementation combined with the push event generator implementation performs better than the event generator implementation that parses the input directly without a lexer. I don’t have an explanation to why, yet.</p>
<p>Lexer iterator implementations combined with any of the event generation implementations perform slower than the event push implementation that parses the input directly, but about as fast as the event iterator implementation that parses the input directly.</p>
<p><strong>Tokens to AST:</strong> Converts tokens to events, builds AST from the events.</p>
<ul>
<li>Recursive descent: 127 MB/s</li>
<li>events_iter to AST: 140 MB/s</li>
<li>events_push to AST: 145 MB/s</li>
<li>tokenize_list + events_iter to AST: 108 MB/s</li>
<li>tokenize_list + events_push to AST: 108 MB/s</li>
<li>tokenize_iter + events_iter to AST: 138 MB/s</li>
<li>tokenize_iter + events_push to AST: 139 MB/s</li>
<li>tokenize_push + events_push to AST: 151 MB/s</li>
</ul>
<p>The first three benchmarks below are from the last post. Rerun and included here for comparison.</p>
<p>When we add an AST building step, which is more complicated compared to the rest of steps, the performance difference between the most convenient implementation (tokenize_iter + events_iter to AST) and the most performant one (tokenize_push + events_push to AST) diminishes. In the event generation benchmark, the fast one is 30% faster, but when building an AST, it’s only 9% faster.</p>
<p>The push implementation is still faster than the recursive descent parser, even with the extra lexing step. I’m planning to investigate this further in a future post.</p>]]></summary>
</entry>
<entry>
    <title>Exploring parsing APIs: what to generate, and how</title>
    <link href="http://osa1.net/posts/2024-11-22-how-to-parse-1.html" />
    <id>http://osa1.net/posts/2024-11-22-how-to-parse-1.html</id>
    <published>2024-11-22T00:00:00Z</published>
    <updated>2024-11-22T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>Consider a simplified and enhanced version of JSON, with these changes:</p>
<ul>
<li>Numbers are 64-bit unsigned integers.</li>
<li>Strings cannot have control and escape characters.</li>
<li>Single-line comments are allowed, with the usual syntax: <code>// ...</code> .</li>
</ul>
<p>When parsing a language like this, a common first step if to define an “abstract syntax tree” (AST), with only the details we want from the parser output.</p>
<p>For example, if we’re implementing a tool like <a href="https://jqlang.github.io/jq/">jq</a>, the AST may look like:</p>
<pre><code>enum Json {
    Int(u64),
    Str(String),
    Bool(bool),
    Array(Vec&lt;Json&gt;),
    Object(Vec&lt;(String, Json)&gt;),
    Null,
}</code></pre>
<p>This type is called an “abstract” syntax tree because it abstracts the unnecessary details from the parse output. In our tool we don’t need locations of nodes and comments, so the AST doesn’t contain them.</p>
<p>It’s easy to implement a parser for this AST: we iterate the input, skip whitespace and comments, then based on the next character decide what type of node (integer, string, etc.) to parse. For nested <code>Json</code> nodes in arrays and objects, we recursively call the parser.</p>
<p>This kind of parser is called a “recursive descent parser”. For our AST above, the parser looks like this:</p>
<pre><code>// The entry point: parses all of the input to JSON.
pub fn parse(input: &amp;str) -&gt; Result&lt;Json, JsonParseError&gt; {
    let mut iter = input.char_indices().peekable();
    let (_, json) = parse_single(&amp;mut iter, input)?;
    skip_trivia(&amp;mut iter)?;
    // Check that all of the input is consumed.
    ...
}

// Parse a single Json. After parsing, the input may have more characters to be parsed.
fn parse_single(
    iter: &amp;mut Peekable&lt;CharIndices&gt;,
    input: &amp;str,
) -&gt; Result&lt;(usize, Json), ParseError&gt; {
    // Skip whitespace and comments.
    skip_trivia(iter)?;

    // Get next character.
    let (byte_offset, char) = match iter.next() { ... }

    if char == &#39;[&#39; {
        // Parse an array. Call `parse_single` recursively for elements.
        ...
    }

    if char == &#39;{&#39; {
        // Parse an object. Call `parse_single` recursively for values.
        ...
    }

    if char == &#39;t&#39; {
        // Parse keyword &quot;true&quot;.
        ...
    }

    // Same for other keywords, integers, strings.
    ...
}</code></pre>
<p>While very common, this kind of parsers are inflexible, and slower than more flexible alternatives for many use cases.</p>
<p>Consider these use cases:</p>
<ul>
<li><p>A JSON formatter: a formatter needs to know about comments to be able to keep them in the formatted code. To support this use case, the AST needs to include comments too, which will make it larger, and parsing will be less efficient for the applications that don’t need comments.</p></li>
<li><p>A configuration file parser for a text editor: to be able to show error locations in configuration errors (such as an invalid value used for a setting), the AST will have to include source locations. Similar to above, this will make the AST larger and slower to parse for other applications that don’t need source locations.</p></li>
<li><p>An RPC server that looks at the command name in incoming JSON messages and relays the messages based on the command name: the server doesn’t even need a full parser, just a parser that can keep track of nesting level so that it can extract the request name field at the right level will suffice. Using a full AST parser will parse the whole message and be inefficient.</p></li>
<li><p>A log sorting tool that reads a file with one JSON log per line, sorts the lines based on top-level “timestamp” field values. Similar to the use case above, this tool only needs to read one field and parsing whole lines is wasteful.</p></li>
</ul>
<p>A well-known solution to these is to introduce a lower level parser that doesn’t generate a fully structured output like an AST, but a stream of “parse events”. These events should be general enough to allow different use cases like the ones we listed above, and should be cheap to allocate and pass around, ideally as stack allocated values, so that applications that don’t need them can skip them efficiently.</p>
<p>This type of parsing is often called “event driven parsing”. In our JSON variant, the events look like this:</p>
<pre><code>/// A parse event, with location of the event in the input.
pub struct ParseEvent {
    pub kind: ParseEventKind,
    pub byte_offset: usize,
}

/// Details of a parse event.
pub enum ParseEventKind {
    StartObject,
    EndObject,
    StartArray,
    EndArray,
    Int(u64),
    Str {
        /// Size of the string, not including the double quotes.
        size_in_bytes: usize,
    },
    Bool(bool),
    Null,
    Comment {
        /// Size of the comment, including the &quot;//&quot; a the beginning and newline at the end.
        size_in_bytes: usize,
    },
}</code></pre>
<p>Note that there’s no heap allocation required for these events. Contents of strings and comments can be obtained by slicing the input using the event location and <code>size_in_bytes</code> field.</p>
<p>When generating these event, it’s important that we don’t scan the whole input and collect all of the events in a list, as that would mean some of the users, like our RPC server and log sorted examples above, would have to do more work than necessary.</p>
<p>This means that the parser will have to be stateful: after returning an event, it needs to be able to continue from the last event location. This complicates the parser implementation quite a bit. Here’s how the parser looks like at a high level:</p>
<pre><code>// The entry point. Use via the `Iterator` interface.
pub fn parse_events(input: &amp;str) -&gt; EventParser {
    EventParser::new(input)
}

// The parser state.
pub struct EventParser&lt;&#39;a&gt; {
    input: &amp;&#39;a str,
    byte_offset: usize,
    container_stack: Vec&lt;Container&gt;,
    state: ParserState,
}

enum Container {
    Array,
    Object,
}

enum ParserState {
    /// Parse any kind of object, update state based on the current container.
    TopLevel,

    /// Finished parsing a top-level object, expect end-of-input.
    Done,

    /// Parsing an object, parse another element on &#39;,&#39;, or finish the array on &#39;}&#39;.
    ObjectExpectComma,

    /// Parsing an object, parse the first element, or finish the array on &#39;]&#39;.
    ObjectExpectKeyValue,

    /// Parsing an object and we&#39;ve just parsed a key, expect &#39;:&#39;.
    ObjectExpectColon,

    /// Parsing an array, parse another element on &#39;,&#39;, or finish the array on &#39;]&#39;.
    ArrayExpectComma,
}

impl&lt;&#39;a&gt; Iterator for EventParser&lt;&#39;a&gt; {
    type Item = Result&lt;ParseEvent, ParseError&gt;;

    fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt; {
        match self.state {
            ParserState::TopLevel =&gt; self.top_level(),
            ParserState::Done =&gt; self.done(),
            ParserState::ObjectExpectComma =&gt; self.object_expect_comma(),
            ParserState::ObjectExpectKeyValue =&gt; self.object_expect_key_value(),
            ParserState::ObjectExpectColon =&gt; self.object_expect_colon(),
            ParserState::ArrayExpectComma =&gt; self.array_expect_comma(),
        }
    }
}

...</code></pre>
<p>The main complexity of this parser comes from the fact that it cannot return an event and keep running, the caller needs to call the relevant method (<code>next</code> from the <code>Iterator</code> trait above) to keep parsing. To be able to continue from where it’s left, the parser needs to maintain some state outside of the parse functions.</p>
<p>This parser is general enough to allow implementing our original AST parser:</p>
<pre><code>pub fn event_to_tree&lt;I: Iterator&lt;Item = Result&lt;ParseEvent, ParseError&gt;&gt;&gt;(
    parser: &amp;mut I,
    input: &amp;str,
) -&gt; Result&lt;Json, ParseError&gt; {
    let mut container_stack: Vec&lt;Container&gt; = vec![];
    let mut current_container: Option&lt;Container&gt; = None;
    let mut parsed_object: Option&lt;Json&gt; = None;

    for event in parser.by_ref() {
        match event {
            ...
        }
    }

    Ok(parsed_object.unwrap())
}</code></pre>
<p>But it also allows parsing to an AST with comments (for our formatter), source locations (for our configuration parser), and our RPC server and log sorter. Here’s how the timestamp parser that stops after finding the field looks like:</p>
<pre><code>/// Parse the &quot;timestamp&quot; field at the top-level map of the JSON.
pub fn parse_timestamp(log_line: &amp;str) -&gt; Result&lt;Option&lt;u64&gt;, ParseError&gt; {
    let mut container_depth: u32 = 0;
    let mut expect_timestamp = false;

    for event in parse_events(log_line) {
        let ParseEvent { kind, byte_offset } = match event {
            Ok(event) =&gt; event,
            Err(err) =&gt; return Err(err),
        };

        let expect_timestamp_ = expect_timestamp;
        expect_timestamp = false;

        match kind {
            ParseEventKind::StartObject =&gt; {
                container_depth += 1;
            }

            ParseEventKind::EndObject =&gt; {
                container_depth -= 1;
            }

            ParseEventKind::StartArray =&gt; {
                if container_depth == 0 {
                    // Array at the top level, the line does not contain the field.
                    return Ok(None);
                }
                container_depth += 1;
            }

            ParseEventKind::EndArray =&gt; {
                container_depth -= 1;
            }

            ParseEventKind::Str { size_in_bytes } =&gt; {
                if container_depth != 1 {
                    continue;
                }
                let str = &amp;log_line[byte_offset..byte_offset + size_in_bytes];
                expect_timestamp = str == &quot;timestamp&quot;;
            }

            ParseEventKind::Int(i) =&gt; {
                if expect_timestamp_ {
                    return Ok(Some(i));
                }
            }

            ParseEventKind::Bool(_)
            | ParseEventKind::Null
            | ParseEventKind::Comment { .. } =&gt; {}
        }
    }

    Ok(None)
}</code></pre>
<p>A nice property of this parser is that it does not allocate at all. It doesn’t build an AST (so no heap-allocated vectors), and parse events are 24-byte stack allocated values. The event parser is also stack allocated by this function.</p>
<p>An alternative design to this that is slightly less flexible and more difficult to use, but easier to implement and faster is what’s sometimes called a “push parser”.</p>
<p>The idea is that, instead of returning one event at a time, the parser takes a “listener” argument, and calls the listener callbacks for each event generated. The listener type directly follows our event type above:</p>
<pre><code>// Methods return a `bool` indicating whether to continue parsing after the event.
pub trait EventListener {
    fn handle_start_object(&amp;mut self, _byte_offset: usize) -&gt; bool {
        true
    }

    fn handle_end_object(&amp;mut self, _byte_offset: usize) -&gt; bool {
        true
    }

    fn handle_start_array(&amp;mut self, _byte_offset: usize) -&gt; bool {
        true
    }

    fn handle_end_array(&amp;mut self, _byte_offset: usize) -&gt; bool {
        true
    }

    fn handle_int(&amp;mut self, _byte_offset: usize, _i: u64) -&gt; bool {
        true
    }

    fn handle_str(&amp;mut self, _byte_offset: usize, _size_in_bytes: usize) -&gt; bool {
        true
    }

    fn handle_bool(&amp;mut self, _byte_offset: usize, _b: bool) -&gt; bool {
        true
    }

    fn handle_null(&amp;mut self, _byte_offset: usize) -&gt; bool {
        true
    }

    fn handle_comment(&amp;mut self, _byte_offset: usize, _size_in_bytes: usize) -&gt; bool {
        true
    }

    fn handle_error(&amp;mut self, _error: ParseError);
}</code></pre>
<p>The parser:</p>
<pre><code>// The entry point. Parse all of the input, call `listener` with the events.
pub fn parse&lt;L: EventListener&gt;(input: &amp;str, listener: &amp;mut L) {
    let mut iter = input.char_indices().peekable();
    let input_size = input.len();

    // Parse a single JSON.
    if !parse_single(&amp;mut iter, input_size, listener) {
        return;
    }

    // Check that all of the input is consumed.
    ...
}

// Returns whether an error was reported.
fn parse_single&lt;L: EventListener&gt;(
    iter: &amp;mut Peekable&lt;CharIndices&gt;,
    input_size: usize,
    listener: &amp;mut L,
) -&gt; bool {
    // Skip whitespace and comments, generate events for comments.
    skip_trivia!(iter, listener);

    // Get next character.
    let (byte_offset, char) = match iter.next() {
        Some(next) =&gt; next,
        None =&gt; {
            listener.handle_error(ParseError {
                byte_offset: input_size,
                reason: &quot;unexpected end of input&quot;,
            });
            return false;
        }
    };

    if char == &#39;[&#39; {
        // Parse an array. Call `parse_single` recursively for elements.
        ...
    }

    if char == &#39;{&#39; {
        // Parse an object. Call `parse_single` recursively for values.
        ...
    }

    if char == &#39;t&#39; {
        // Parse keyword &quot;true&quot;.
        ...
    }

    // Same for other keywords, integers, strings.
    ...
}</code></pre>
<p>Note that the parser functions are identical (in terms of names and what they do) to our simple recursive descent parser. This is because the parser no longer needs to maintain state to be able to return and continue from where it was left, as it does all of the work in one go. Instead of building an AST or a list of events, it takes an <code>EventListener</code> argument and calls the handle methods.</p>
<p>This is a bit less convenient to use, but it’s still flexible enough to build an AST. An <code>EventListener</code> implementation that builds up a <code>Json</code> AST looks like this:</p>
<pre><code>pub struct AstBuilderListener&lt;&#39;a&gt; {
    input: &amp;&#39;a str,
    container_stack: Vec&lt;Container&gt;,
    current_container: Option&lt;Container&gt;,
    parsed_object: Option&lt;Json&gt;,
    error: Option&lt;ParseError&gt;,
}

impl&lt;&#39;a&gt; EventListener for AstBuilderListener&lt;&#39;a&gt; {
    ...
}</code></pre>
<p>However, if you need to be able to stop parsing and continue later, this parser can’t do that.</p>
<p>The main advantage of this parser is that, with the right programming language and parser design, it can be faster than the alternatives, while still being flexible enough for most use cases. See below for benchmarks.</p>
<hr />
<h1 id="aside-event-parsing-vs.-lexing">Aside: event parsing vs. lexing</h1>
<p>Our <code>ParseEvent</code> type has no nested data and looks like what we could define as the “tokens” in a parser for a programming language.</p>
<p>So it shouldn’t be surprising that we can use a lexer generator to implement a parse event generator:</p>
<pre><code>// Same `parse_events` as above, but uses a generated lexer.
pub fn parse_events(input: &amp;str) -&gt; LexgenIteratorAdapter {
    LexgenIteratorAdapter {
        lexer: Lexer::new(input),
    }
}

// An adapter is necessary to convert lexgen values to `parse_events` items.
pub struct LexgenIteratorAdapter&lt;&#39;a&gt; {
    lexer: Lexer&lt;&#39;a, std::str::Chars&lt;&#39;a&gt;&gt;,
}

impl&lt;&#39;a&gt; Iterator for LexgenIteratorAdapter&lt;&#39;a&gt; {
    type Item = Result&lt;ParseEvent, ParseError&gt;;

    fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt; {
        ...
    }
}

struct LexerState {
    container_stack: Vec&lt;Container&gt;,
}

lexgen::lexer! {
    Lexer(LexerState) -&gt; ParseEvent;

    type Error = &amp;&#39;static str;

    let comment = &quot;//&quot; (_ # &#39;\n&#39;)* &#39;\n&#39;;

    rule Init {
        $$ascii_whitespace,

        $comment =&gt; comment,

        &#39;[&#39; =&gt; ...,

        &#39;]&#39; =&gt; ...,

        &#39;{&#39; =&gt; ...,

        &quot;true&quot; =&gt; ...,

        &quot;false&quot; =&gt; ...,

        &quot;null&quot; =&gt; ...,

        [&#39;0&#39;-&#39;9&#39;]+ =&gt; ...,

        &#39;&quot;&#39; (_ # &#39;&quot;&#39;)* &#39;&quot;&#39; =&gt; ...
    }

    rule Done { ... }

    rule ArrayExpectComma { ... }

    rule ObjectExpectKeyValue { ... }

    rule ObjectExpectColon { ... }

    rule ObjectExpectComma { ... }
}</code></pre>
<p>This uses <a href="https://github.com/osa1/lexgen">lexgen</a>. lexgen generates slightly different values than what we want, so we have a <code>map</code> in the entry point to convert the lexgen values.</p>
<p>The main difference between an event parser and lexer is that an event parser maintains some of the structure of the parsed format. For example, we check that brackets are balanced, after a key in a map a colon follows, and so on.</p>
<p>A lexer generator can be used to implement an event parser, as demonstrated above.</p>
<hr />
<h1 id="references-and-benchmarks">References and benchmarks</h1>
<p>All of the code in this blog post, and more, is here: <a href="https://github.com/osa1/how-to-parse">github.com/osa1/how-to-parse</a>.</p>
<p>In the benchmark program (run with <code>cargo bench</code>), we generate a 10M large JSON, and parse it to either an AST or a vector of events.</p>
<p><strong>AST building benchmarks:</strong></p>
<ul>
<li><p>Recursive descent: the recursive descent parser that generates an AST.</p>
<p>Throughput: 128 Mb/s.</p></li>
<li><p>Event generator to AST: the iterator-style event generator, events processed by <code>event_to_tree</code> to build an AST.</p>
<p>Throughput: 138 Mb/s.</p></li>
<li><p>Lexgen event to AST: same as above, but the event parser is implemented with lexgen.</p>
<p>Throughput: 106 Mb/s.</p></li>
<li><p>Push event parser to AST: the “push” event parser, <code>AstBuilderListener</code> as the event listener.</p>
<p>Throughput: 147 Mb/s.</p></li>
</ul>
<p><strong>Event generation benchmarks:</strong> (collect events in a <code>Vec</code>)</p>
<ul>
<li><p>Parse events: the iterator-style event generator.</p>
<p>Throughput: 274 Mb/s.</p></li>
<li><p>Parse events lexgen: the lexgen-generated event generator.</p>
<p>Throughput: 179 Mb/s.</p></li>
<li><p>Parse events via push: the push event parser, events added to a <code>Vec</code> via by the listener.</p>
<p>Throughput: 304 Mb/s.</p></li>
</ul>
<p><strong>Notes:</strong></p>
<ul>
<li><p>lexgen-generated event parser is the slowest, but I think it should be possible to make it perform at least as good as the hand-written one. So far I’ve spent very little time to optimize lexgen’s code generator.</p></li>
<li><p>Push-based implementation is faster than the iterator-style implementation, both for generating events in a list, and also for building an AST.</p>
<p>The main advantage of the push-based implementation is that the control flow is as simple as the recursive descent parsing (contained within parse functions, as opposed to externally in a struct), as it does all of the parsing in one go. It looks like managing the parser state externally in a struct is not free.</p></li>
<li><p>I think the tradeoffs between the push-based and iterator implementations will be different in most high-level languages without control over allocations and monomorphisation.</p>
<ul>
<li><p>In the Rust implementation, events are stack allocated values, which will be heap-allocated objects in some of the other languages.</p></li>
<li><p>In the push-based implementation, the parser is monomorphised based on the listener type. Both the listener and parser are stack allocated. All event handler method calls are direct calls (as opposed to virtual, or via some other dynamic invocation method), which can be inlined. None of these will be the case in, e.g., Haskell and Dart.</p></li>
</ul>
<p>It would be interesting to implement the same in some other languages to see how they perform relative to each other.</p></li>
<li><p>I’m not sure why the recursive descent parser is not at least as fast as the push-based implementation, and not faster than the iterator-style one. If you have any insights into this, please let me know.</p></li>
</ul>
<h1 id="more-use-cases">More use cases</h1>
<p>The use cases described at the beginning of the post are all extracted from real-world use cases of various other formats.</p>
<p>Here are more use cases that require flexible and fast parser design:</p>
<ul>
<li><p>“Outline” views in text editors or online code browsing tools may want to process top-level definitions, and definitions nested in <code>class</code>, <code>impl</code>, and similar blocks. Parsing the whole file to an AST would be inefficient.</p></li>
<li><p>Syntax-aware code search tools like <a href="https://github.com/osa1/sg">sg</a> can implement searching only in identifiers, string literals, comments with an event-based parser. This could also be implemented with a lexer.</p></li>
<li><p>As mentioned in a <a href="https://osa1.net/posts/2024-11-04-resumable-exceptions.html">previous post</a>, ideally a formatter, language server, compiler, and refactoring tools, should reuse as much parsing code as possible. It’s difficult to do this with an AST parser, as the AST would have too much information for each of these tools. Event-based parsing makes this easier.</p></li>
</ul>
<h1 id="event-parsing-examples-from-programming-languages">Event parsing examples from programming languages</h1>
<p>I think event-driven parsing is common in some languages when parsing data formats like XML, but less common for parsing programming languages. Two examples that I’m aware of that applies the ideas to programming languages:</p>
<ul>
<li><p>rust-analyzer’s parser is <a href="https://github.com/rust-lang/rust-analyzer/blob/c0bbbb3e5d7d1d1d60308c8270bfd5b250032bb4/docs/dev/architecture.md#cratesparser">a hand written one that generates events</a>. The architecture documentation mentions that Kotlin uses a similar idea:</p>
<blockquote>
<p>It is a hand-written recursive descent parser, which produces a sequence of events like “start node X”, “finish node Y”. It works similarly to kotlin’s parser, which is a good source of inspiration for dealing with syntax errors and incomplete input</p>
</blockquote></li>
<li><p>Dart’s parser <a href="https://github.com/dart-lang/sdk/blob/19da943583e020e96026f797904dc5c6b993d4ac/pkg/_fe_analyzer_shared/lib/src/parser/listener.dart#L35-L46">uses the push-based API</a>. This parser is the only Dart language parser used by the SDK. It’s used by the analyzer, language server, compilers, and anything else that the SDK includes.</p></li>
</ul>]]></summary>
</entry>
<entry>
    <title>Top-down expression parsing is easy</title>
    <link href="http://osa1.net/posts/2015-01-29-top-down-expr-parsing-easy.html" />
    <id>http://osa1.net/posts/2015-01-29-top-down-expr-parsing-easy.html</id>
    <published>2015-01-29T00:00:00Z</published>
    <updated>2015-01-29T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>I recently fixed <a href="http://hackage.haskell.org/package/language-lua">language-lua</a>’s 2-years-old expression parsing bug. Previously it was using <a href="http://hackage.haskell.org/package/parsec-3.1.8/docs/Text-Parsec-Expr.html">Parsec’s expression parser</a>, which is actually horrible because it can’t handle chained unary operators.</p>
<p>Two weeks ago I decided to take a look into Lua’s original implementation, and in about an hour or so the algorithm was crystal clear to me. I immediately <a href="https://github.com/osa1/language-lua/commit/b4bebe36e927dcc671dbe6dd19572b83073dc556#diff-630bbd2d118baf109da6ad79d3f168bfR257">implemented it</a> and closed the <a href="https://github.com/osa1/language-lua/issues/2">2-years-old bug report</a>.</p>
<p>This implementation is essentially a port of Lua’s expression parser. Recently I thought about the algorithm and I was wondering if this has a name – the algorithm looked pretty obvious to me once I understand and given how much we know about parsing I thought this should have a name.</p>
<p>I found <a href="http://www.engr.mun.ca/~theo/Misc/exp_parsing.htm#climbing">this algorithm named “precedence climbing”</a>. This is almost the same algorithm, only difference is that instead of using <code>lookahead</code> I’m just consuming the binary operator and returning it to the caller(which is parsing an expression with lower precedence than current parser) if precedence is lower. Associativity handling is also different(I use different left and right precedences to handle associativity) but the idea is really the same.</p>
<p>Now, there is also another algorithm called Pratt, and I can’t read the original paper(paywall), but according to <a href="http://lambda-the-ultimate.org/node/3682">this LtU discussion</a> it should also be similar. Indeed, <a href="http://journal.stuffwithstuff.com/2011/03/19/pratt-parsers-expression-parsing-made-easy/">this explanation of it</a> looks pretty similar, and <a href="http://stackoverflow.com/a/13637731/691032">this StackOverflow answer</a> says that Lua’s implementation is “Pratt style parsing”.</p>
<p>So it seems like we have two, or maybe one since they’re actually very similar, solution(s) to solve top-down expression parsing problem and Haskell implementation using Parsec is possible in only 12 lines of code.</p>
<h1 id="a-challenge">A challenge</h1>
<p>One challenge might be to modify Parsec’s expression parser so that internally it generates a Pratt/precedence climbing parser. I’m hoping to spare some time to work on this.</p>]]></summary>
</entry>
<entry>
    <title>An idea to handle left-recursion in Parsec</title>
    <link href="http://osa1.net/posts/2014-03-07-parsec-left-recursion.html" />
    <id>http://osa1.net/posts/2014-03-07-parsec-left-recursion.html</id>
    <published>2014-03-07T00:00:00Z</published>
    <updated>2014-03-07T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>I recently realized that it may be possible to handle left-recursion in Parsec style parser combinator libraries. I quickly wrote a simple prototype implementation that demonstrates the idea.</p>
<p>The idea is to keep track of parser functions that are called without consuming any tokens from input stream. You should be able to run failure procedures when same parser function is encountered more than one time without consuming any tokens. When a token is consumed, state that is used to keep track of parser functions should be reset.</p>
<p>One assumption here is that I’m assuming parser functions do not alter any states. Otherwise when you come across a same parser, you can have different state and parser may behave differently.</p>
<p>Implementing the idea is easy even without having any extra support from Parsec. Here’s a demonstration on my favorite ambiguous grammar:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true"></a><span class="co">-- Exp ::= Int</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true"></a><span class="co">--      |  Exp `+` Exp</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true"></a><span class="kw">data</span> <span class="dt">Exp</span> <span class="ot">=</span> <span class="dt">Int</span> <span class="dt">Int</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true"></a>         <span class="op">|</span> <span class="dt">Add</span> <span class="dt">Exp</span> <span class="dt">Exp</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true"></a>         <span class="kw">deriving</span> (<span class="dt">Show</span>)</span></code></pre></div>
<p>Parsers for non-terminals are easy:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true"></a>plus <span class="ot">=</span> char <span class="ch">&#39;+&#39;</span> <span class="op">&gt;&gt;</span> spaces</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true"></a>int  <span class="ot">=</span> <span class="fu">fmap</span> (<span class="dt">Int</span> <span class="op">.</span> <span class="fu">read</span>) <span class="op">$</span> many1 digit <span class="op">&lt;*</span> spaces</span></code></pre></div>
<p>I’m using a set to keep track of already visited parsers:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true"></a><span class="kw">type</span> <span class="dt">MarksSeen</span> <span class="ot">=</span> <span class="dt">S.Set</span> <span class="dt">Int</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true"></a><span class="kw">type</span> <span class="dt">RecParser</span> s m a <span class="ot">=</span> <span class="dt">ParsecT</span> s <span class="dt">MarksSeen</span> m a</span></code></pre></div>
<p>Auxiliary functions to alter the state that keeps track of visited parsers:</p>
<pre><code>putMark i = do
    is &lt;- getState
    if S.member i is
      then fail &quot;recursion&quot;
      else putState $ S.insert i is

resetMarks = putState S.empty</code></pre>
<p>Now the interesting part, <code>add</code> parser marks itself as first thing to do, and calls <code>exp</code> parser. Since <code>exp</code> parser is entry point, this means an indirect recursive call. When same <code>putMark</code> call is made, Parsec runs failure actions instead of going into infinte loop:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true"></a>add <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true"></a>    putMark <span class="dv">1</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true"></a>    e1 <span class="ot">&lt;-</span> <span class="fu">exp</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true"></a>    resetMarks</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true"></a>    plus</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true"></a>    e2 <span class="ot">&lt;-</span> <span class="fu">exp</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true"></a>    spaces</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true"></a>    <span class="fu">return</span> <span class="op">$</span> <span class="dt">Add</span> e1 e2</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true"></a><span class="fu">exp</span> <span class="ot">=</span> choice [try add, int]</span></code></pre></div>
<p><code>resetMarks</code> call is also important, <code>exp</code> has to consume some tokens, so after parsing <code>e1</code>, I’m calling <code>resetMarks</code>.</p>
<p>Here’s an example call of this parser:</p>
<pre><code>ghci&gt; runParser exp S.empty &quot;&quot; &quot;1 + 2 + 3 + 4&quot;
Right (Add (Int 1) (Add (Int 2) (Add (Int 3) (Int 4))))</code></pre>
<p>You can observe that parser gets into an infinite loop when marks are removed. Here’s an example demonstrating the error message:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true"></a>rec <span class="ot">=</span> putMark <span class="dv">0</span> <span class="op">&gt;&gt;</span> rec <span class="op">&gt;&gt;</span> resetMarks</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true"></a>ghci<span class="op">&gt;</span> runParser rec S.empty <span class="st">&quot;&quot;</span> <span class="st">&quot;&quot;</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true"></a><span class="dt">Left</span> (line <span class="dv">1</span>, column <span class="dv">1</span>)<span class="op">:</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true"></a>recursion</span></code></pre></div>
<p>One problem with this approach is that it requires more typing, and you should be careful too. Marks can be placed using TemplateHaskell to ensure unique numbers are given to each <code>putMark</code> call. As a second improvement, I think with some modifications on Parsec we can make Parsec to reset marks when a token is consumed(using <em>consumed-ok</em> continuation of <code>ParsecT</code>).</p>
<p>You can see the complete program <a href="https://gist.github.com/osa1/9414577">here</a>.</p>
<hr />
<p>Removing left-recursions in your grammar may not be a huge problem – except when you’re working on functional languages with ML-like syntax. Then you’re out of luck because being functional means you’re <code>Exp</code> non-terminal contains several dozen of productions and function applications is a part of that too, and it’s left-recursive:</p>
<pre><code>Exp ::= ...
     |  Exp Exp_1 ... Exp_N [left-associative]
     ... a hundred more productions ...</code></pre>
<p>Whenever I need to write a parser for a grammar like this, I’m thinking for an easier way to parse it. I still couldn’t come up with a solution. Idea I just explained does not solve it, because it parses right-associatively. There is one workaround, but I’m not sure if that results with a parser for same grammar:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true"></a>app <span class="ot">=</span> <span class="kw">do</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true"></a>    putMark <span class="dv">2</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true"></a>    fn <span class="ot">&lt;-</span> <span class="fu">exp</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true"></a>    resetMarks</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true"></a></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true"></a>    putMark <span class="dv">2</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true"></a>    as <span class="ot">&lt;-</span> many1 <span class="fu">exp</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true"></a>    resetMarks</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true"></a></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true"></a>    <span class="fu">return</span> <span class="op">$</span> <span class="fu">foldl</span> <span class="dt">App</span> fn as</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true"></a></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true"></a></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true"></a>ghci<span class="op">&gt;</span> runParser pgm S.empty <span class="st">&quot;&quot;</span> <span class="st">&quot;1 2 3 4 + 5 + 6 7&quot;</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true"></a><span class="dt">Right</span> (<span class="dt">Add</span> (<span class="dt">App</span> (<span class="dt">App</span> (<span class="dt">App</span> (<span class="dt">Int</span> <span class="dv">1</span>) (<span class="dt">Int</span> <span class="dv">2</span>)) (<span class="dt">Int</span> <span class="dv">3</span>)) (<span class="dt">Int</span> <span class="dv">4</span>)) (<span class="dt">Add</span> (<span class="dt">Int</span> <span class="dv">5</span>) (<span class="dt">App</span> (<span class="dt">Int</span> <span class="dv">6</span>) (<span class="dt">Int</span> <span class="dv">7</span>))))</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true"></a></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true"></a>ghci<span class="op">&gt;</span> runParser pgm S.empty <span class="st">&quot;&quot;</span> <span class="st">&quot;1 + 2 + 3 4&quot;</span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true"></a><span class="dt">Right</span> (<span class="dt">Add</span> (<span class="dt">Int</span> <span class="dv">1</span>) (<span class="dt">Add</span> (<span class="dt">Int</span> <span class="dv">2</span>) (<span class="dt">App</span> (<span class="dt">Int</span> <span class="dv">3</span>) (<span class="dt">Int</span> <span class="dv">4</span>))))</span></code></pre></div>]]></summary>
</entry>
<entry>
    <title>Memoized parsing in continuation-passing style</title>
    <link href="http://osa1.net/posts/2013-10-21-memoized-parsing-cps.html" />
    <id>http://osa1.net/posts/2013-10-21-memoized-parsing-cps.html</id>
    <published>2013-10-21T00:00:00Z</published>
    <updated>2013-10-21T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>Continuations are truly magical things. They’re the “ultimate abstractions of control flow”. Even without using any fancy language features like <code>call/cc</code>, you can have seriously cool and mind-boggling programs.</p>
<p>In <a href="http://arxiv.org/pdf/cmp-lg/9504016">“Memoization in Top-Down Parsing”</a> paper, Mark Johnson builds up from memoizing top-down parsers and describes a way to handle left recursion in top-down parsers by combining memoization techniques with continuations.</p>
<p>I ported the code to Lua to experiment, you can see it <a href="https://gist.github.com/osa1/7089333">here</a>. Most interesting part is the memoized CPS parser generator from a normal CPS parser function:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode lua"><code class="sourceCode lua"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true"></a><span class="kw">function</span> memo<span class="op">(</span>parser<span class="op">)</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true"></a>    <span class="co">-- </span><span class="al">WARNING</span><span class="co">: this function is badly implemented in the sense that</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true"></a>    <span class="co">-- if you parser generated by this function on two different streams</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true"></a>    <span class="co">-- it will generate wrong results</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true"></a>    <span class="kw">local</span> tbl <span class="op">=</span> <span class="op">{}</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true"></a>    <span class="cf">return</span> <span class="kw">function</span> <span class="op">(</span>stream<span class="op">,</span> idx<span class="op">,</span> cont<span class="op">)</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true"></a>        <span class="cf">if</span> tbl<span class="op">[</span>idx<span class="op">]</span> <span class="op">==</span> <span class="kw">nil</span> <span class="cf">then</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true"></a>            tbl<span class="op">[</span>idx<span class="op">]</span> <span class="op">=</span> <span class="op">{</span> results <span class="op">=</span> <span class="op">{},</span> conts <span class="op">=</span> <span class="op">{}</span> <span class="op">}</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true"></a>            <span class="fu">table.insert</span><span class="op">(</span>tbl<span class="op">[</span>idx<span class="op">].</span>conts<span class="op">,</span> cont<span class="op">)</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true"></a>            parser<span class="op">(</span>stream<span class="op">,</span> idx<span class="op">,</span> <span class="kw">function</span> <span class="op">(</span>parse_result<span class="op">)</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true"></a>                <span class="co">-- check if same parse_result is already in the table</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true"></a>                <span class="kw">local</span> exists <span class="op">=</span> <span class="kw">false</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true"></a>                <span class="cf">for</span> <span class="cn">_</span><span class="op">,</span> result <span class="kw">in</span> <span class="fu">ipairs</span><span class="op">(</span>tbl<span class="op">[</span>idx<span class="op">].</span>results<span class="op">)</span> <span class="cf">do</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true"></a>                    <span class="cf">if</span> result <span class="op">==</span> parse_result <span class="cf">then</span> <span class="co">-- </span><span class="al">TODO</span><span class="co">: this equality is probably wrong</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true"></a>                        exists <span class="op">=</span> <span class="kw">true</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true"></a>                        <span class="cf">break</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true"></a>                    <span class="cf">end</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true"></a>                <span class="cf">end</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true"></a> </span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true"></a>                <span class="cf">if</span> <span class="kw">not</span> exists <span class="cf">then</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true"></a>                    <span class="fu">table.insert</span><span class="op">(</span>tbl<span class="op">[</span>idx<span class="op">].</span>results<span class="op">,</span> parse_result<span class="op">)</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true"></a>                    <span class="cf">for</span> <span class="cn">_</span><span class="op">,</span> cont <span class="kw">in</span> <span class="fu">ipairs</span><span class="op">(</span>tbl<span class="op">[</span>idx<span class="op">].</span>conts<span class="op">)</span> <span class="cf">do</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true"></a>                        cont<span class="op">(</span>parse_result<span class="op">)</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true"></a>                    <span class="cf">end</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true"></a>                <span class="cf">end</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true"></a>            <span class="kw">end</span><span class="op">)</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true"></a>        <span class="cf">else</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true"></a>            <span class="fu">table.insert</span><span class="op">(</span>tbl<span class="op">[</span>idx<span class="op">].</span>conts<span class="op">,</span> cont<span class="op">)</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true"></a>            <span class="cf">for</span> <span class="cn">_</span><span class="op">,</span> result <span class="kw">in</span> <span class="fu">ipairs</span><span class="op">(</span>tbl<span class="op">[</span>idx<span class="op">].</span>results<span class="op">)</span> <span class="cf">do</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true"></a>                cont<span class="op">(</span>result<span class="op">)</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true"></a>            <span class="cf">end</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true"></a>        <span class="cf">end</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true"></a>        <span class="cf">return</span> tbl<span class="op">[</span>idx<span class="op">]</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true"></a>    <span class="kw">end</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true"></a><span class="kw">end</span></span></code></pre></div>
<p>(btw, I found porting this code to a purely functional setting very hard thing to do. If you find a way to do this, please send me your code. Thanks.)</p>
<p>This piece of code didn’t make sense to me for a while. I think the key to understand this function is to find answer to this question:</p>
<p>How is this different from keeping a set of productions visited without consuming any input from input stream and when you come to the same production, just failing instead of trying to parse? Because trying to derive same production without consuming any input means you’ll end up with infinite loop.</p>
<p>This function different in that it accounts for parsing same production after following a different path of production. Think this CFG as an example:</p>
<pre><code>T ::= T + T
    | int</code></pre>
<p>In order to derive first production, it first needs to parse a <code>T</code>. But then it will be already noted that it was already trying to parse <code>T</code>, and add the continuation to the list of continuations to be called when a T at input position 1 is parsed.</p>
<p>While trying alternatives, it will parse an <code>int</code>, and derive <code>T -&gt; int</code> at input position 1. And since it had saved the continuations to call when it successfully parse a <code>T</code> at location 1, it will call this continuations and thus parsing will continue.</p>
<p>I hope this helps other people to understand the trick.</p>]]></summary>
</entry>

</feed>
