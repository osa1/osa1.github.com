<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>osa1.net - Posts tagged en</title>
    <link href="http://osa1.net/tags/en.xml" rel="self" />
    <link href="http://osa1.net" />
    <id>http://osa1.net/tags/en.xml</id>
    <author>
        <name>Ömer Sinan Ağacan</name>
        <email>omeragacan@gmail.com</email>
    </author>
    <updated>2025-04-17T00:00:00Z</updated>
    <entry>
    <title>Throwing iterators in Fir</title>
    <link href="http://osa1.net/posts/2025-04-17-throwing-iterators-fir.html" />
    <id>http://osa1.net/posts/2025-04-17-throwing-iterators-fir.html</id>
    <published>2025-04-17T00:00:00Z</published>
    <updated>2025-04-17T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>Recently I’ve been working on extending <a href="https://github.com/fir-lang/fir">Fir</a>’s <code>Iterator</code> trait to allow iterators to throw exceptions.</p>
<p>It took a few months of work, because we needed multiple parameter traits for it to work, which took <a href="https://github.com/fir-lang/fir/pull/73">a few months of hacking</a><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> to implement.</p>
<p>Then there was a lot of bug fixing and experimentation, but it finally works, and I’m excited to share what you can do with Fir iterators today.</p>
<p>As usual, link to the online interpreter with all of the code in this post is at the end.</p>
<p>Before starting, I recommend reading the <a href="https://osa1.net/posts/2025-01-18-fir-error-handling.html">previous post</a>. It’s quite short and it explains the basics of error handling in Fir.</p>
<p>Previous post did not talk about traits at all, so in short, traits in Fir is the same feature as Rust’s traits and Haskell’s typeclasses<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>.</p>
<p>The <code>Iterator</code> trait in Fir is also the same as the trait with the same name in Rust, and it’s used the same way, in <code>for</code> loops.</p>
<p>Here’s a simple example of what you can do with iterators:</p>
<pre><code>sum(nums: Vec[U32]): U32
    let result: U32 = 0
    for i: U32 in nums.iter():
        result += i
    result</code></pre>
<p>The <code>Vec.iter</code> method returns an iterator that returns the next element every time its <code>next</code> method is called. <code>for</code> loop implicitly calls the <code>next</code> method to get the next element, until the <code>next</code> method returns <code>Option.None</code>.</p>
<p>Similar to Rust’s <code>Iterator</code>, Fir’s <code>Iterator</code> trait also comes with a <code>map</code> method that allows mapping iterated elements:</p>
<pre><code>parseSum(nums: Vec[Str]): U32
    let result: U32 = 0
    for i: U32 in nums.iter().map(parseU32):
        result += i
    result

parseU32(s: Str): U32
    if s.len() == 0:
        panic(&quot;Empty input&quot;)

    let result: U32 = 0

    for c: Char in s.chars():
        if c &lt; &#39;0&#39; || c &gt; &#39;9&#39;:
            panic(&quot;Invalid digit&quot;)

        let digit = c.asU32() - &#39;0&#39;.asU32()

        result *= 10
        result += digit

    result</code></pre>
<p>This version takes a <code>Vec[Str]</code> as argument, and parses the elements as integers.</p>
<p>The problem with this version is that it panics on unexpected cases: invalid digits and empty input, and it ignores overflows.</p>
<p>Until now, there wasn’t a convenient way to use the <code>Iterator</code> API and <code>for</code> loops to do this kind of thing, while also propagating exceptions to the call site of the <code>for</code> loop, or to the loop variable. But now we can do this: (<code>parseU32Exn</code> is from the previous post)</p>
<pre><code>parseSum(nums: Vec[Str]): [Overflow, EmptyInput, InvalidDigit, ..errs] U32
    let result: U32 = 0
    for i: U32 in nums.iter().map(parseU32Exn):
        result += i
    result</code></pre>
<p>Errors that <code>parseU32Exn</code> can throw are now implicitly thrown from the <code>for</code> loop and reflected in the function’s type.</p>
<p>This new <code>Iterator</code> API is flexible enough to allow handling some (or all) of the exceptions thrown by a previous iterator. For example, here’s how we can handle <code>InvalidDigit</code> exceptions and yield <code>0</code> instead:</p>
<pre><code>parseSumHandleInvalidDigits(nums: Vec[Str]): [Overflow, EmptyInput, ..errs] U32
    let result: U32 = 0
    for i: U32 in nums.iter().map(parseU32Exn).mapResult(handleInvalidDigit):
        result += i
    result

handleInvalidDigit(parseResult: Result[[InvalidDigit, ..errs], Option[U32]]): [..errs] Option[U32]
    match parseResult:
        Result.Ok(result): result
        Result.Err(~InvalidDigit): Option.Some(0u32)
        Result.Err(other): throw(other)</code></pre>
<p><code>InvalidDigit</code> is no longer in the exception type of the function because <code>mapResult(handleInvalidDigit)</code> handles them.</p>
<p>We can also convert exceptions thrown by an iterator to <code>Result</code> values:</p>
<pre><code>parseSumHandleInvalidDigitsLogRest(nums: Vec[Str]): U32
    let result: U32 = 0
    for i: Result[[Overflow, EmptyInput], U32] in \
            nums.iter().map(parseU32Exn).mapResult(handleInvalidDigit).try():
        match i:
            Result.Err(~Overflow): printStr(&quot;Overflow&quot;)
            Result.Err(~EmptyInput): printStr(&quot;Empty input&quot;)
            Result.Ok(i): result += i
    result</code></pre>
<p>This function no longer has an exception type, because exceptions thrown by the iterator are passed to the loop variable.</p>
<p>In summary, we started with an iterator that doesn’t throw (<code>nums.iter()</code>), mapped it with a function that throws (<code>map(parseU32Exn)</code>), which made the <code>for</code> loop propagate the exceptions thrown by the map function. We then handled one of the exceptions (<code>mapResult(handleInvalidDigit)</code>), and finally, we handled all of the exceptions and started passing a <code>Result</code> value to the loop variable (<code>try()</code>).</p>
<p>The function’s exception type was updated each time to reflect the exceptions thrown by the function.</p>
<p>Once we had multiple parameter traits (which are important even without exceptions, and something we were going to implement anyway), no language features were needed specifically for the throwing iterators API that composes. Changes in the <code>for</code> loop type checking were necessary to allow throwing iterators in <code>for</code> loops. Composing iterators like <code>iter().map(...).mapResult(...).try()</code> in the examples above did not require any changes to the trait system or exceptions.</p>
<p>This demonstrates that Fir traits and exceptions work nicely together.</p>
<p>You can try the code in this blog post <a href="https://fir-lang.github.io/?file=ThrowingIter.fir">in your browser</a>.</p>
<h1 id="im-looking-for-contributors">I’m looking for contributors</h1>
<p>I’m planning a blog post on my vision of Fir, why I think it matters, and a roadmap, but if you already like what you see, know a thing or two about implementing programming languages, and have the time to energy to contribute to a new language, please don’t hesitate to reach out!</p>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>I started this work in one country, and when finished, I was living in another! This PR really felt like an eternity to finish.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>Implementation-wise, it’s closer to Rust than Haskell as we monomorphise.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></summary>
</entry>
<entry>
    <title>Error handling in Fir</title>
    <link href="http://osa1.net/posts/2025-01-18-fir-error-handling.html" />
    <id>http://osa1.net/posts/2025-01-18-fir-error-handling.html</id>
    <published>2025-01-18T00:00:00Z</published>
    <updated>2025-01-18T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>A while ago I came up with an <a href="https://gist.github.com/osa1/38fd51abe5247462eddb7d014f320cd2">“error handling expressiveness benchmark”</a>, some common error handling cases that I want to support in <a href="https://github.com/fir-lang/fir">Fir</a>.</p>
<p>After 7 months of pondering and hacking, I think I designed a system that meets all of the requirements. Error handling in Fir is safe, expressive, and convenient to use.</p>
<p>Here are some examples of what we can do in Fir today:</p>
<p>(Don’t pay too much attention to type syntax for now. Fir is still a prototype, the syntax will be improved.)</p>
<p>When we have multiple ways to fail, we don’t have to introduce a sum type with all the possible ways that we can fail, we can use variants:</p>
<pre><code>parseU32(s: Str): Result[[InvalidDigit, Overflow, EmptyInput, ..r], U32]
    if s.len() == 0:
        return Result.Err(~EmptyInput)

    let result: U32 = 0

    for c in s.chars():
        if c &lt; &#39;0&#39; || c &gt; &#39;9&#39;:
            return Result.Err(~InvalidDigit)

        let digit = c.asU32() - &#39;0&#39;.asU32()

        result = match checkedMul(result, 10):
            Option.None: return Result.Err(~Overflow)
            Option.Some(newResult): newResult

        result = match checkedAdd(result, digit):
            Option.None: return Result.Err(~Overflow)
            Option.Some(newResult): newResult

    Result.Ok(result)</code></pre>
<p>An advantage of variants is, in pattern matching, we “refine” types of binders to drop handled variants from the type. This allows handling some of the errors and returning the rest to the caller:</p>
<pre><code>defaultEmptyInput(res: Result[[EmptyInput, ..r], U32]): Result[[..r], U32]
    match res:
        Result.Err(~EmptyInput): Result.Ok(0u32)
        Result.Err(other): Result.Err(other)
        Result.Ok(val): Result.Ok(val)</code></pre>
<p>Here <code>EmptyInput</code> is removed from the error value type in the return type. The caller does not need to handle <code>EmptyInput</code>.</p>
<p>(We don’t refine types of variants nested in other types for now, so the last two branches cannot be replaced with <code>other: other</code> for now.)</p>
<p>Another advantage is that they allow composing error returning functions that return different error types:</p>
<p>(Fir supports variant constructors with fields, but to keep things simple we don’t use them in this post.)</p>
<pre><code>readFile(s: Str): Result[[IoError, ..r], Str]
    # We don&#39;t have the standard library support for file IO yet, just return
    # an error for now.
    Result.Err(~IoError)

parseU32FromFile(filePath: Str): Result[[InvalidDigit, Overflow, EmptyInput, IoError, ..r], U32]
    let fileContents = match readFile(filePath):
        Result.Err(err): return Result.Err(err)
        Result.Ok(contents): contents

    parseU32(fileContents)</code></pre>
<p>In the early return I don’t have to manually convert <code>readFile</code>s error value to <code>parseU32</code>s error value to make the types align.</p>
<p>Variants work nicely with higher-order functions as well. Here’s a function that parses a vector of strings, returning any errors to the caller:</p>
<pre><code>parseWith(vec: Vec[Str], parseFn: Fn(Str): Result[errs, a]): Result[errs, Vec[a]]
    let ret = Vec.withCapacity(vec.len())

    for s in vec.iter():
        match parseFn(s):
            Result.Err(err): return Result.Err(err)
            Result.Ok(val): ret.push(val)

    Result.Ok(ret)</code></pre>
<p>If I have a function argument that returns more errors than my callback, I can still call it without any adjustments:</p>
<pre><code>parseWith2(vec: Vec[Str], parseFn: Fn(Str): Result[[OtherError, ..r], a]): Result[[..r], Vec[a]]
    let ret = Vec.withCapacity(vec.len())

    for s in vec.iter():
        match parseFn(s):
            Result.Err(~OtherError): continue
            Result.Err(err): return Result.Err(err)
            Result.Ok(val): ret.push(val)

    Result.Ok(ret)</code></pre>
<p><code>parseWith2(vec, parseU32)</code> type checks even though <code>parseU32</code> doesn’t return <code>OtherError</code>.</p>
<p>Similarly, if I have a function that handles more cases, I can pass it as a function that handles less:</p>
<pre><code>handleSomeErrs(error: [Overflow, OtherError]): U32 = 0

parseWithErrorHandler(
        input: Str,
        handler: Fn([Overflow, ..r1]): U3
    ): Result[[InvalidDigit, EmptyInput, ..r2], U32]
    match parseU32(input):
        Result.Err(~Overflow): Result.Ok(handler(~Overflow))
        Result.Err(other): Result.Err(other)
        Result.Ok(val): Result.Ok(val)</code></pre>
<p>Here I’m able to pass <code>handleSomeErrs</code> to <code>parseWithErrorHandler</code>, even though it handles more errors than what <code>parseWithErrorHandler</code> argument needs.</p>
<h1 id="variants-as-exceptions">Variants as exceptions</h1>
<p>When we use variants as exception values, we end up with a system that is</p>
<ul>
<li>Safe: All exceptions need to be handled before <code>main</code> returns.</li>
<li>Flexible: All of the flexibility of variants shown above apply to exceptions as well.</li>
<li>Convenient:
<ul>
<li>Error values are implicitly propagated to the caller when not handled.</li>
<li>When a library uses one way of error reporting (error values or exceptions) and you need the other, conversion is just a matter of calling one function.</li>
</ul></li>
</ul>
<p>At the core of exceptions in Fir are these three functions:</p>
<ul>
<li><p><code>throw</code>, which converts a variant into an exception:</p>
<pre><code>throw(exn: exn): exn a</code></pre></li>
<li><p><code>try</code>, which converts exceptions into <code>Result.Err</code> values:</p>
<pre><code>try(cb: Fn(): exn a): Result[exn, a]</code></pre></li>
<li><p><code>untry</code>, which converts a <code>Result.Err</code> value into an exception:</p>
<pre><code>untry(res: Result[exn, a]): exn a</code></pre></li>
</ul>
<p>Here are some of the code above, using exceptions instead of error values:</p>
<pre><code>parseU32Exn(s: Str): [InvalidDigit, Overflow, EmptyInput, ..r] U32
    if s.len() == 0:
        throw(~EmptyInput)

    let result: U32 = 0

    for c in s.chars():
        if c &lt; &#39;0&#39; || c &gt; &#39;9&#39;:
            throw(~InvalidDigit)

        let digit = c.asU32() - &#39;0&#39;.asU32()

        result = match checkedMul(result, 10):
            Option.None: throw(~Overflow)
            Option.Some(newResult): newResult

        result = match checkedAdd(result, digit):
            Option.None: throw(~Overflow)
            Option.Some(newResult): newResult

    result

readFileExn(s: Str): [IoError, ..r] Str
    # We don&#39;t have the standard library support for file IO yet, just throw
    # an error for now.
    throw(~IoError)

parseU32FromFileExn(filePath: Str): [InvalidDigit, Overflow, EmptyInput, IoError, ..r] U32
    parseU32Exn(readFileExn(filePath))

parseWithExn(vec: Vec[Str], parseFn: Fn(Str): exn a): exn Vec[a]
    let ret = Vec.withCapacity(vec.len())
    for s in vec.iter():
        ret.push(parseFn(s))
    ret</code></pre>
<p>When a library provides one of these, it’s trivial to convert to the other:</p>
<pre><code>parseU32UsingExnVersion(s: Str): Result[[InvalidDigit, Overflow, EmptyInput, ..r], U32]
    try({ parseU32Exn(s) })

parseU32UsingResultVersion(s: Str): [InvalidDigit, Overflow, EmptyInput, ..r] U32
    untry(parseU32(s))</code></pre>
<p>Nice!</p>
<hr />
<p>I’m quite excited about these results. There’s still so much to do, but I think it’s clear that this way of error handling has a lot of potential.</p>
<p>I’ll be working on some of the improvements I mentioned above (and I have others planned as well), and the usual stuff that every language needs (standard library, tools etc.). Depending on interest, I may also write more about variants, error handling, or anything else related to Fir.</p>
<p>You can try Fir online <a href="https://fir-lang.github.io/?file=ErrorHandling.fir">here</a>.</p>]]></summary>
</entry>
<entry>
    <title>When is inlining useful?</title>
    <link href="http://osa1.net/posts/2024-12-07-inlining.html" />
    <id>http://osa1.net/posts/2024-12-07-inlining.html</id>
    <published>2024-12-07T00:00:00Z</published>
    <updated>2024-12-07T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>Especially in high-level languages, inlining is most useful when it leads to:</p>
<ul>
<li>Optimizing the callee’s body based on the arguments passed.</li>
<li>Optimizing the call site based on the callee’s return value.</li>
</ul>
<p>Let’s look at some examples.</p>
<h1 id="example-avoiding-redundant-bounds-checks">Example: avoiding redundant bounds checks</h1>
<p>Suppose we have a library for decoding some format of binary files with length-prefixed vectors of 32-bit integers, with all integers encoded as little-endian.</p>
<p>A simple implementation would be:</p>
<pre><code>/// Decode a length-prefixed 32-bit unsigned integer vector.
///
/// # Panics
///
/// Panics if the buffer does not have enough bytes.
pub fn decode_u32_vec(buffer: &amp;[u8]) -&gt; Vec&lt;u32&gt; {
    let len = decode_32_le(buffer, 0) as usize;
    let mut vec = vec![0; len];
    for i in 0..len {
        vec[i] = decode_32_le(buffer, (i + 1) * 4);
    }
    vec
}

/// Decode a single 32-bit unsigned integer, encoded as little-endian.
///
/// # Panics
///
/// Panics if the buffer does not have 4 bytes starting at `offset`.
#[inline(never)]  // this version isn&#39;t inlined
pub fn decode_32_le(buffer: &amp;[u8], offset: usize) -&gt; u32 {
    let b1 = buffer[offset];
    let b2 = buffer[offset + 1];
    let b3 = buffer[offset + 2];
    let b4 = buffer[offset + 3];
    u32::from_le_bytes([b1, b2, b3, b4])
}</code></pre>
<p>This version is not ideal, because <code>decode_32_le</code> checks bounds on each byte access. (<a href="https://godbolt.org/z/eTMa38nqT">compiler explorer</a>)</p>
<p>We can improve it by checking the bounds for all of the reads once:</p>
<pre><code>#[inline(never)]  // this version isn&#39;t inlined
pub fn decode_32_le(buffer: &amp;[u8], offset: usize) -&gt; u32 {
    if buffer.len() &lt; 4 || buffer.len() - 4 &lt; offset {
        panic!();
    }
    let b1 = buffer[offset];
    let b2 = buffer[offset + 1];
    let b3 = buffer[offset + 2];
    let b4 = buffer[offset + 3];
    u32::from_le_bytes([b1, b2, b3, b4])
}</code></pre>
<p>The conditional makes sure that in the rest of the function the slice indices are all within bounds, so the compiler doesn’t generate bounds checks for the accesses. The compiler is also able to optimize further now to load just one 32-bit word from the memory, instead of reading one byte at a time. (<a href="https://godbolt.org/z/MG9EaE5G3">compiler explorer</a>)</p>
<p><code>decode_32_le</code> is quite good now, but it still has to do one bounds check, and since <code>decode_u32_vec</code> calls it in each iteration, it does a bounds check in each iteration.</p>
<p>Ideally we want to be able to do one bounds check before the loop, just like we did in <code>decode_32_le</code>, and then omit bounds checks within the loop:</p>
<pre><code>pub fn decode_u32_vec(buffer: &amp;[u8]) -&gt; Vec&lt;u32&gt; {
    let len = decode_32_le(buffer, 0) as usize;
    if buffer.len() &lt; (len + 1) * 4 {
        panic!();
    }
    let mut vec = vec![0; len];
    for i in 0..len {
        vec[i] = decode_32_le(buffer, (i + 1) * 4);
    }
    vec
}</code></pre>
<p>But this cannot make the bounds checks in <code>decode_32_le</code> disappear, as it may have other call sites that may not always check the bounds before calling it.</p>
<p>Inlining <code>decode_32_le</code> in the use effectively lets us propagate the information in the call site to the callee’s code, and optimize it based on this information. If we change the <code>inline(never)</code> to <code>inline</code> in <code>decode_32_le</code>, with the extra bounds check in <code>decode_u32_vec</code>, we now check bounds once before the loop and don’t check it again in the loop. (<a href="https://godbolt.org/z/EeGbYrfP7">compiler explorer</a>)</p>
<h1 id="example-avoiding-redundant-error-checks">Example: avoiding redundant error checks</h1>
<p>Dart doesn’t have unsigned integers, and many standard library functions throw an <code>ArgumentError</code> when they are passed negative numbers.</p>
<p>One example of these functions is the <a href="https://api.dart.dev/dart-core/int/operator_triple_shift.html">unsigned right shift operator</a>. In many call sites, the shift amount is a <a href="https://github.com/dart-lang/sdk/blob/abb17bc59d8163273451b3ffb2aba784d20001b4/sdk/lib/_internal/wasm/lib/internal_patch.dart#L88-L97">constant positive number</a>. If we call the standard library function in these cases, the library function will check the sign of the arguments that we already know in the call site to be positive.</p>
<p>When we inline <a href="https://github.com/dart-lang/sdk/blob/abb17bc59d8163273451b3ffb2aba784d20001b4/sdk/lib/_internal/wasm/lib/boxed_int.dart#L94-L107">the operator</a> in a call site where the shift argument is constant, the conditional becomes <code>if (constant &lt; 0) throw ...</code>. The compiler can then simplify the condition as <code>true</code> or <code>false</code>, and then simplify this code to just <code>throw ...</code> or eliminate the conditional.</p>
<p>The <code>mix64</code> function linked above when compiled to Wasm, without inlining the right shift operator:</p>
<pre><code>(func $mix64 (param $var0 i64) (result i64)
  ...
  local.tee $var0
  i64.const 24
  call $BoxedInt.&gt;&gt;&gt;
  ...)

(func $BoxedInt.&gt;&gt;&gt; (param $var0 i64) (param $var1 i64) (result i64)
  local.get $var1
  i64.const 64
  i64.lt_u
  if
    local.get $var0
    local.get $var1
    i64.shr_u
    return
  end
  local.get $var1
  i64.const 0
  i64.lt_s
  if
    i32.const 64
    local.get $var1
    struct.new $BoxedInt
    call $ArgumentError
    call $Error._throwWithCurrentStackTrace
    unreachable
  end
  i64.const 0)</code></pre>
<p>With the shift operator inlined:</p>
<pre><code>(func $mix64 (param $var0 i64) (result i64)
  ...
  local.get $var0
  i64.const 24
  i64.shr_u
  ...)</code></pre>
<p>The call to <code>BoxedInt.&gt;&gt;&gt;</code> with error checking is optimized to a single <code>shr_u</code> (shift right, unsigned) instruction.</p>
<h1 id="example-avoiding-boxing">Example: avoiding boxing</h1>
<p>In languages where most values are passed around as boxed, inlining can eliminate boxing.</p>
<p>A common use case where this happens is FFI: pointers/references obtained from FFI calls need to be wrapped in a struct/class/etc. to make them the same “shape” as the language’s native values.</p>
<p>When you have a function that gets a reference from an FFI call, and pass it around to more FFI calls, inlining these FFI calls can avoid boxing the pointer/reference value.</p>
<p>Somewhat silly example, in Dart:</p>
<pre><code>import &#39;dart:ffi&#39;;

@Native&lt;Pointer&lt;Int64&gt; Function()&gt;()
external Pointer&lt;Int64&gt; intPtr();

@Native&lt;Int64 Function(Pointer&lt;Int64&gt;)&gt;()
external int derefIntPtr(Pointer&lt;Int64&gt; ptr);

void main() {
  Pointer&lt;Int64&gt; ptr = intPtr();
  ptr += 1;
  int i = derefIntPtr(ptr);
  print(i);
}</code></pre>
<p>Relevant parts of the generated code when compiled to Wasm:</p>
<pre><code>(func $main
  ...
  call $intPtr
  struct.get $Pointer $field2
  i32.const 8
  i32.add
  call $ffi.derefIntPtr (import)
  ...)

(func $intPtr (result (ref $Pointer))
  i32.const 71
  i32.const 0
  call $ffi.intPtr (import)
  struct.new $Pointer)</code></pre>
<p><code>intPtr</code> allocates a struct, which the call site directly unpacks (reads the field). Inlining <code>intPtr</code> eliminates this allocation:</p>
<pre><code>(func $main
  ...
  call $ffi.intPtr (import)
  i32.const 8
  i32.add
  call $ffi.derefIntPtr (import)
  ...)</code></pre>
<h1 id="example-avoiding-polymorphism">Example: avoiding polymorphism</h1>
<p>When a monomorphic type is passed to a polymorphic function, the polymorphic function can often be inlined to avoid polymorphic access to the monomorphic type.</p>
<p>An example, again in Dart, is <code>Int64List</code>, which is a monomorphic <code>List&lt;int&gt;</code>. It stores the integers unboxed, and when used directly, the integer arguments and return values do not need to be boxed.</p>
<p>When used in a polymorphic site though, the integer elements need to be boxed. Example:</p>
<pre><code>import &#39;dart:typed_data&#39;;

int sum(List&lt;int&gt; list) {
  int ret = 0;
  for (int i = 0; i &lt; list.length; i += 1) {
    ret += list[i];
  }
  return ret;
}

void main() {
  Int64List intList = Int64List.fromList([1, 2, 3, 4]);
  sum(intList);
  sum([1, 2, 3, 4]);
}</code></pre>
<p>Relevant parts of the output when compiled to Wasm:</p>
<pre><code>(func $main
  ;; Allocate `Int64List`, call `sum`:
  ...
  call $sum

  ;; Allocate the other `List&lt;int&gt;`, call `sum`:
  ...
  call $sum)

(func $sum (param $var0 (ref $Object))
  (local $var1 i64)
  (local $var2 i64)
  loop $label0
    ...
    if
      ...
      ;; Virtual call to `operator []`:
      struct.get $Object $field0
      i32.const 747
      i32.add
      call_indirect (param (ref $Object) i64) (result (ref null $#Top))

      ;; The virtual call returns a boxed integer, which we directly unbox:
      ref.cast $BoxedInt
      struct.get $BoxedInt $field1
      i64.add
      ...
    end
  end $label0)</code></pre>
<p>If we inline <code>sum</code>, the loop that iterates the <code>Int64List</code> accesses the unboxed integers directly:</p>
<pre><code>(func $main
  ...
  loop $label1
      ...
      local.get $var3
      local.get $var4
      local.get $var1
      i32.wrap_i64
      ;; Array access is now direct, no boxing.
      array.get $Array&lt;WasmI64&gt;
      i64.add
      local.set $var3
      local.get $var1
      i64.const 1
      i64.add
      local.set $var1
      br $label1
    end
  end $label1)</code></pre>
<p>A similar case is when a monomorphic type is used directly, but via a polymorphic interface. In the <code>Int64List</code> type above, <code>List64List.[]</code> is an override of <a href="https://api.dart.dev/dart-core/List/operator_get.html"><code>List&lt;E&gt;.[]</code></a>, and all overrides of a method need to have the same type.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>So when not inlined, it needs to return a boxed integer:</p>
<pre><code>(func $Int64List.[] (param $var0 (ref $Object)) (param $var1 i64) (result (ref null $#Top))
  ...
  array.get $Array&lt;WasmI64&gt;
  struct.new $BoxedInt)</code></pre>
<p>Similar to the previous example, inlining it eliminates the boxing in monomorphic use sites, as the allocated struct is immediately unboxed.</p>
<h1 id="example-eliminating-closures-and-indirect-calls">Example: eliminating closures and indirect calls</h1>
<p>In languages without monomorphisation, higher-order function arguments need to be allocated as closures.</p>
<p>When calling a closure, the caller needs to get the function pointer from the closure and call the function via the pointer.</p>
<p>These closure allocations and indirect function calls can be too slow in hot loops.</p>
<p>An example of this is in <a href="https://github.com/google/protobuf.dart">Dart protobuf library</a>. In protobufs, packed fields are encoded as a length prefix followed by the elements. In the Dart library, we decode these fields using <a href="https://github.com/google/protobuf.dart/blob/610943a3bed70c1c2079af5fca02462df10d223f/protobuf/lib/src/protobuf/coded_buffer.dart#L271-L277">this helper</a>:</p>
<pre><code>void _readPacked(CodedBufferReader input, void Function() readFunc) {
  input._withLimit(input.readInt32(), () {
    while (!input.isAtEnd()) {
      readFunc();
    }
  });
}</code></pre>
<p>Here the caller of <code>_readPacked</code> passes <code>readFunc</code> as a closure (allocation). <code>_readPacked</code> then allocates another closure, to be passed to <code>_withLimit</code>.</p>
<p>That’s two closure allocations for every packed field in the input. Calling these closures are slow too, because they’re indirect.</p>
<p>Inlining <code>_withLimit</code> in <code>_readPacked</code>, and <code>_readPacked</code> in its call sites (in <code>_mergeFromCodedBufferReader</code>) eliminates closure allocations, and calls to the closures become direct function calls. This improves packed field decoding significantly.</p>
<p>(This change isn’t merged yet, the PR is <a href="https://github.com/google/protobuf.dart/pull/959">here</a>.)</p>
<h1 id="a-trick-for-effective-inlining-outlining">A trick for effective inlining: outlining</h1>
<p>Consider <code>Int64List.[]</code> again. The implementation needs to check that the index is in bounds of the array, and throw an exception if not: (slightly simplified)</p>
<pre><code>class Int64List ... {
  ...

  @override
  int operator [](int index) {
    if (length.leU(index)) { // shorthand for `index &lt; 0 || index &gt;= length`
      ... // throw exception
    }
    return _data.read(index);
  }
}</code></pre>
<p>To avoid boxing when calling this function we want to always inline it, but if we’re not careful with the error throwing code path (the <code>if</code> body above), the function can get quite large, and when inlined the binary can bloat up significantly.</p>
<p>Ideally we want to inline the happy path that can lead to improvements when inlined, and leave the error path separate in a function, so that we can have the benefits of inlining without adding to the binary size too much.</p>
<p>This can be done by moving the error handling code to a separate function, and making sure that separate function is never inlined (ideally with an annotation to the compiler). In the example above, this may look like:</p>
<pre><code>class Int64List ... {
  @override
  @pragma(&#39;inline&#39;)
  int operator [](int index) {
    if (length.leU(index)) { // shorthand for `index &lt; 0 || index &gt;= length`
      fail(index, length);
    }
    return _data.read(index);
  }
}

@pragma(&#39;never-inline&#39;)
void fail(int index, int length) { ... }</code></pre>
<p>With this it doesn’t matter how large the error handling code is, because we never inline it.</p>
<p>This way of separating inlineable parts of a function from the parts we don’t want to inline is sometimes called “outlining” or “partial inlining”. We can do it manually (as in the example above), but it can also be done by a compiler based on heuristics.</p>
<p>An example transformation to this is <a href="https://www.cambridge.org/core/services/aop-cambridge-core/content/view/75629BBEDB11D8463553A09BF5DEA235/S0956796809007175a.pdf/div-class-title-the-worker-wrapper-transformation-div.pdf">GHC’s worker/wrapper transformation</a>, which splits a function into parts that (1) handle unboxing and boxing before calling the function’s body, and (2) the function’s body that work on the unboxed representations of the arguments. (1) is then inlined to avoid redundant boxing of the arguments and return values.</p>
<p>Another example is <a href="https://github.com/WebAssembly/binaryen">wasm-opt</a> which does <a href="https://github.com/WebAssembly/binaryen/blob/6fe5103ab58a4eb751998d13768a0f25795a0de6/src/passes/Inlining.cpp#L684-L754">partial inlining</a>.</p>
<h1 id="a-tip-for-effective-inlining-avoid-inlining-in-slow-paths">A tip for effective inlining: avoid inlining in slow paths</h1>
<p>In the <a href="https://github.com/google/protobuf.dart/blob/610943a3bed70c1c2079af5fca02462df10d223f/protobuf/lib/src/protobuf/coded_buffer.dart#L54-L267">protobuf decoding loop</a> that we mentioned above, we have a big type switch to determine how to decode a field. It looks like this:</p>
<pre><code>switch (fieldType) {
  case PbFieldType._OPTIONAL_BOOL:
    fs._setFieldUnchecked(meta, fi, input.readBool());
    break;
  ...
  case PbFieldType._REPEATED_UINT64:
    final list = fs._ensureRepeatedField(meta, fi);
    if (wireType == WIRETYPE_LENGTH_DELIMITED) {
      _readPacked(input, () =&gt; list.add(input.readUint64()));
    } else {
      list.add(input.readUint64());
    }
    break;
  ...
  // 37 cases in total. 
}</code></pre>
<p>Assume that there’s a function that throws an exception (e.g. <code>_ensureRepeatedField</code> above), and it’s used in only once, in one of the <code>case</code>s of the <code>switch</code>.</p>
<p>Because the function is only used once, it may look like inlining it makes sense, as that would eliminate function call overhead, shave at least a few bytes off the binary, and potentially allow optimizations in the call site.</p>
<p>However since this switch is in a hot loop, and the inlined code branch is taken very rarely (unless the application is receiving a lot of malformed input), inlining this slow path can make the instruction cache usage much worse and slow down the decoder.</p>
<p>This kind of inlining can commonly happen when optimizing for size, e.g. with <code>wasm-opt -Os</code>. Because inlining single-use functions reduce binary sizes, optimization modes that aim to make the final binaries smaller can inline slow-path error throwing functions.</p>
<p>If we really want to inline a slow-path function, a way to avoid making instruction cache usage worse is to move the slow-path code to the end of the function, away from the common code paths.</p>
<p>This is often done with branch prediction hints, such as clang’s <a href="https://llvm.org/docs/BranchWeightMetadata.html#builtin-expect"><code>__builtin_expect</code></a>. When a branch is annotated as “not likely to be taken”, the compiler can move the branch target (the basic blocks) to the end of the current function, away from the hot code. This gives us the binary size benefits of inlining single-use functions, without filling the instruction cache with instructions that will never be executed.</p>
<h1 id="remarks">Remarks</h1>
<p>The main reason why the examples above are mostly in Dart is because I’ve been spending most of my time this year optimizing Dart’s Wasm backend’s standard library implementation, so the examples are still fresh in my memory.</p>
<p>The principles apply to most languages: inlining a function makes any information about the arguments available to the function’s body, and any information on its return value to the call site.</p>
<p>A big part of optimizing high-level statically-typed languages is about avoiding polymorphism, boxing, and redundant error checks. I’m not aware of any cases where inlining a function in a high-level language, when it doesn’t result in improving one of these, is worthwhile.</p>
<p>In a lower-level language with monomorphisation and control over allocations (e.g. Rust, C++), monomorphisation eliminates polymorphism in compile time, boxing is explicit, and redundant checks can be avoided by using unchecked (unsafe) functions.</p>
<p>In these cases where programs are often much faster by default, inlining to avoid stack/register shuffling and simplify control flow can make a difference.</p>
<p>One example of simplified control flow making a big difference can be seen in <a href="https://osa1.net/posts/2024-11-29-how-to-parse-3.html">the previous post</a>, where implementing a recursive parsing function in a non-recursive way improved performance by 22%.</p>
<h1 id="updates">Updates</h1>
<ul>
<li><strong>2024-12-07:</strong> Added link to wasm-opt in partial inlining section.</li>
<li><strong>2025-02-12:</strong> Added a higher-order function example.</li>
<li><strong>2025-02-14:</strong> Added a section about inlining in slow paths.</li>
</ul>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>More accurately, a method that can be called in a polymorphic call site needs to have a type that is a subtype of the least-upper-bound of the types of all functions that can be called in the polymorphic call sites.</p>
<p>Or more briefly, all methods in an override group need to have the same type.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></summary>
</entry>
<entry>
    <title>Exploring parsing APIs: the cost of recursion</title>
    <link href="http://osa1.net/posts/2024-11-29-how-to-parse-3.html" />
    <id>http://osa1.net/posts/2024-11-29-how-to-parse-3.html</id>
    <published>2024-11-29T00:00:00Z</published>
    <updated>2024-11-29T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>In the <a href="https://osa1.net/posts/2024-11-22-how-to-parse-1.html">first post</a> of this series we looked at a few different ways of parsing a simple JSON-like language. In the <a href="https://osa1.net/posts/2024-11-28-how-to-parse-2.html">second post</a> we implemented a few lexers, and looked at the performance when the parsers from the first post are combined with the lexers in the second post.</p>
<p>One of the surprising results in these posts is that our recursive descent parser, which parses the input directly to an AST, and in some sense is the simplest possible implementation when we need an AST, actually performs the worst.</p>
<p>(The implementations that collect tokens in a vector before parsing perform worse than recursive descent parsing, but those implementations have other issues as well, and can’t be used in many cases. Maybe I should’ve omitted them entirely.)</p>
<p>We had a lot of benchmarks in the last post, but to keep things simple, let’s consider these three:</p>
<ul>
<li>Recursive descent: 127 Mb/s</li>
<li>tokenize_iter + events_iter to AST: 138 Mb/s</li>
<li>tokenize_push + events_push to AST: 151 Mb/s</li>
</ul>
<p>To recap, the “iter” variants are <code>Iterator</code>s that return one parse (or lexing) event at a time. The “push” variants take a “listener” argument with callbacks for events. See the first post for details.</p>
<p>In the “iter” benchmark, the code that generates the AST iterates an event parser:</p>
<pre><code>fn event_to_tree&lt;I: Iterator&lt;Item = Result&lt;ParseEvent, ParseError&gt;&gt;&gt;(
    parser: &amp;mut I,
    input: &amp;str,
) -&gt; Result&lt;Json, ParseError&gt; {
  ...
}</code></pre>
<p>And the event parser iterates a lexer:</p>
<pre><code>fn parse_events_iter_using_lexer_iter&lt;I: Iterator&lt;Item = Result&lt;(usize, Token), usize&gt;&gt;&gt;(
    lexer: I,
    input_size: usize,
) -&gt; EventParser&lt;I&gt; {
    ...
}</code></pre>
<p>When the AST generator asks for the next parse event, the parse event generator asks for the next token (maybe multiple times) and returns an event. AST generator consumes all of the events and builds the AST.</p>
<p>In the “push” benchmark, we have a “listener” that handles parse events and builds up an AST:</p>
<pre><code>struct AstBuilderListener&lt;&#39;a&gt; {
    input: &amp;&#39;a str,
    container_stack: Vec&lt;Container&gt;,
    current_container: Option&lt;Container&gt;,
    parsed_object: Option&lt;Json&gt;,
    error: Option&lt;ParseError&gt;,
}

impl&lt;&#39;a&gt; EventListener for AstBuilderListener&lt;&#39;a&gt; {
    ...
}</code></pre>
<p>And another listener that handles tokens:</p>
<pre><code>struct LexerEventListenerImpl&lt;&#39;a, L: EventListener&gt; {
    listener: &amp;&#39;a mut L,
    container_stack: Vec&lt;Container&gt;,
    state: ParserState,
}

impl&lt;&#39;a, L: EventListener&gt; LexerEventListener for LexerEventListenerImpl&lt;&#39;a, L&gt; {
  ...
}</code></pre>
<p>This implementation is driven by the lexer which “pushes” the tokens to the event parser, which (sometimes after handling multiple tokens) “pushes” parse events to the AST builder.</p>
<p>Both of these setups are considerably more complicated than recursive descent, yet they perform better. How?</p>
<p>When we consider what the recursive descent parser does that these don’t, it’s kind of obvious. It’s even in the name: recursion.</p>
<p>Our lexers and event parsers all optimize really well: there is no heap allocation in the parsing code (only when building the arrays and objects in the AST, which all implementations have), the code that “pushes” events are all monomorphised based on the handler type, so the handler calls are direct calls and can be (and probably is) inlined. There’s no recursion anywhere.</p>
<p>The recursive descent parser is basically one function that recursively calls itself for nested objects. It turns out this recursion has a cost. When I eliminate the recursion with some more state:</p>
<pre><code>enum ParserState {
    /// Parse any kind of object, update state based on the current container.
    TopLevel,

    /// Parsing a container, parse another element on &#39;,&#39;, or finish the
    /// container on &#39;]&#39; or &#39;}&#39;.
    ExpectComma,

    /// Parsing an object, parse a key.
    ObjectExpectKeyValue,

    /// Parsing an object, parse a key, or terminate the object.
    ObjectExpectKeyValueTerminate,

    /// Parsing an object and we&#39;ve just parsed a key, expect &#39;:&#39;.
    ObjectExpectColon,
}

fn parse_single(iter: &amp;mut Peekable&lt;CharIndices&gt;, input: &amp;str) -&gt; Result&lt;Json, ParseError&gt; {
    let mut container_stack: Vec&lt;Container&gt; = vec![];
    let mut state = ParserState::TopLevel;

    loop {
      ...
    }
}</code></pre>
<p>It performs better than the recursive descent parser and the iterator based parser, and on par with the “push” based parser: (numbers are slightly different than above as I rerun them again together)</p>
<ul>
<li>Recursive descent: 127 Mb/s</li>
<li>tokenize_iter + events_iter to AST: 136 Mb/s</li>
<li>tokenize_push + events_push to AST: 158 Mb/s</li>
<li>Direct parser without recursion: 156 Mb/s</li>
</ul>
<p>I’m not quite sure what about recursion that makes the recursive descent parser perform so much worse, but my guess is that it makes the control flow more complicated to analyze, and in runtime, you have to move things around (in registers and stack locations) based on calling conventions. When moving between registers and stack locations you do memory reads and writes. My guess is that when combined, these cost something.</p>
<h1 id="other-considerations-with-recursion">Other considerations with recursion</h1>
<p>If you checkout the git repo and run the tests with <code>cargo test</code>, you will see that a test fails with a stack overflow.</p>
<p>This is something else to keep in mind when parsing recursively. Stack overflows are a real issue with recursive parsing, and I know some libraries that are <a href="https://github.com/google/protobuf.dart/blob/ccf104dbc36929c0f8708285d5f3a8fae206343e/protobuf/lib/src/protobuf/coded_buffer_reader.dart#L29">explicit about it</a>.</p>
<p>In practice though, I’m not sure if this can be the main reason to avoid recursive parsing. Recursion can happen in other places as well, and in a server application you would probably monitor runtime, memory consumption, and maybe even other resources of a handler, and have some kind of error handler that handles everything else.</p>
<p>Some higher level languages like <a href="https://hackage.haskell.org/package/base-4.20.0.1/docs/GHC-IO-Exception.html#t:AsyncException">Haskell</a> and <a href="https://api.dart.dev/dart-core/StackOverflowError-class.html">Dart</a> make stack overflows exceptions/errors that can be caught and handled, so they can be handled as a part of “unexpected” crashes easily. In Rust, stack overflows can be handled at thread boundaries.</p>
<p>If the application is a command line tool or a compiler, where the input is provided by the user and handled on the user’s computer, it’s less of a problem and you can probably just let the application crash.</p>
<p>So I don’t think we can say that recursion should be avoided at all costs when parsing.</p>
<h1 id="references">References</h1>
<p>As usual, the code is available: <a href="https://github.com/osa1/how-to-parse/tree/main/part3">github.com/osa1/how-to-parse-3</a>.</p>
<p>To work around the stack overflow when testing, test in release mode: <code>cargo test --release</code>.</p>
<p>If you want to profile the code and understand more about why one version is faster than the other, I added 4 executables to the package, one for each benchmark listed above. You can generate a 100M input and run the parsers individually with:</p>
<pre><code>$ cargo build --release
...

$ ./target/release/test_gen 100000000 &gt; input

$ time ./target/release/parse_non_recursive input
./target/release/parse_non_recursive input  0.64s user 0.22s system 99% cpu 0.854 total</code></pre>]]></summary>
</entry>
<entry>
    <title>Exploring parsing APIs: adding a lexer</title>
    <link href="http://osa1.net/posts/2024-11-28-how-to-parse-2.html" />
    <id>http://osa1.net/posts/2024-11-28-how-to-parse-2.html</id>
    <published>2024-11-28T00:00:00Z</published>
    <updated>2024-11-28T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>In the <a href="https://osa1.net/posts/2024-11-22-how-to-parse-1.html">previous post</a> we looked at three different parsing APIs, and compared them for runtime and the use cases they support.</p>
<p>In this post we’ll add a lexer (or “tokenizer”), with two APIs, and for each lexer see how the parsers from the previous post perform when combined with the lexer.</p>
<p><strong>What is a lexer?</strong> A lexer is very similar to the event parsers we saw in the previous post, but it doesn’t try to maintain any structure. It generates “tokens”, which are parts of the program that cannot be split into smaller parts. A lexer doesn’t care about parentheses or other delimiters being balanced, or that values in an array are separated by commas, or anything else. It simply splits the input into tokens.</p>
<p><strong>Why is a lexer useful?</strong> If you already have an event parser, adding a lexer may not allow a lot of new use cases. The main use cases that I’m aware of are:</p>
<ul>
<li><p>Syntax highlighting: when higlighting syntax we don’t care about the tree structure, we care about keywords, punctuation (list separators, dots in paths etc.), delimiters (commas, bracets, brackets), and literals. A lexer gives us exactly these and nothing else.</p></li>
<li><p>Supporting incremental parsing: one way of incrementally update an AST is by starting re-lexing a few (often just one) tokens before the edited token, re-lexing until after the edit location, until generating a token identical to an existing token again. AST nodes of modified tokens are then marked as “modified” and re-parsed.</p>
<p>The details are complicated, I recommend chapter 2 of <a href="https://diekmann.uk/diekmann_phd.pdf">this PhD thesis</a> for an introduction to incremental parsing.</p>
<p>If you need to re-parse code as it gets edited, even if you don’t need or want incremental parsing, incremental lexing is easy, it makes sense to re-lex incrementally and then parse from scratch using the incrementally updated token list, because incremental lexing is so simple.</p></li>
<li><p>For separating complex parsing code into smaller parts: modern languages can have complicated literal syntax, with multiple string literals with varying delimiters (like <code>r#"..."#</code> syntax in Rust, or <code>[=[...]=]</code> in Lua), multiple variants of comments (single line and multi-line, documentation and normal), multiple number syntaxes (with different suffixes like <code>123u32</code> in Rust, underscores to separate digits for readability) and so on.</p>
<p>A lexer separates handling of these from the part of the parser that deals with the program structure.</p></li>
</ul>
<h1 id="the-apis">The APIs</h1>
<p>Similar to the previous post, we will look at three different APIs for lexing:</p>
<ul>
<li>A lexer that generates a list of tokens directly: <code>tokenize_list</code>.</li>
<li>An iterator that generates one token at a time: <code>tokenize_iter</code>.</li>
<li>A “push” API that calls “listener” methods for the tokens: <code>tokenize_push</code>.</li>
</ul>
<p>For our simplified (and enhanced, with comments) JSON, our token type is:</p>
<pre><code>pub enum Token {
    Int(u64),
    Str { size_in_bytes: usize },
    True,
    False,
    Null,
    LBracket,
    RBracket,
    LBrace,
    RBrace,
    Colon,
    Comma,
    Comment { size_in_bytes: usize },
}</code></pre>
<p>Similar to our event type from the previous post, this type needs to be cheap to generate (ideally stack allocated).</p>
<p>The tokens are generated along with byte offsets in the input, also similar to events.</p>
<p>For the push API, the listener interface also directly follows the token type:</p>
<pre><code>pub trait LexerEventListener {
    fn handle_int(&amp;mut self, byte_offset: usize, i: u64);

    fn handle_str(&amp;mut self, byte_offset: usize, size_in_bytes: usize);

    // Similar for other token types.
    ...

    fn handle_error(&amp;mut self, byte_offset: usize);
}</code></pre>
<p>To keep things simple, the event handlers don’t return a <code>bool</code> to stop parsing. It can be added in a few lines of code and it doesn’t affect performance.</p>
<p>Unlike the different types of event parsers from the previous post, implementations of these lexer APIs are almost identical. This is because the lexer has only one state, which is the current position in the input. A <code>next</code> call in the iterator implementation simply continues from the current location in the input, and updates the current location as it reads characters from the input.</p>
<p>The entry points are:</p>
<pre><code>pub fn tokenize_iter&lt;&#39;a&gt;(input: &amp;&#39;a str) -&gt; Lexer&lt;&#39;a&gt; { ... }
  // Lexer implements `Iterator`

pub fn tokenize_push&lt;L: LexerEventListener&gt;(input: &amp;str, listener: &amp;mut L) { ... }

pub fn tokenize_list(input: &amp;str) -&gt; Result&lt;Vec&lt;(usize, Token)&gt; usize&gt; { ... }</code></pre>
<h1 id="combining-with-the-event-parsers">Combining with the event parsers</h1>
<p>We have 3 lexers and 2 event parsers, so 6 combinations in total:</p>
<ol type="1">
<li>tokenize_list + parse_events_iter</li>
<li>tokenize_list + parse_events_push</li>
<li>tokenize_iter + parse_events_iter</li>
<li>tokenize_iter + parse_events_push</li>
<li>tokenize_push + parse_events_iter</li>
<li>tokenize_push + parse_events_push</li>
</ol>
<p>However (5) is not easily possible in Rust. The problem is that a push implementation cannot be converted into an iterator, as it will scan the entire input without ever returning and keep calling the listener methods. To convert a push API into an iterator, we need a language feature that allows us to stop the current thread (or maybe a “fiber”, green thread etc.) and resume it later. In Rust, this is possible with <code>async</code> or threads. Threads are expensive, and <code>async</code> requires a lot of refactoring, and all the call sites to be made <code>async</code> as well.</p>
<p>So in this post we won’t consider this combination.</p>
<h1 id="notes-on-implementations">Notes on implementations</h1>
<p>Implementing these combinations is mostly straightforward. Full code is linked below as usual. The takeaways are:</p>
<ul>
<li>The push API cannot be converted into an iterator API, without language features.</li>
<li>The push API requires state management in the consumer: the consumer will have to save the state that needs to be maintained between the calls to the listener methods.</li>
<li>The iterator API is more flexible as it can be converted into a push API.</li>
<li>The iterator API is also easier to use: the consumer can iterate through the elements in nested loops, and needs less state management. The state can also be function locals, instead of fields of a struct (or class etc.).</li>
<li>The list API (generates an entire vector of tokens) only makes sense when you need to collect all of the tokens in memory. The only use case for this that I’m aware of is incremental parsing.</li>
</ul>
<h1 id="references-and-benchmarks">References and benchmarks</h1>
<p>The code (including benchmarks) is here: <a href="https://github.com/osa1/how-to-parse/tree/main/part2">github.com/osa1/how-to-parse-2</a>.</p>
<p><strong>Token generation benchmarks:</strong> Collect all of the tokens in a <code>Vec</code>.</p>
<ul>
<li>tokenize_list: 305 MB/s</li>
<li>tokenize_push: 303 MB/s</li>
<li>tokenize_iter: 329 MB/s</li>
</ul>
<p>In the event generation benchmarks in the last post, the push implementation is about 10% faster than the iterator. But in the lexer, the iterator is faster when collecting the tokens in a vector. It looks like when the state that the parser manages between the <code>next</code> calls gets simpler, the compiler is able to optimize the code better, and iterator implementation beats the push implementation.</p>
<p>The vector generator and push implementation adding the elements to a vector via the listener perform the same, which shows that when monomorphised, the push implementation optimizes quite well for simple cases (but also in complex cases, as we will see below). In languages without monomorphisation, the push API should be slower.</p>
<p><strong>Tokens to events:</strong> Convert tokens to events.</p>
<ul>
<li>events_iter: 282 MB/s</li>
<li>events_push: 315 MB/s</li>
<li>tokenize_list + events_iter: 181 MB/s</li>
<li>tokenize_list + events_push: 187 MB/s</li>
<li>tokenize_iter + events_iter: 269 MB/s</li>
<li>tokenize_iter + events_push: 275 MB/s</li>
<li>tokenize_push + events_push: 351 MB/s</li>
</ul>
<p>The first two benchmarks are the ones from the previous post that don’t use a lexer, generate events directly. The numbers are slightly different than the numbers from the previous post as I rerun them again.</p>
<p>If you need some kind of incremental implementation, scanning the entire input and collecting the events or tokens in a vector performs bad. There’s no point in combining the list API with push or iterator APIs.</p>
<p>What’s surprising is that the push lexer implementation combined with the push event generator implementation performs better than the event generator implementation that parses the input directly without a lexer. I don’t have an explanation to why, yet.</p>
<p>Lexer iterator implementations combined with any of the event generation implementations perform slower than the event push implementation that parses the input directly, but about as fast as the event iterator implementation that parses the input directly.</p>
<p><strong>Tokens to AST:</strong> Converts tokens to events, builds AST from the events.</p>
<ul>
<li>Recursive descent: 127 MB/s</li>
<li>events_iter to AST: 140 MB/s</li>
<li>events_push to AST: 145 MB/s</li>
<li>tokenize_list + events_iter to AST: 108 MB/s</li>
<li>tokenize_list + events_push to AST: 108 MB/s</li>
<li>tokenize_iter + events_iter to AST: 138 MB/s</li>
<li>tokenize_iter + events_push to AST: 139 MB/s</li>
<li>tokenize_push + events_push to AST: 151 MB/s</li>
</ul>
<p>The first three benchmarks below are from the last post. Rerun and included here for comparison.</p>
<p>When we add an AST building step, which is more complicated compared to the rest of steps, the performance difference between the most convenient implementation (tokenize_iter + events_iter to AST) and the most performant one (tokenize_push + events_push to AST) diminishes. In the event generation benchmark, the fast one is 30% faster, but when building an AST, it’s only 9% faster.</p>
<p>The push implementation is still faster than the recursive descent parser, even with the extra lexing step. I’m planning to investigate this further in a future post.</p>]]></summary>
</entry>
<entry>
    <title>Exploring parsing APIs: what to generate, and how</title>
    <link href="http://osa1.net/posts/2024-11-22-how-to-parse-1.html" />
    <id>http://osa1.net/posts/2024-11-22-how-to-parse-1.html</id>
    <published>2024-11-22T00:00:00Z</published>
    <updated>2024-11-22T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>Consider a simplified and enhanced version of JSON, with these changes:</p>
<ul>
<li>Numbers are 64-bit unsigned integers.</li>
<li>Strings cannot have control and escape characters.</li>
<li>Single-line comments are allowed, with the usual syntax: <code>// ...</code> .</li>
</ul>
<p>When parsing a language like this, a common first step if to define an “abstract syntax tree” (AST), with only the details we want from the parser output.</p>
<p>For example, if we’re implementing a tool like <a href="https://jqlang.github.io/jq/">jq</a>, the AST may look like:</p>
<pre><code>enum Json {
    Int(u64),
    Str(String),
    Bool(bool),
    Array(Vec&lt;Json&gt;),
    Object(Vec&lt;(String, Json)&gt;),
    Null,
}</code></pre>
<p>This type is called an “abstract” syntax tree because it abstracts the unnecessary details from the parse output. In our tool we don’t need locations of nodes and comments, so the AST doesn’t contain them.</p>
<p>It’s easy to implement a parser for this AST: we iterate the input, skip whitespace and comments, then based on the next character decide what type of node (integer, string, etc.) to parse. For nested <code>Json</code> nodes in arrays and objects, we recursively call the parser.</p>
<p>This kind of parser is called a “recursive descent parser”. For our AST above, the parser looks like this:</p>
<pre><code>// The entry point: parses all of the input to JSON.
pub fn parse(input: &amp;str) -&gt; Result&lt;Json, JsonParseError&gt; {
    let mut iter = input.char_indices().peekable();
    let (_, json) = parse_single(&amp;mut iter, input)?;
    skip_trivia(&amp;mut iter)?;
    // Check that all of the input is consumed.
    ...
}

// Parse a single Json. After parsing, the input may have more characters to be parsed.
fn parse_single(
    iter: &amp;mut Peekable&lt;CharIndices&gt;,
    input: &amp;str,
) -&gt; Result&lt;(usize, Json), ParseError&gt; {
    // Skip whitespace and comments.
    skip_trivia(iter)?;

    // Get next character.
    let (byte_offset, char) = match iter.next() { ... }

    if char == &#39;[&#39; {
        // Parse an array. Call `parse_single` recursively for elements.
        ...
    }

    if char == &#39;{&#39; {
        // Parse an object. Call `parse_single` recursively for values.
        ...
    }

    if char == &#39;t&#39; {
        // Parse keyword &quot;true&quot;.
        ...
    }

    // Same for other keywords, integers, strings.
    ...
}</code></pre>
<p>While very common, this kind of parsers are inflexible, and slower than more flexible alternatives for many use cases.</p>
<p>Consider these use cases:</p>
<ul>
<li><p>A JSON formatter: a formatter needs to know about comments to be able to keep them in the formatted code. To support this use case, the AST needs to include comments too, which will make it larger, and parsing will be less efficient for the applications that don’t need comments.</p></li>
<li><p>A configuration file parser for a text editor: to be able to show error locations in configuration errors (such as an invalid value used for a setting), the AST will have to include source locations. Similar to above, this will make the AST larger and slower to parse for other applications that don’t need source locations.</p></li>
<li><p>An RPC server that looks at the command name in incoming JSON messages and relays the messages based on the command name: the server doesn’t even need a full parser, just a parser that can keep track of nesting level so that it can extract the request name field at the right level will suffice. Using a full AST parser will parse the whole message and be inefficient.</p></li>
<li><p>A log sorting tool that reads a file with one JSON log per line, sorts the lines based on top-level “timestamp” field values. Similar to the use case above, this tool only needs to read one field and parsing whole lines is wasteful.</p></li>
</ul>
<p>A well-known solution to these is to introduce a lower level parser that doesn’t generate a fully structured output like an AST, but a stream of “parse events”. These events should be general enough to allow different use cases like the ones we listed above, and should be cheap to allocate and pass around, ideally as stack allocated values, so that applications that don’t need them can skip them efficiently.</p>
<p>This type of parsing is often called “event driven parsing”. In our JSON variant, the events look like this:</p>
<pre><code>/// A parse event, with location of the event in the input.
pub struct ParseEvent {
    pub kind: ParseEventKind,
    pub byte_offset: usize,
}

/// Details of a parse event.
pub enum ParseEventKind {
    StartObject,
    EndObject,
    StartArray,
    EndArray,
    Int(u64),
    Str {
        /// Size of the string, not including the double quotes.
        size_in_bytes: usize,
    },
    Bool(bool),
    Null,
    Comment {
        /// Size of the comment, including the &quot;//&quot; a the beginning and newline at the end.
        size_in_bytes: usize,
    },
}</code></pre>
<p>Note that there’s no heap allocation required for these events. Contents of strings and comments can be obtained by slicing the input using the event location and <code>size_in_bytes</code> field.</p>
<p>When generating these event, it’s important that we don’t scan the whole input and collect all of the events in a list, as that would mean some of the users, like our RPC server and log sorted examples above, would have to do more work than necessary.</p>
<p>This means that the parser will have to be stateful: after returning an event, it needs to be able to continue from the last event location. This complicates the parser implementation quite a bit. Here’s how the parser looks like at a high level:</p>
<pre><code>// The entry point. Use via the `Iterator` interface.
pub fn parse_events(input: &amp;str) -&gt; EventParser {
    EventParser::new(input)
}

// The parser state.
pub struct EventParser&lt;&#39;a&gt; {
    input: &amp;&#39;a str,
    byte_offset: usize,
    container_stack: Vec&lt;Container&gt;,
    state: ParserState,
}

enum Container {
    Array,
    Object,
}

enum ParserState {
    /// Parse any kind of object, update state based on the current container.
    TopLevel,

    /// Finished parsing a top-level object, expect end-of-input.
    Done,

    /// Parsing an object, parse another element on &#39;,&#39;, or finish the array on &#39;}&#39;.
    ObjectExpectComma,

    /// Parsing an object, parse the first element, or finish the array on &#39;]&#39;.
    ObjectExpectKeyValue,

    /// Parsing an object and we&#39;ve just parsed a key, expect &#39;:&#39;.
    ObjectExpectColon,

    /// Parsing an array, parse another element on &#39;,&#39;, or finish the array on &#39;]&#39;.
    ArrayExpectComma,
}

impl&lt;&#39;a&gt; Iterator for EventParser&lt;&#39;a&gt; {
    type Item = Result&lt;ParseEvent, ParseError&gt;;

    fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt; {
        match self.state {
            ParserState::TopLevel =&gt; self.top_level(),
            ParserState::Done =&gt; self.done(),
            ParserState::ObjectExpectComma =&gt; self.object_expect_comma(),
            ParserState::ObjectExpectKeyValue =&gt; self.object_expect_key_value(),
            ParserState::ObjectExpectColon =&gt; self.object_expect_colon(),
            ParserState::ArrayExpectComma =&gt; self.array_expect_comma(),
        }
    }
}

...</code></pre>
<p>The main complexity of this parser comes from the fact that it cannot return an event and keep running, the caller needs to call the relevant method (<code>next</code> from the <code>Iterator</code> trait above) to keep parsing. To be able to continue from where it’s left, the parser needs to maintain some state outside of the parse functions.</p>
<p>This parser is general enough to allow implementing our original AST parser:</p>
<pre><code>pub fn event_to_tree&lt;I: Iterator&lt;Item = Result&lt;ParseEvent, ParseError&gt;&gt;&gt;(
    parser: &amp;mut I,
    input: &amp;str,
) -&gt; Result&lt;Json, ParseError&gt; {
    let mut container_stack: Vec&lt;Container&gt; = vec![];
    let mut current_container: Option&lt;Container&gt; = None;
    let mut parsed_object: Option&lt;Json&gt; = None;

    for event in parser.by_ref() {
        match event {
            ...
        }
    }

    Ok(parsed_object.unwrap())
}</code></pre>
<p>But it also allows parsing to an AST with comments (for our formatter), source locations (for our configuration parser), and our RPC server and log sorter. Here’s how the timestamp parser that stops after finding the field looks like:</p>
<pre><code>/// Parse the &quot;timestamp&quot; field at the top-level map of the JSON.
pub fn parse_timestamp(log_line: &amp;str) -&gt; Result&lt;Option&lt;u64&gt;, ParseError&gt; {
    let mut container_depth: u32 = 0;
    let mut expect_timestamp = false;

    for event in parse_events(log_line) {
        let ParseEvent { kind, byte_offset } = match event {
            Ok(event) =&gt; event,
            Err(err) =&gt; return Err(err),
        };

        let expect_timestamp_ = expect_timestamp;
        expect_timestamp = false;

        match kind {
            ParseEventKind::StartObject =&gt; {
                container_depth += 1;
            }

            ParseEventKind::EndObject =&gt; {
                container_depth -= 1;
            }

            ParseEventKind::StartArray =&gt; {
                if container_depth == 0 {
                    // Array at the top level, the line does not contain the field.
                    return Ok(None);
                }
                container_depth += 1;
            }

            ParseEventKind::EndArray =&gt; {
                container_depth -= 1;
            }

            ParseEventKind::Str { size_in_bytes } =&gt; {
                if container_depth != 1 {
                    continue;
                }
                let str = &amp;log_line[byte_offset..byte_offset + size_in_bytes];
                expect_timestamp = str == &quot;timestamp&quot;;
            }

            ParseEventKind::Int(i) =&gt; {
                if expect_timestamp_ {
                    return Ok(Some(i));
                }
            }

            ParseEventKind::Bool(_)
            | ParseEventKind::Null
            | ParseEventKind::Comment { .. } =&gt; {}
        }
    }

    Ok(None)
}</code></pre>
<p>A nice property of this parser is that it does not allocate at all. It doesn’t build an AST (so no heap-allocated vectors), and parse events are 24-byte stack allocated values. The event parser is also stack allocated by this function.</p>
<p>An alternative design to this that is slightly less flexible and more difficult to use, but easier to implement and faster is what’s sometimes called a “push parser”.</p>
<p>The idea is that, instead of returning one event at a time, the parser takes a “listener” argument, and calls the listener callbacks for each event generated. The listener type directly follows our event type above:</p>
<pre><code>// Methods return a `bool` indicating whether to continue parsing after the event.
pub trait EventListener {
    fn handle_start_object(&amp;mut self, _byte_offset: usize) -&gt; bool {
        true
    }

    fn handle_end_object(&amp;mut self, _byte_offset: usize) -&gt; bool {
        true
    }

    fn handle_start_array(&amp;mut self, _byte_offset: usize) -&gt; bool {
        true
    }

    fn handle_end_array(&amp;mut self, _byte_offset: usize) -&gt; bool {
        true
    }

    fn handle_int(&amp;mut self, _byte_offset: usize, _i: u64) -&gt; bool {
        true
    }

    fn handle_str(&amp;mut self, _byte_offset: usize, _size_in_bytes: usize) -&gt; bool {
        true
    }

    fn handle_bool(&amp;mut self, _byte_offset: usize, _b: bool) -&gt; bool {
        true
    }

    fn handle_null(&amp;mut self, _byte_offset: usize) -&gt; bool {
        true
    }

    fn handle_comment(&amp;mut self, _byte_offset: usize, _size_in_bytes: usize) -&gt; bool {
        true
    }

    fn handle_error(&amp;mut self, _error: ParseError);
}</code></pre>
<p>The parser:</p>
<pre><code>// The entry point. Parse all of the input, call `listener` with the events.
pub fn parse&lt;L: EventListener&gt;(input: &amp;str, listener: &amp;mut L) {
    let mut iter = input.char_indices().peekable();
    let input_size = input.len();

    // Parse a single JSON.
    if !parse_single(&amp;mut iter, input_size, listener) {
        return;
    }

    // Check that all of the input is consumed.
    ...
}

// Returns whether an error was reported.
fn parse_single&lt;L: EventListener&gt;(
    iter: &amp;mut Peekable&lt;CharIndices&gt;,
    input_size: usize,
    listener: &amp;mut L,
) -&gt; bool {
    // Skip whitespace and comments, generate events for comments.
    skip_trivia!(iter, listener);

    // Get next character.
    let (byte_offset, char) = match iter.next() {
        Some(next) =&gt; next,
        None =&gt; {
            listener.handle_error(ParseError {
                byte_offset: input_size,
                reason: &quot;unexpected end of input&quot;,
            });
            return false;
        }
    };

    if char == &#39;[&#39; {
        // Parse an array. Call `parse_single` recursively for elements.
        ...
    }

    if char == &#39;{&#39; {
        // Parse an object. Call `parse_single` recursively for values.
        ...
    }

    if char == &#39;t&#39; {
        // Parse keyword &quot;true&quot;.
        ...
    }

    // Same for other keywords, integers, strings.
    ...
}</code></pre>
<p>Note that the parser functions are identical (in terms of names and what they do) to our simple recursive descent parser. This is because the parser no longer needs to maintain state to be able to return and continue from where it was left, as it does all of the work in one go. Instead of building an AST or a list of events, it takes an <code>EventListener</code> argument and calls the handle methods.</p>
<p>This is a bit less convenient to use, but it’s still flexible enough to build an AST. An <code>EventListener</code> implementation that builds up a <code>Json</code> AST looks like this:</p>
<pre><code>pub struct AstBuilderListener&lt;&#39;a&gt; {
    input: &amp;&#39;a str,
    container_stack: Vec&lt;Container&gt;,
    current_container: Option&lt;Container&gt;,
    parsed_object: Option&lt;Json&gt;,
    error: Option&lt;ParseError&gt;,
}

impl&lt;&#39;a&gt; EventListener for AstBuilderListener&lt;&#39;a&gt; {
    ...
}</code></pre>
<p>However, if you need to be able to stop parsing and continue later, this parser can’t do that.</p>
<p>The main advantage of this parser is that, with the right programming language and parser design, it can be faster than the alternatives, while still being flexible enough for most use cases. See below for benchmarks.</p>
<hr />
<h1 id="aside-event-parsing-vs.-lexing">Aside: event parsing vs. lexing</h1>
<p>Our <code>ParseEvent</code> type has no nested data and looks like what we could define as the “tokens” in a parser for a programming language.</p>
<p>So it shouldn’t be surprising that we can use a lexer generator to implement a parse event generator:</p>
<pre><code>// Same `parse_events` as above, but uses a generated lexer.
pub fn parse_events(input: &amp;str) -&gt; LexgenIteratorAdapter {
    LexgenIteratorAdapter {
        lexer: Lexer::new(input),
    }
}

// An adapter is necessary to convert lexgen values to `parse_events` items.
pub struct LexgenIteratorAdapter&lt;&#39;a&gt; {
    lexer: Lexer&lt;&#39;a, std::str::Chars&lt;&#39;a&gt;&gt;,
}

impl&lt;&#39;a&gt; Iterator for LexgenIteratorAdapter&lt;&#39;a&gt; {
    type Item = Result&lt;ParseEvent, ParseError&gt;;

    fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt; {
        ...
    }
}

struct LexerState {
    container_stack: Vec&lt;Container&gt;,
}

lexgen::lexer! {
    Lexer(LexerState) -&gt; ParseEvent;

    type Error = &amp;&#39;static str;

    let comment = &quot;//&quot; (_ # &#39;\n&#39;)* &#39;\n&#39;;

    rule Init {
        $$ascii_whitespace,

        $comment =&gt; comment,

        &#39;[&#39; =&gt; ...,

        &#39;]&#39; =&gt; ...,

        &#39;{&#39; =&gt; ...,

        &quot;true&quot; =&gt; ...,

        &quot;false&quot; =&gt; ...,

        &quot;null&quot; =&gt; ...,

        [&#39;0&#39;-&#39;9&#39;]+ =&gt; ...,

        &#39;&quot;&#39; (_ # &#39;&quot;&#39;)* &#39;&quot;&#39; =&gt; ...
    }

    rule Done { ... }

    rule ArrayExpectComma { ... }

    rule ObjectExpectKeyValue { ... }

    rule ObjectExpectColon { ... }

    rule ObjectExpectComma { ... }
}</code></pre>
<p>This uses <a href="https://github.com/osa1/lexgen">lexgen</a>. lexgen generates slightly different values than what we want, so we have a <code>map</code> in the entry point to convert the lexgen values.</p>
<p>The main difference between an event parser and lexer is that an event parser maintains some of the structure of the parsed format. For example, we check that brackets are balanced, after a key in a map a colon follows, and so on.</p>
<p>A lexer generator can be used to implement an event parser, as demonstrated above.</p>
<hr />
<h1 id="references-and-benchmarks">References and benchmarks</h1>
<p>All of the code in this blog post, and more, is here: <a href="https://github.com/osa1/how-to-parse/tree/main/part1">github.com/osa1/how-to-parse</a>.</p>
<p>In the benchmark program (run with <code>cargo bench</code>), we generate a 10M large JSON, and parse it to either an AST or a vector of events.</p>
<p><strong>AST building benchmarks:</strong></p>
<ul>
<li><p>Recursive descent: the recursive descent parser that generates an AST.</p>
<p>Throughput: 128 Mb/s.</p></li>
<li><p>Event generator to AST: the iterator-style event generator, events processed by <code>event_to_tree</code> to build an AST.</p>
<p>Throughput: 138 Mb/s.</p></li>
<li><p>Lexgen event to AST: same as above, but the event parser is implemented with lexgen.</p>
<p>Throughput: 106 Mb/s.</p></li>
<li><p>Push event parser to AST: the “push” event parser, <code>AstBuilderListener</code> as the event listener.</p>
<p>Throughput: 147 Mb/s.</p></li>
</ul>
<p><strong>Event generation benchmarks:</strong> (collect events in a <code>Vec</code>)</p>
<ul>
<li><p>Parse events: the iterator-style event generator.</p>
<p>Throughput: 274 Mb/s.</p></li>
<li><p>Parse events lexgen: the lexgen-generated event generator.</p>
<p>Throughput: 179 Mb/s.</p></li>
<li><p>Parse events via push: the push event parser, events added to a <code>Vec</code> via by the listener.</p>
<p>Throughput: 304 Mb/s.</p></li>
</ul>
<p><strong>Notes:</strong></p>
<ul>
<li><p>lexgen-generated event parser is the slowest, but I think it should be possible to make it perform at least as good as the hand-written one. So far I’ve spent very little time to optimize lexgen’s code generator.</p></li>
<li><p>Push-based implementation is faster than the iterator-style implementation, both for generating events in a list, and also for building an AST.</p>
<p>The main advantage of the push-based implementation is that the control flow is as simple as the recursive descent parsing (contained within parse functions, as opposed to externally in a struct), as it does all of the parsing in one go. It looks like managing the parser state externally in a struct is not free.</p></li>
<li><p>I think the tradeoffs between the push-based and iterator implementations will be different in most high-level languages without control over allocations and monomorphisation.</p>
<ul>
<li><p>In the Rust implementation, events are stack allocated values, which will be heap-allocated objects in some of the other languages.</p></li>
<li><p>In the push-based implementation, the parser is monomorphised based on the listener type. Both the listener and parser are stack allocated. All event handler method calls are direct calls (as opposed to virtual, or via some other dynamic invocation method), which can be inlined. None of these will be the case in, e.g., Haskell and Dart.</p></li>
</ul>
<p>It would be interesting to implement the same in some other languages to see how they perform relative to each other.</p></li>
<li><p>I’m not sure why the recursive descent parser is not at least as fast as the push-based implementation, and not faster than the iterator-style one. If you have any insights into this, please let me know.</p></li>
</ul>
<h1 id="more-use-cases">More use cases</h1>
<p>The use cases described at the beginning of the post are all extracted from real-world use cases of various other formats.</p>
<p>Here are more use cases that require flexible and fast parser design:</p>
<ul>
<li><p>“Outline” views in text editors or online code browsing tools may want to process top-level definitions, and definitions nested in <code>class</code>, <code>impl</code>, and similar blocks. Parsing the whole file to an AST would be inefficient.</p></li>
<li><p>Syntax-aware code search tools like <a href="https://github.com/osa1/sg">sg</a> can implement searching only in identifiers, string literals, comments with an event-based parser. This could also be implemented with a lexer.</p></li>
<li><p>As mentioned in a <a href="https://osa1.net/posts/2024-11-04-resumable-exceptions.html">previous post</a>, ideally a formatter, language server, compiler, and refactoring tools, should reuse as much parsing code as possible. It’s difficult to do this with an AST parser, as the AST would have too much information for each of these tools. Event-based parsing makes this easier.</p></li>
</ul>
<h1 id="event-parsing-examples-from-programming-languages">Event parsing examples from programming languages</h1>
<p>I think event-driven parsing is common in some languages when parsing data formats like XML, but less common for parsing programming languages. Two examples that I’m aware of that applies the ideas to programming languages:</p>
<ul>
<li><p>rust-analyzer’s parser is <a href="https://github.com/rust-lang/rust-analyzer/blob/c0bbbb3e5d7d1d1d60308c8270bfd5b250032bb4/docs/dev/architecture.md#cratesparser">a hand written one that generates events</a>. The architecture documentation mentions that Kotlin uses a similar idea:</p>
<blockquote>
<p>It is a hand-written recursive descent parser, which produces a sequence of events like “start node X”, “finish node Y”. It works similarly to kotlin’s parser, which is a good source of inspiration for dealing with syntax errors and incomplete input</p>
</blockquote></li>
<li><p>Dart’s parser <a href="https://github.com/dart-lang/sdk/blob/19da943583e020e96026f797904dc5c6b993d4ac/pkg/_fe_analyzer_shared/lib/src/parser/listener.dart#L35-L46">uses the push-based API</a>. This parser is the only Dart language parser used by the SDK. It’s used by the analyzer, language server, compilers, and anything else that the SDK includes.</p></li>
</ul>]]></summary>
</entry>
<entry>
    <title>Resumable exceptions</title>
    <link href="http://osa1.net/posts/2024-11-04-resumable-exceptions.html" />
    <id>http://osa1.net/posts/2024-11-04-resumable-exceptions.html</id>
    <published>2024-11-04T00:00:00Z</published>
    <updated>2024-11-04T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>The main use case of resumable exceptions would be collecting a bunch of errors (instead of bailing out after the first one) to log or show to the user, or actually recovering and continuing from the point of error detection, rather than in a call site.</p>
<p><em>Why not design the code to allow error recovery, instead of using a language feature?</em> There are a few problems with this:</p>
<ul>
<li><p>Without a language feature to do this, libraries will have to implement their own ways to recover from errors, causing inconsistencies and fragmented ecosystem of error handling libraries.</p></li>
<li><p>With resumable exceptions, any code can be trivially made to transfer control to a exception handler, and back. Manually refactoring code to do the same can be a big task. This may even be infeasible.</p></li>
<li><p>With resumable exceptions as a part of the language, libraries will be designed with resumption in mind. Libraries that would normally not allow error recovery will allow error recovery, as it will be easy to do, and it will be a common thing to resume from errors.</p></li>
</ul>
<h1 id="example-use-case-parser-shared-by-a-compiler-and-a-language-server">Example use case: parser shared by a compiler and a language server</h1>
<p>Modern programming languages have complex syntax. Parsers for these languages are often thousands of lines of code.</p>
<p>Ideally, all tooling for a language would share the parser, as it’s a significant amount of work to implement, debug, maintain parsers for such large languages.</p>
<p>However not all of these tools will have the same error handling behavior. A compiler <em>cannot</em> continue in the presence of a parse error, but a language server <em>has to</em> continue.</p>
<p>With resumable exceptions, the compiler can abort after a parse error, the language server can provide placeholder AST nodes for failing parse operations and resume. This flexibility does not make the parser API any more complicated than a parser that throws an exception in any other language. A one-off refactoring script that uses the parser library doesn’t have to deal with error recovery just because the language server, which uses the same parser, needs to recover from parse errors and continue parsing.</p>
<h1 id="types-of-resumable-exceptions">Types of resumable exceptions</h1>
<p>With resumable exceptions <code>throw</code> expressions generate a value. The value depends on the exception type thrown. For example, a <code>FooDecodingException</code> can be resumed with a value of <code>Foo</code> provided by the handler.</p>
<p>This can be implemented with an abstract base class or typeclass/trait with a type parameter:</p>
<pre><code>// in a system with classes:
abstract class ResumableException&lt;Resume&gt; { 
    prim Resume throw();
    prim Never resume(Resume resumptionValue);
}

// or in a system with typeclasses/traits:
trait ResumableException&lt;Resume&gt; {
    prim fn throw(self) -&gt; Resume;
    prim fn resume(resumptionValue: Resume) -&gt; !;
}</code></pre>
<p>Here the <code>prim</code> keyword indicates that the <code>throw</code> and <code>resume</code> methods are provided by the compiler.</p>
<p><code>throw e</code> can then be type checked as <code>e.throw()</code>, and <code>resume e with value</code> can be type checked as <code>e.resume(value)</code>. Or we can use the function call syntax instead of special syntax for throwing and resuming.</p>
<p>Whether to make exceptions thrown by a function a part of its type signature or not is an orthogonal concern.</p>
<h1 id="exception-type-design-considerations">Exception type design considerations</h1>
<p>The same considerations when designing non-resumable exceptions apply to resumable exceptions:</p>
<ul>
<li>The more general an exception type gets, the less you can do with it.</li>
<li>We still want to distinguish “log and stop” kind of exceptions from recoverable ones.</li>
</ul>
<p>For example, it doesn’t make sense to resume from an <a href="https://api.dart.dev/stable/3.5.4/dart-core/ArgumentError-class.html"><code>ArgumentError</code></a>, so we don’t implement <code>ResumableException</code> for it.</p>
<p>To be able to meaningfully resume from an exception, the exception type should document when exactly it is thrown, or have a resumption value type that is specific enough to give an idea on when it is thrown.</p>
<p>For example, an exception that can be resumed with an <code>int</code> cannot be resumed without knowing what that <code>int</code> is going to be used for, so this should be documented. But an exception <code>FooDecodingError implements ResumableException&lt;Foo&gt;</code> makes it clear that it’s thrown when there’s an error during decoding a <code>Foo</code>, and the resumption value is the value to be used as the <code>Foo</code> being decoded.</p>]]></summary>
</entry>
<entry>
    <title>Idea: a more structural code editor</title>
    <link href="http://osa1.net/posts/2024-11-02-structural-editor.html" />
    <id>http://osa1.net/posts/2024-11-02-structural-editor.html</id>
    <published>2024-11-02T00:00:00Z</published>
    <updated>2024-11-02T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>Code is tree structured, but manipulated as a sequence of characters.</p>
<p>Most language tools<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> need to convert these sequence of characters to the tree form as the first thing to be able to do anything.</p>
<p>When the program is being edited, the tree structure is often broken, and often to the point where the tool cannot operate.</p>
<p>For example:</p>
<ul>
<li>An opening parenthesis, brace, or bracket, without a matching closing one</li>
<li>An unterminated string literal or multi-line comment</li>
<li>A keyword inserted at a wrong place, or without the right tokens afterwards</li>
</ul>
<p>These can make it impossible to main the tree structure of the code.</p>
<p>Since these cases are common, tools need to deal with these. A lot of time and effort is spent on error recovery so that when one of these common cases occur, the tool can still operate and do something useful.</p>
<p>For some tools handling these cases is a requirement: many of the language server functions need to work even when the code is being edited and not in a valid state. For example, “go to definition” should work, “outline” shouldn’t be reset every time the user inserts an opening brace, bracket, or parenthesis.</p>
<p><em>We can’t invent a new language</em> to solve this problem: this creates a thousand new problems, each bigger than the one we are trying to solve. Designing and implementing a new language is major undertaking on its own. We can’t design and implement a language <em>and</em> an experimental code editor at the same time, and succeed in both.</p>
<p>So we want need to support existing languages, <em>but existing languages are incredibly complex</em>, sometimes with a hundred kinds of statements, expressions, types, and so on.</p>
<p>What I’d like to propose as a solution is a “mostly structural” editor, where programs are edited in a structural way at the highest levels, but as text at the statement and expression level.</p>
<p>The details depend on the language. As an example, let’s consider Rust. In Rust, packages (called “crates”), modules, and the items in modules (function and type definitions) can be defined structurally, because there aren’t a lot of different kinds of top-level declarations. Then in the function (and method) bodies, we write the code as text, as usual.</p>
<p>The advantages of this approach are:</p>
<ul>
<li><p>We avoid inventing a new language. The idea can be applied to most languages.</p></li>
<li><p>Because we isolate invalid syntax to function bodies, no edit can cause syntax errors in the other functions in the same module, or in the other modules and packages.</p></li>
<li><p>Because we define function and method signatures separately from function/method bodies, syntax errors cannot invalidate types and cannot generate type errors outside of the function being edited.</p></li>
<li><p>For the same reason as above, “outline” view in the IDE is never broken. Functions like “go to definition” and “find references” always work.</p></li>
</ul>
<p>As for the GUI part, I imagine an editor “pane” for each function being edited. I should be able to quickly switch between functions (maybe with a fuzzy search similar to <code>ctrl-p</code> in some editors), and when working on a function I should be able to quickly open documentation or definitions of the symbols used in the function, in new panes. I imagine there will be a lot of panes open at any time. This may require a solution like a tiling window manager to quickly arrange them and switch between them.</p>
<p>This problem is not new, I do a lot of buffer/split management every day while coding, and almost never use just a single editor window. However with each pane editing just one function, there will be a lot of splits and panes. Some creativity will be needed here to make managing these panes easy for the users.</p>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>I’m not aware of any language tool that doesn’t need to parse the source. Please let me know if you know such tools.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></summary>
</entry>
<entry>
    <title>Subtyping and subsumption</title>
    <link href="http://osa1.net/posts/2024-10-21-subtyping-subsumption.html" />
    <id>http://osa1.net/posts/2024-10-21-subtyping-subsumption.html</id>
    <published>2024-10-21T00:00:00Z</published>
    <updated>2024-10-21T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>Subtyping is a relation between two types. It often comes with a typing rule called “subsumption”, which says that if type B is a subtype of type A (usually shown as <code>B &lt;: A</code>), then a value of type B can be assumed to have type A.</p>
<p>The crucial part is that subsumption is <em>implicit</em>, the programmer doesn’t explicitly cast the value with type <code>B</code> to type <code>A</code>.</p>
<p>When we make an operation implicit in a language, we need to make sure that it is (1) safe (2) performant. Users will be doing it without realizing, and we don’t want to accidentally break things or make them slow.</p>
<p>Let’s consider how we can make subsumption safe and performant.</p>
<h1 id="safety-of-subsumption">Safety of subsumption</h1>
<p>Different languages give different safety guarantees. High-level languages often guarantee:</p>
<ol type="1">
<li><p>Memory safety: a memory read or write shouldn’t cause undefined behavior.</p>
<p>Examples: out-of-bounds array accesses should be caught, dangling pointers shouldn’t be allowed or dereferencing them should be caught in runtime.</p></li>
<li><p>Type safety: static guarantees of the language’s type system should be uphold.</p>
<p>Example: if I have a function <code>f : A -&gt; B</code> and a value <code>x : A</code> after subsumption, <code>f(x)</code> shouldn’t fail in compile time or runtime.</p></li>
</ol>
<p>There could be different safeties that the language guarantees. Some of those safeties may also be checked in runtime instead of compile time.</p>
<p>Whatever safeties the language guarantees, they must be preserved with subsumption.</p>
<p>From a programmer’s perspective however, these are not enough to make sure that the program will work as before when subsumption is used. If I can pass a value of type <code>B</code> where <code>A</code> is expected, I need to make sure <code>B</code>, when used as <code>A</code>, acts like <code>A</code>.</p>
<p>This is called “behavioral subtyping” (or “substitutability”), and it depends on not the types of <code>A</code>’s operations but the observable behaviors of <code>A</code> and its subtypes.</p>
<p>I don’t have a good real-world example of this, but you can imagine two types with the same public APIs that work differently. Since the public APIs are the same one can be made subtype of the other and (1) and (2) would still be satisfied, but doing that would cause bugs when one is accidentally passed as the other.</p>
<h1 id="performance-of-subsumption">Performance of subsumption</h1>
<p>Definition of “fast” or “performant” also depends on the language. A C++ programmer’s fast and Python programmer’s fast are often not the same.</p>
<p>However in general, heap allocation should be avoided.</p>
<p>Object-oriented languages (as defined in my <a href="https://osa1.net/posts/2024-10-09-oop-good.html">previous post</a>) without multiple inheritance can often implement subsumption of reference values as no-op, i.e. values of type <code>B</code> work as <code>A</code> in runtime without any changes or copying.</p>
<p>Multiple inheritance makes things more complicated, but a reference to an object can still be converted to a reference of one of its supertypes by just <a href="https://people.montefiore.uliege.be/declercq/INFO0004/documents/vtable.html">adjusting the pointer value</a>.</p>
<p>With unboxed/value types, conceptually, the value needs to be copied as its supertype, but that operation is often no-op. Consider an unboxed record <code>(x: Int, y: Int, z: Int)</code> that we store in a variable <code>a</code>. In runtime, <code>a</code> actually holds multiple stack locations or registers. When we copy it as <code>let b: (x: Int, y: Int) = a</code>, we don’t have to allocate new stack locations for <code>b.x</code> and <code>b.y</code>, we just map those locations to the same locations as <code>a.x</code> and <code>a.y</code>. When we pass <code>b</code> to a function, we pass <code>a.x</code> and <code>a.y</code>.</p>
<p>Where copying becomes a requirement and prohibitive is when you have something like <code>ReadOnlyList&lt;(x: Int, y: Int, z: Int)&gt;</code> and want to upcast it to <code>ReadOnlyList&lt;(x: Int, y: Int)&gt;</code> (the records are unboxed). From the safety perspective this operation is fine, but you have to allocate a new list and copy all the values.</p>
<p>I think this is rarely a problem in practice though, because most generic types, like <code>List&lt;T&gt;</code>, end up being invariant in <code>T</code> anyway, because their API often uses <code>T</code> in both covariant and contravariant positions. So <code>List&lt;(x: Int, y: Int)&gt;</code> is not a supertype of <code>List&lt;(x: Int, y: Int, z: Int)&gt;</code> and subsumption does not apply.</p>
<h1 id="no-conclusions-this-time">No conclusions this time</h1>
<p>In this short post I just wanted to give some definitions that I’m hoping to refer to in future posts.</p>]]></summary>
</entry>
<entry>
    <title>OOP is not that bad, actually</title>
    <link href="http://osa1.net/posts/2024-10-09-oop-good.html" />
    <id>http://osa1.net/posts/2024-10-09-oop-good.html</id>
    <published>2024-10-09T00:00:00Z</published>
    <updated>2024-10-09T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>OOP is certainly not my favorite paradigm, but I think mainstream statically-typed OOP does a few things right that are very important for programming with many people, over long periods of time.</p>
<p>In this post I want to explain what I think is the most important one of these things that the mainstream statically-typed OOP languages do well.</p>
<p>I will then compare the OOP code with Haskell, to try to make the point that OOP is not as bad in everything as some functional programmers seem to think.</p>
<h1 id="what-even-is-oop">What even is OOP?</h1>
<p>In this post I use the word “OOP” to mean programming in statically-typed language with:</p>
<ol type="1">
<li>Classes, that combine state and methods that can modify the state.</li>
<li>Inheritance, which allows classes to reuse state and methods of other classes.</li>
<li>Subtyping, where if a type <code>B</code> implements the public interface of type <code>A</code>, values of type <code>B</code> can be passed as <code>A</code>.</li>
<li>Virtual calls, where receiver class of a method call is not determined by the static type of the receiver but its runtime type.</li>
</ol>
<p>Examples of OO languages according to this definition: C++, Java, C#, Dart.</p>
<h1 id="an-example-of-what-this-allows">An example of what this allows</h1>
<p>This set of features allows a simple and convenient way of developing composable libraries, and extending the libraries with new functionality in a backwards compatible way.</p>
<p>It’s probably best explained with an example. Suppose we have a simple logger library:</p>
<pre><code>class Logger {
  // Private constructor: initializes state, returns an instance of `Logger`.
  Logger._();

  // Public factory: can return `Logger` or any of the subtypes.
  factory Logger() =&gt; Logger._();

  void log(String message, Severity severity) { /* ... */ }
}

enum Severity {
  Info,
  Error,
  Fatal,
}</code></pre>
<p>and another library that does some database stuff:</p>
<pre><code>class DatabaseHandle {
  /* ... */
}</code></pre>
<p>and an application that uses both:</p>
<pre><code>class MyApp {
  final Logger _logger;
  final DatabaseHandle _dbHandle;

  MyApp()
      : _logger = Logger(),
        _dbHandle = DatabaseHandle(...);
}</code></pre>
<p>As is usually the case, things that make network connections, change shared state etc. need to be mocked, faked, or stubbed to be able to test applications. We may also want to extend the libraries with new functionality. With the features that we have, we don’t have to see this coming and prepare the types based on this.</p>
<p>In the first iteration we might just add a concrete class that is just the copy of the current class, and make the current class abstract:</p>
<pre><code>// The class is now abstract.
abstract class Logger {
  // Public factory now returns an instance of a concrete subtype.
  factory Logger() =&gt; _SimpleLogger();

  Logger._();

  // `log` is now abstract.
  void log(String message, Severity severity);
}

class _SimpleLogger extends Logger {
  factory _SimpleLogger() =&gt; _SimpleLogger._();

  _SimpleLogger._() : super._() {/* ... */}

  @override
  void log(String message, Severity severity) {/* ... */}
}</code></pre>
<p>This change is backwards compatible, requires no changes in user code.</p>
<p>Now we might add more implementations, e.g. for ignoring log messages:</p>
<pre><code>abstract class Logger {
  factory Logger() =&gt; _SimpleLogger();

  // New.
  factory Logger.ignoring() =&gt; _IgnoringLogger();

  Logger._();

  void log(String message, Severity severity);
}

class _IgnoringLogger extends Logger {
  factory _IgnoringLogger() =&gt; _IgnoringLogger._();

  _IgnoringLogger._() : super._() {}

  @override
  void log(String message, Severity severity) {}
}</code></pre>
<p>Similarly we can add a logger that logs to a file, to a DB, etc.</p>
<p>We can do the same for the database handle class, but for mocking, faking, or stubbing, in tests.</p>
<p>To be able to use these new subtypes in our app, we implement a factory, or add a constructor to allow passing a logger and a db handle:</p>
<pre><code>class MyApp {
  final Logger _logger;
  final DatabaseHandle _dbHandle;

  MyApp()
      : _logger = Logger(),
        _dbHandle = DatabaseHandle();

  MyApp.withLoggerAndDb(this._logger, this._dbHandle);
}</code></pre>
<p>Note that we did not have to change any types, or add type parameters. Any methods of <code>MyApp</code> that use the <code>_logger</code> and <code>_dbHandle</code> fields do not have to know about the changes.</p>
<p>Now suppose one of the <code>DatabaseHandle</code> implementations also start using the logger library:</p>
<pre><code>abstract class DatabaseHandle {
  factory DatabaseHandle.withLogger(Logger logger) =&gt;
      _LoggingDatabaseHandle._(logger);

  factory DatabaseHandle() =&gt; _LoggingDatabaseHandle._(Logger.ignoring());

  DatabaseHandle._();

  /* ... */
}

class _LoggingDatabaseHandle extends DatabaseHandle {
  final Logger _logger;

  _LoggingDatabaseHandle._(this._logger) : super._();

  /* ... */
}</code></pre>
<p>In our app, we might test by disabling logging in the db library, but start logging db operations in production:</p>
<pre><code>class MyApp {
  // New
  MyApp.testingSetup()
      : _logger = Logger(),
        _dbHandle = DatabaseHandle.withLogger(Logger.ignoring());

  // Updated to start using the logging feature of the DB library.
  MyApp()
      : _logger = Logger(),
        _dbHandle = DatabaseHandle.withLogger(Logger.toFile(...));

  /* ... */
}</code></pre>
<p>As an example that adds more state to the types, we can add a logger implementation that only logs messages above certain severity:</p>
<pre><code>class _LogAboveSeverity extends _SimpleLogger {
  // Only logs messages with this severity or more severe.
  final Severity _severity;

  _LogAboveSeverity(this._severity) : super._();

  @override
  void log(String message, Severity severity) { /* ... */ }
}</code></pre>
<p>We can add another factory to the <code>Logger</code> abstract class that returns this type, or we can even implement this in another library:</p>
<pre><code>// Implemented in another library, not in `Logger`&#39;s library.
class LogAboveSeverity implements Logger {
  // Only logs messages with this severity or more severe.
  final Severity _severity;

  final Logger _logger;

  LogAboveSeverity(this._severity) : _logger = Logger();

  LogAboveSeverity.withLogger(this._severity, this._logger);

  @override
  void log(String message, Severity severity) { /* ... */ }
}</code></pre>
<p>As a final example to demonstrate adding more operations (rather than more state), we can have a logger that logs to a file, with a <code>flush</code> operation:</p>
<pre><code>class FileLogger implements Logger {
  final File _file;

  FileLogger(this._file);

  @override
  void log(String message, Severity severity) {/* ... */}

  void flush() {/* ... */}
}</code></pre>
<p>In summary:</p>
<ul>
<li>We started with a simple logging and database library and wrote an app.</li>
<li>We added more capabilities to the logging and database libraries for testing and also in production use. In particular, we added:
<ul>
<li>New functionality to the logger library, to disable logging, or logging to a file.</li>
<li>A new dependency to the database library for logging database operations. We also allowed the users to override the default logger used.</li>
</ul></li>
</ul>
<p>Crucially, we didn’t have to change any types while doing these changes, and the new code is still as type safe as before.</p>
<p>The logger and database libraries evolved in a completely backwards compatible way.</p>
<p>Since none of the types used in our application changed, <code>MyApp</code> methods didn’t have to change at all.</p>
<p>When we decide to take advantage of the new functionality, we updated only how we construct the logger and db handle instances in our app. Rest of the app didn’t change.</p>
<p>Now let’s consider how something like this could be done in Haskell.</p>
<h1 id="attempting-it-in-haskell">Attempting it in Haskell</h1>
<p>Immediately at the start, we have a few choices on how to represent it.</p>
<p><strong>Option 1:</strong> An ADT, with callback fields to be able to add different types of loggers later:</p>
<pre><code>data Logger = MkLogger
    { _log :: Message -&gt; Severity -&gt; IO ()
    }

simpleLogger :: IO Logger

data Severity = Info | Error | Fatal
    deriving (Eq, Ord)

log :: Logger -&gt; String -&gt; Severity -&gt; IO ()</code></pre>
<p>In this representation, extra state like the minimum severity level in our <code>_LogAboveSeverity</code> is not added to the type, but captured by the closures:</p>
<pre><code>logAboveSeverity :: Severity -&gt; IO Logger
logAboveSeverity minSeverity = MkLogger
    { _log = \message severity -&gt; if severity &gt;= minSeverity then ... else pure ()
    }</code></pre>
<p>If we need to update some of the state shared by the closures, the state needs to be stored in some kind of reference type like <code>IORef</code>.</p>
<p>Similar to the OOP code, the <code>FileLogger</code> needs to be a separate type:</p>
<pre><code>data FileLogger = MkFileLogger
  { _logger :: Logger   -- callbacks capture the file descriptor/buffer and write to it
  , _flush  :: IO ()    -- similarly captures the file descriptor/buffer, flushes it
  }

logFileLogger :: FileLogger -&gt; String -&gt; Severity -&gt; IO ()
logFileLogger = log . _logger</code></pre>
<p>However, unlike our OOP example, existing code that uses the <code>Logger</code> type and <code>log</code> function cannot work with this new type. There needs to be some refactoring, and how the user code will need to be refactored depends on how we want to expose this new type to the users.</p>
<p><strong>Option 2:</strong> A typeclass that we can implement for our concrete logger types:</p>
<pre><code>class Logger a where
    log :: a -&gt; String -&gt; Severity -&gt; IO ()

data SimpleLogger = MkSimpleLogger { ... }

simpleLogger :: IO SimpleLogger
simpleLogger = ...

instance Logger SimpleLogger where
  log = ...</code></pre>
<p>To allow backwards-compatible changes in the logger library, we need to hide the concrete logger class:</p>
<pre><code>module Logger
    ( Logger
    , simpleLogger -- I can export this without exporting its return type
    ) where

...</code></pre>
<p>With this module, we have to either add a type parameter to the functions and other types that use <code>Logger</code>, or use existentials.</p>
<p>Adding a type parameter is not a backwards compatible change, and in general it can cause snowball effect of propagating the type parameter to the direct users, and then their users, and so on, creating a massive change and difficult to use types.</p>
<p>The problem with existentials is that they are limited in how you can use them, and are somewhat strange in some areas. In our application we can do this:</p>
<pre><code>data MyApp = forall a . Logger a =&gt; MkMyApp
  { _logger :: a
  }</code></pre>
<p>But we can’t have a local variable with this existential type:</p>
<pre><code>createMyApp :: IO MyApp
createMyApp = do
  -- I can&#39;t add a type annotation to myLogger without the concrete type
  myLogger &lt;- simpleLogger      -- simpleLogger :: IO SimpleLogger
  return MkMyApp { _logger = myLogger }</code></pre>
<p>I also cannot have an existential type in a function argument:</p>
<pre><code>-- The type signature is accepted by the compiler, but the value cannot be used.
doStuffWithLogging :: (forall a . Logger a =&gt; a) -&gt; IO ()
doStuffWithLogging logger = log logger &quot;test&quot; Info -- some obscure type error</code></pre>
<p>Instead we have to “pack” the logger value with its typeclass dictionary in a new type:</p>
<pre><code>data LoggerBox = forall a . Logger a =&gt; LoggerBox a

doStuffWithLogging :: LoggerBox -&gt; IO ()
doStuffWithLogging (LoggerBox logger) = log logger &quot;test&quot; Info</code></pre>
<p>Other problems and limitations of this approach:</p>
<ul>
<li>The syntax is just awful to the point where it’s deterrent: <code>forall a . Logger a =&gt; ... a ...</code> instead of just <code>Logger</code>.</li>
<li>It allows implementing <code>FileLogger</code>, but
<ul>
<li>All subtypes need to be a new typeclass + an implementation (in OOP: just one class).</li>
<li>This cannot be used for safe downcasting of a <code>Logger</code> value to <code>FileLogger</code>, without knowing the concrete type of the <code>FileLogger</code>.</li>
</ul></li>
</ul>
<h1 id="effect-monad-approach">Effect monad approach</h1>
<p>The effect monad approach is a variation of option (2) without existentials. Instead of</p>
<pre><code>class Logger a where
    log :: a -&gt; String -&gt; Severity -&gt; IO ()</code></pre>
<p>We add the ability to log in a monad type parameter:</p>
<pre><code>class MonadLogger m where
    log :: String -&gt; Severity -&gt; m ()</code></pre>
<p>Then provide a “monad transformer” for each of the logger implementations:</p>
<pre><code>newtype SimpleLoggerT m a = SimpleLoggerT { runSimpleLoggerT :: m a }

instance MonadIO m =&gt; MonadLogger (SimpleLoggerT m) where
  log msg sev = SimpleLoggerT { runSimpleLoggerT = liftIO (logStdout msg sev) }

newtype FileLoggerT m a = FileLoggerT { runFileLoggerT :: Handle -&gt; m a }

instance MonadIO m =&gt; MonadLogger (FileLoggerT m) where
  log msg sev = FileLoggerT { runFileLoggerT = \handle -&gt; liftIO (logFile handle msg sev) }</code></pre>
<p>The database library does the same, and the app combines these together:</p>
<pre><code>newtype MyAppMonad a = ...

instance MonadLogger MyAppMonad where ...

instance MonadDb MyAppMonad where ...</code></pre>
<p>Because we have one type parameter that encapsulates all side effects (instead of one for logging, one for database operations), this avoids the issues with snowballed type parameters in the use sites.</p>
<p>The database library can also add a logger dependency without breaking the user code.</p>
<p>I think this is the best we can get in Haskell, and it’s quite similar to our OOP solution in terms of code changes needed to be done in the user code.</p>
<p>However for this to work the entire ecosystem of libraries need to do things this way. If the database library decides to use the ADT approach, we will need an “adapter”, e.g. a monad typeclass for the DB operations, with a concrete monad transformer type to call the DB library functions.</p>
<p>This is also the main problem with the composable effects libraries.</p>
<p>(There are also issues with how this kind of code performs in runtime, but that’s probably a topic for another blog post.)</p>
<h1 id="composable-effects">Composable effects</h1>
<p>Haskellers have been developing various ways of modelling side effects (such as DB operations, logging) as “effects” and various ways of composing them.</p>
<p>A simple and widespread way of doing this is via the effect monads, as we’ve seen in the previous section.</p>
<p>However these systems have a few drawbacks, compared to our OOP solution:</p>
<ul>
<li><p>Different effect libraries generally don’t work together. For example, <a href="https://hackage.haskell.org/package/mtl">mtl</a> and <a href="https://github.com/hasura/eff">eff</a> functions won’t work together without some kind of adapter turning one into the other.</p></li>
<li><p>Even if the entire Haskell ecosystem decides to use one particular effect system, things like using two different handlers for different parts of the program, such as the example of using different logger in the db library and the main app, requires type juggling. In some effect libraries this is not even possible.</p></li>
<li><p>Finally, note that the OOP code shown in this post are very basic and straightforward code that even a beginner in OOP can write. Any new person who joins the project, or any one time contributor who just wants to fix a bug and move on, will be able to work on either one of the libraries or the application code. It’s difficult to say the same with the composable effects libraries in Haskell.</p></li>
</ul>
<h1 id="conclusions">Conclusions</h1>
<p>Mainstream statically-typed OOP allows straightforward backwards compatible evolution of types, while keeping them easy to compose. I consider this to be one of the killer features of mainstream statically-typed OOP, and I believe it is an essential feature for programming with many people, over long periods of time.</p>
<p>Just like OOP, Haskell has design patterns, such as the effect monad pattern we’ve shown above. Some of these design patterns solve the problem nicely, but they need an entire ecosystem to follow the same pattern to be useful.</p>
<p>I think it would be beneficial for the functional programming community to stop dismissing OOP’s successes in the industry as an accident of history and try to understand what OOP does well.</p>
<hr />
<p>Thanks to Chris Penner and Matthías Páll Gissurarson for reviewing a draft of this blog post.</p>]]></summary>
</entry>

</feed>
