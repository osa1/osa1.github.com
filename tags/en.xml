<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>osa1.net - Posts tagged en</title>
    <link href="http://osa1.net/tags/en.xml" rel="self" />
    <link href="http://osa1.net" />
    <id>http://osa1.net/tags/en.xml</id>
    <author>
        <name>Ömer Sinan Ağacan</name>
        <email>omeragacan@gmail.com</email>
    </author>
    <updated>2025-06-28T00:00:00Z</updated>
    <entry>
    <title>Why I'm excited about effect systems</title>
    <link href="http://osa1.net/posts/2025-06-28-why-effects.html" />
    <id>http://osa1.net/posts/2025-06-28-why-effects.html</id>
    <published>2025-06-28T00:00:00Z</published>
    <updated>2025-06-28T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>Imagine a programming language where you can have full control over whether and how functions, modules, or libraries interact with shared resources like the scheduler for threading, the file system and other OS-level resources like sockets and other file descriptors, timers for things like delaying the current thread for timed updates or scheduling timed callbacks, and so on.</p>
<p>In this language, a function (or module, library, …) needs to declare its interactions with the shared resources in its type.</p>
<p>When a function accesses e.g. the file system, the caller has full control over how it accesses the file system. All file system access functions can be specified (or overridden if they have a default) by the caller.</p>
<p>Furthermore, assume that this language can also suspend functions and resume them later, similar to <code>async</code> functions in many languages today, which are paused and resumed later when the value of e.g. a <code>Future</code> becomes available.</p>
<p>This language lends itself to a more composable system compared to anything that we have today. This system is composable, flexible, and testable by default.</p>
<p>If you think about it, it’s really strange that today we find it acceptable that I can import a library, and the library can spawn threads, use the file system, block the current thread with things like <code>sleep</code> or with blocking IO operations, and I have no control over it.</p>
<p>Most of the time, this kind of thing will be at least documented, but if I use a library that fundamentally needs these things, unless the library accounts for my use case, I may not be able to use it in my application.</p>
<p>For example, maybe it spawns threads but I want it to use my own thread pool where in addition to limiting number of threads, I attach priorities to threads and schedule based on priorities.</p>
<p>Or, maybe I have a library that builds/compiles things by reading files, processing them, and generating files. If I have control over the file system API that the library uses, it takes no effort (e.g. no planning ahead of time) to test this library using an in-memory file system, in parallel, without worrying about races and IO bottlenecks. I don’t have to consider testing scenarios in the library and structure my code accordingly.</p>
<p>Or, maybe I have code that polls some resources, and maybe posts periodic updates. It creates a thread that does the periodic work, and <code>sleep</code>s. With control over threads, schedulers, and timers, I can fast-forward in time (to the next event) in my tests without actually waiting for <code>sleep</code>s and any other timed events, to test my code quickly.</p>
<p>These are some of the things I get to do with an effect system.</p>
<h2 id="whats-in-an-effect-system">What’s in an effect system?</h2>
<p>At a high-level, an effect system has two components: (1) a type system, and (2) runtime features.</p>
<p>These two components are somewhat orthogonal: you can have one without the other, depending on what you want to make possible.</p>
<p>In the systems available today, (1) typically involves adding a type component to function types, for the effects a function can invoke.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>For example, in <a href="https://koka-lang.github.io/">Koka</a>, if you define stdin/stdout operations in an effect named <code>console</code>, and have a function that uses the <code>console</code> effects, the function’s type signature looks like this:</p>
<pre><code>fun sayHi() -&gt; console ()
  print(&quot;hi&quot;)</code></pre>
<p>This type says <code>sayHi</code> returns unit (<code>()</code>) and uses the <code>console</code> effect.</p>
<p>(2) typically involves capturing the continuation of the effect invocation and passing it to a “handler”. Depending on the system, the handler can then do things (e.g. memory operations, invoking other effects) and “jump” to (or “tail call”) the continuation with the value returned by the invoked effect.</p>
<p>With the <code>console</code> effect above, a handler may just record the printed string in a data structure, which can then be used for testing. Another handler may actually write to <code>stdout</code>, which would then be used when you run the application.</p>
<p>Depending on the exact (1) and (2) features, you get to do different things. The current effect systems in various languages support different (1) and (2) features, and there are some systems that omit one of (1) or (2) entirely.</p>
<p>For the purposes of this blog post, we won’t consider the full spectrum of features you can have, and what those features allow.</p>
<h2 id="example-a-simple-grep-implementation-in-koka">Example: a simple grep implementation in Koka</h2>
<p>There isn’t a language today that gives us everything we need for the use cases I describe at the beginning.</p>
<p>However among the languages that we have, Koka comes close, so we’ll use Koka for a simple example.</p>
<p>Imagine a simple “grep” command that takes a string and a list of file paths as arguments, and finds occurrences of the string in the file contents and reports them.</p>
<p>In Koka, the standard library definitions for these “effects” could look like this:</p>
<pre><code>effect fs
  ctl read-file(path: path): string

effect console
  ctl println(s: string): ()</code></pre>
<p>Using these effects, the code that reads the files and searches for the string is not different from how it would look like in any other “functional”<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> language:</p>
<pre><code>fun search(pattern: string, files: list&lt;string&gt;): &lt;fs, console&gt;()
  val pattern-size = pattern.count()
  files.foreach fn(file)
    val contents = read-file(file.path)
    val parts = contents.split(pattern)
    report-matches(file, pattern-size, parts)

fun report-matches(file: string, pattern-size: int, parts: list&lt;string&gt;): &lt;console&gt;()
  if parts.length == 0 then
    return ()

  println(file)

  var line := 0
  var column := 0
  parts.init.foreach fn(part)
    part.vector.foreach fn(char)
      if char == &#39;\n&#39; then
        line := line + 1
        column := 0
      else
        column := column + 1

    println((line + 1).show ++ &quot;:&quot; ++ (column + 1).show)</code></pre>
<p>When calling <code>search</code>, I have to provide handlers for <code>fs</code> and <code>console</code> effects.</p>
<p>In the executable that I generate for users, I can use handlers that do actual file system operations and print to <code>stdout</code>:</p>
<pre><code>val fs-io = handler
  ctl read-file(path: path)
    resume(read-text-file(path))

val console-terminal = handler
  ctl println(s: string)
    write-to-stdout(s)
    resume(())</code></pre>
<p>In the tests, I can use a <code>read-file</code> handler that reads from an in-memory map, and add printed lines to a list, to compare with the expected test outputs:</p>
<pre><code>struct test-case
  files: list&lt;test-file&gt;
  pattern: string
  expected-output: list&lt;string&gt;

struct test-file
  path: path
  contents: string

val test-cases: list&lt;test-case&gt; = [
  Test-case(
    files = [Test-file(&quot;file1&quot;.path, &quot;test\ntest&quot;), Test-file(&quot;file2&quot;.path, &quot;a\n test\nb&quot;)],
    pattern = &quot;test&quot;,
    expected-output = [&quot;file1&quot;, &quot;1:1&quot;, &quot;2:1&quot;, &quot;file2&quot;, &quot;2:2&quot;]
  ),
]

fun test(): &lt;exn&gt;()
  var printed-lines := Nil

  test-cases.foreach fn (test)
    with handler
      ctl read-file(path_: path)
        match test.files.find(fn (file) file.path.string == path_.string)
          Just(file) -&gt; resume(file.contents)
          Nothing -&gt; throw(&quot;file not found&quot;, ExnAssert)

    with handler
      ctl println(s: string)
        printed-lines := Cons(s, printed-lines)
        resume(())

    search(test.pattern, test.files.map(fn (file) file.path.string))

    if printed-lines.reverse != test.expected-output then
      throw(&quot;unexpected test output&quot;, ExnAssert)</code></pre>
<p>You can see the full example <a href="https://gist.github.com/osa1/a5e7fdfa30d69125970c0797c525ede2">here</a>.</p>
<h2 id="i-can-already-do-this-in-language-x-using-libraryframework-y">I can already do this in language X using library/framework Y?</h2>
<p>The point with effect systems is that, you don’t get a composable and testable system <em>when you design for it</em>, you get it <em>by default</em>.</p>
<p>If you implement a library that uses the file system, I can run it with an in-memory file system, or intercept file accesses to prevent certain things, or log certain things, and so on, regardless of whether you designed for it or not.</p>
<p>The Koka code above does not demonstrate this fully, and there’s no system available today that can. I’m just using whatever is available today.</p>
<p>In an ideal system, you would have to go out of your way to have access to the filesystem without using an effect, rather than the other way around.</p>
<p>When comparing languages we never talk about what’s possible: almost everything is possible in almost every general purpose programming language.</p>
<p>What we’re talking about is things like: the idiomatic and performant way of doing things.</p>
<p>The language where what I talk about is idiomatic and performant does not exist today.</p>
<h2 id="how-do-we-know-that-this-ideal-system-is-possible">How do we know that this ideal system is possible?</h2>
<p>The short answer is that I’ve been working on compilers for more than a decade at this point, I know that I can implement this, and I’m working on implementing it.</p>
<p>The slightly longer answer is: we mentioned that the two components of an effect system are somewhat orthogonal. In the design that I have in mind (more on this below), without the type system part of it you still get 90% of the benefits. So let’s focus on the runtime parts.</p>
<p>What you need for a flexible effect system is, <em>conceptually</em>, a way of suspending the stack when calling an effect, passing the suspended stack (you may want to call it a “continuation”) to the handler for the effect invoked.</p>
<p>This kind of thing is already possible in many of the high-level languages today. If your language supports lightweight threads (green threads, fibers, etc.), coroutines, generators, or similar features where the code is suspended when it does something like <code>await</code> or <code>yield</code>, and then resumed later, you already have the runtime features for a flexible effect system.</p>
<h2 id="for-me-its-about-composable-and-testable-libraries">For me, it’s about composable and testable libraries</h2>
<p>I deliberately didn’t mention in this blog post so far that effect systems generalize features like async/await, iterators/generators, exceptions, and many other features.</p>
<p>The reason is because, as a user, I don’t care whether these features are implemented using an effect system under the hood, or in some other ways. For example, Dart has all of these features, but it doesn’t use an effect system to implement them. As a user, it doesn’t matter to me as long as I have the features.</p>
<p>Instead, what I’m more interested in as a user is: how it influences or affects library design, and what it allows me to do at a high level, in large code bases.</p>
<p>However it would be a shame to not mention that, yes, effect systems generalize all these features, and more. The paper <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2017/05/asynceffects-msr-tr-2017-21.pdf">“Structured Asynchrony with Algebraic Effects”</a> shows how these features can be implemented in Koka.</p>
<h2 id="to-be-continued">To be continued</h2>
<p>Some of the recent discussions online about effect systems left me somewhat dissatisfied, because most posts seem to focus on small-scale benefits of effect systems, and I wanted to share my incomplete (but hopefully not incoherent!) perspective on effect systems.</p>
<p>In the future posts I’m hoping to cover some of the open problems when designing such a system.</p>
<hr />
<p>Thanks to <a href="https://github.com/TimWhiting/">Tim Whiting</a> for reviewing a draft of this blog post.</p>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>This is a somewhat rough estimate on what these effect types in function types indicate. In practice it’s more complicated than “effects the function invokes”: if you read it as that you fail to explain some of the type errors, or why some code of the code type checks. More on this (hopefully) in a future post.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>“Functional” in quotes because I don’t think that word means much these days. Maybe more on this later.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></summary>
</entry>
<entry>
    <title>Changes to variants in Fir</title>
    <link href="http://osa1.net/posts/2025-06-12-fir-new-variants.html" />
    <id>http://osa1.net/posts/2025-06-12-fir-new-variants.html</id>
    <published>2025-06-12T00:00:00Z</published>
    <updated>2025-06-12T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>In the previous two posts (<a href="https://osa1.net/posts/2025-01-18-fir-error-handling.html">1</a>, <a href="https://osa1.net/posts/2025-04-17-throwing-iterators-fir.html">2</a>) we looked at how Fir utilizes variant types for exceptions tracked at function types, aka. checked exceptions.</p>
<p>As I wrote more and more Fir, it quickly became obvious that the current variant type design is just too verbose and difficult to use.</p>
<p>To see the problems, consider a JSON parsing library. This library may throw a parse error when the input is not valid. Before the recent changes, the parsing function would look like this:</p>
<pre><code>parse(input: Str) Json / [ParseError, ..exn]:
    ...
    # When things go wrong:
    throw(~ParseError)
    ...</code></pre>
<p>(As a reminder: <code>[ParseError, ..exn]</code> part is the variant type for the exceptions that this function throws. <code>ParseError</code> is a label for the exception value, and it has no fields. <code>..exn</code> part is the row extension, allowing this function to be called in functions that throw other exceptions.)</p>
<p>This error type is not that useful, because the label <code>ParseError</code> doesn’t contain any information like the error location.</p>
<p>When we start adding fields to it, things quickly get verbose:</p>
<pre><code>parse(input: Str) Json / [ParseError(errorByteIdx: U32, msg: Str), ..exn]:
    ...
    # When things go wrong:
    throw(~ParseError(
        errorByteIdx = ...,
        msg = ...,
    ))
    ...</code></pre>
<p>Now every function that propagates this error needs to include the same fields in the label.</p>
<p>As a second problem, suppose that there’s another library that parses YAML, which also throws an exception with the same label <code>ParseError</code>. Because we can’t have the same label multiple times in a variant (as we would have no way of distinguishing them in pattern matching), we can’t call both library functions in the same function, doing that would result in a type error about duplicate labels with different fields.</p>
<p><em>For the verbosity of labels with fields:</em> we could have type synonyms for variant alternatives, but this doesn’t solve the problem with using the same labels in different libraries.</p>
<p><em>For the label conflicts:</em> we could manually make the labels unique, maybe by including library name in the label, like <code>JsonParseError(...)</code> and <code>YamlParseError(...)</code>.</p>
<p>This makes labels longer, and it doesn’t guarantee that conflicts won’t occur. For example, if we allow linking different versions of the same library in a program, two different versions of the library might have the same label <code>JsonParseError</code>, but with different fields.</p>
<p>A combination of more creative features may solve the problem completely, but features add complexity to the language, even when they work well together. If possible, it would be preferable to improve the utility of existing features instead.</p>
<p>As a solution that uses only existing features, Fir variants now hold named types. The example above now looks like this:</p>
<pre><code>type ParseError:
    errorByteIdx: U32
    msg: Str

parse(input: Str) Json / [ParseError, ..exn]:
    ...
    # When things go wrong:
    throw(~ParseError(
        errorByteIdx = ...,
        msg = ...,
    ))
    ...</code></pre>
<p>(A named type in Fir is anything other than a record or variant. See <a href="https://osa1.net/posts/2021-04-10-sums-and-products.html">this post</a> for more details on named and anonymous types.)</p>
<p>From the type checker’s point of view, a variant is still a map of labels to fields, but we now implicitly use the fully qualified names of types as the labels.</p>
<p>So the variant above looks like this to the type checker: <code>[Label("P.M.ParseError")(P.M.ParseError), ...exn]</code>, where <code>P</code> is the package name and <code>M</code> is the module path to the type <code>ParseError</code>, and <code>(...)</code> part after the label indicates a single positional field.</p>
<p>This solves all of the problems with labels, and has several of other advantages:</p>
<ul>
<li><p>Named types are concise as we don’t have to list all of the fields every time we mention them.</p></li>
<li><p>Named types and their fields can be documented.</p></li>
<li><p>Named types can have methods.</p></li>
<li><p>Named types can be extended with more fields without breaking backwards compatibility. So now it’s possible to add more fields to <code>ParseError</code> without breaking existing users.</p></li>
<li><p>A type with the same name defined in different packages or even modules can now be used in the same variant type.</p>
<p>(When showing a variant type to the user in an error message, we add package and module prefixes as necessary to disambiguate.)</p></li>
<li><p>If I import a named type <code>Foo</code> as <code>Bar</code> in a module, I can use <code>Bar</code> in my variant types and it would be seen as <code>Foo</code> elsewhere.</p></li>
<li><p>Named types can implement traits. This opens up possibilities for implicitly deriving traits for variant types.</p></li>
</ul>
<p>One implication of using the fully qualified path of a type as the label is that we don’t allow the same type constructor applied to different types in the same variant. E.g. <code>[Option[U32], Option[Bool]]</code> is not allowed.</p>
<p>This is the same limitation with duplicate labels in the original version, where <code>[Label1(x: U32), Label1(y: Str)]</code> wasn’t allowed. I don’t think this will be an issue in practice.</p>
<p>Pattern matching works as before, but we now omit the labels, as they’re inferred from the types of patterns. Here’s a contrived example demonstrating the syntax:</p>
<pre><code>f() / [Option[U32], ..exn]:
    throw(~Option.None)

g() / [Result[Str, Bool], ..exn]:
    throw(~Result.Ok(Bool.True))

main():
    match try({
        f()
        g()
    }):
        Result.Ok(()): print(&quot;OK&quot;)
        Result.Err(~Option.None): print(&quot;NA&quot;)
        Result.Err(~Result.Ok(bool)): print(&quot;Bool: `bool`&quot;)
        Result.Err(~Result.Err(str)): print(&quot;Str: `str`&quot;)</code></pre>
<p>This is essentially the same as before, just with variant labels omitted.</p>
<p>To keep things simple, I haven’t implemented supporting literals in variant syntax yet: <code>~123</code>, <code>~"Hi"</code>, or <code>~'a'</code> doesn’t work yet. It wouldn’t be too much work to implement this, but I don’t need it right now.</p>
<hr />
<p>In retrospect, using named types in variants is such an obvious improvement, with practically no downsides. But it took a few thousands of lines of Fir for me to realize this.</p>
<p>If I discover cases where explicit labels are useful, the current design is not incompatible with the old one. The type checker still uses the same variant representation, with a label and a field for each alternative (with multiple fields are represented as records). It shouldn’t be too difficult to support both named types and labels in variant types.</p>
<p>This new design improves error handling quite a bit, but there are still a few problems we need to solve. In a future post I’m hoping to talk about the issues with adding a type component to the function types for exceptions.</p>]]></summary>
</entry>
<entry>
    <title>Throwing iterators in Fir</title>
    <link href="http://osa1.net/posts/2025-04-17-throwing-iterators-fir.html" />
    <id>http://osa1.net/posts/2025-04-17-throwing-iterators-fir.html</id>
    <published>2025-04-17T00:00:00Z</published>
    <updated>2025-04-17T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>Recently I’ve been working on extending <a href="https://github.com/fir-lang/fir">Fir</a>’s <code>Iterator</code> trait to allow iterators to throw exceptions.</p>
<p>It took a few months of work, because we needed multiple parameter traits for it to work, which took <a href="https://github.com/fir-lang/fir/pull/73">a few months of hacking</a><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> to implement.</p>
<p>Then there was a lot of bug fixing and experimentation, but it finally works, and I’m excited to share what you can do with Fir iterators today.</p>
<p>As usual, link to the online interpreter with all of the code in this post is at the end.</p>
<p>Before starting, I recommend reading the <a href="https://osa1.net/posts/2025-01-18-fir-error-handling.html">previous post</a>. It’s quite short and it explains the basics of error handling in Fir.</p>
<p>Previous post did not talk about traits at all, so in short, traits in Fir is the same feature as Rust’s traits and Haskell’s typeclasses<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>.</p>
<p>The <code>Iterator</code> trait in Fir is also the same as the trait with the same name in Rust, and it’s used the same way, in <code>for</code> loops.</p>
<p>Here’s a simple example of what you can do with iterators:</p>
<pre><code>sum(nums: Vec[U32]): U32
    let result: U32 = 0
    for i: U32 in nums.iter():
        result += i
    result</code></pre>
<p>The <code>Vec.iter</code> method returns an iterator that returns the next element every time its <code>next</code> method is called. <code>for</code> loop implicitly calls the <code>next</code> method to get the next element, until the <code>next</code> method returns <code>Option.None</code>.</p>
<p>Similar to Rust’s <code>Iterator</code>, Fir’s <code>Iterator</code> trait also comes with a <code>map</code> method that allows mapping iterated elements:</p>
<pre><code>parseSum(nums: Vec[Str]): U32
    let result: U32 = 0
    for i: U32 in nums.iter().map(parseU32):
        result += i
    result

parseU32(s: Str): U32
    if s.len() == 0:
        panic(&quot;Empty input&quot;)

    let result: U32 = 0

    for c: Char in s.chars():
        if c &lt; &#39;0&#39; || c &gt; &#39;9&#39;:
            panic(&quot;Invalid digit&quot;)

        let digit = c.asU32() - &#39;0&#39;.asU32()

        result *= 10
        result += digit

    result</code></pre>
<p>This version takes a <code>Vec[Str]</code> as argument, and parses the elements as integers.</p>
<p>The problem with this version is that it panics on unexpected cases: invalid digits and empty input, and it ignores overflows.</p>
<p>Until now, there wasn’t a convenient way to use the <code>Iterator</code> API and <code>for</code> loops to do this kind of thing, while also propagating exceptions to the call site of the <code>for</code> loop, or to the loop variable. But now we can do this: (<code>parseU32Exn</code> is from the previous post)</p>
<pre><code>parseSum(nums: Vec[Str]): [Overflow, EmptyInput, InvalidDigit, ..errs] U32
    let result: U32 = 0
    for i: U32 in nums.iter().map(parseU32Exn):
        result += i
    result</code></pre>
<p>Errors that <code>parseU32Exn</code> can throw are now implicitly thrown from the <code>for</code> loop and reflected in the function’s type.</p>
<p>This new <code>Iterator</code> API is flexible enough to allow handling some (or all) of the exceptions thrown by a previous iterator. For example, here’s how we can handle <code>InvalidDigit</code> exceptions and yield <code>0</code> instead:</p>
<pre><code>parseSumHandleInvalidDigits(nums: Vec[Str]): [Overflow, EmptyInput, ..errs] U32
    let result: U32 = 0
    for i: U32 in nums.iter().map(parseU32Exn).mapResult(handleInvalidDigit):
        result += i
    result

handleInvalidDigit(parseResult: Result[[InvalidDigit, ..errs], Option[U32]]): [..errs] Option[U32]
    match parseResult:
        Result.Ok(result): result
        Result.Err(~InvalidDigit): Option.Some(0u32)
        Result.Err(other): throw(other)</code></pre>
<p><code>InvalidDigit</code> is no longer in the exception type of the function because <code>mapResult(handleInvalidDigit)</code> handles them.</p>
<p>We can also convert exceptions thrown by an iterator to <code>Result</code> values:</p>
<pre><code>parseSumHandleInvalidDigitsLogRest(nums: Vec[Str]): U32
    let result: U32 = 0
    for i: Result[[Overflow, EmptyInput], U32] in \
            nums.iter().map(parseU32Exn).mapResult(handleInvalidDigit).try():
        match i:
            Result.Err(~Overflow): printStr(&quot;Overflow&quot;)
            Result.Err(~EmptyInput): printStr(&quot;Empty input&quot;)
            Result.Ok(i): result += i
    result</code></pre>
<p>This function no longer has an exception type, because exceptions thrown by the iterator are passed to the loop variable.</p>
<p>In summary, we started with an iterator that doesn’t throw (<code>nums.iter()</code>), mapped it with a function that throws (<code>map(parseU32Exn)</code>), which made the <code>for</code> loop propagate the exceptions thrown by the map function. We then handled one of the exceptions (<code>mapResult(handleInvalidDigit)</code>), and finally, we handled all of the exceptions and started passing a <code>Result</code> value to the loop variable (<code>try()</code>).</p>
<p>The function’s exception type was updated each time to reflect the exceptions thrown by the function.</p>
<p>Once we had multiple parameter traits (which are important even without exceptions, and something we were going to implement anyway), no language features were needed specifically for the throwing iterators API that composes. Changes in the <code>for</code> loop type checking were necessary to allow throwing iterators in <code>for</code> loops. Composing iterators like <code>iter().map(...).mapResult(...).try()</code> in the examples above did not require any changes to the trait system or exceptions.</p>
<p>This demonstrates that Fir traits and exceptions work nicely together.</p>
<p>You can try the code in this blog post <a href="https://fir-lang.github.io/?file=ThrowingIter.fir">in your browser</a>.</p>
<h1 id="im-looking-for-contributors">I’m looking for contributors</h1>
<p>I’m planning a blog post on my vision of Fir, why I think it matters, and a roadmap, but if you already like what you see, know a thing or two about implementing programming languages, and have the time to energy to contribute to a new language, please don’t hesitate to reach out!</p>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>I started this work in one country, and when finished, I was living in another! This PR really felt like an eternity to finish.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>Implementation-wise, it’s closer to Rust than Haskell as we monomorphise.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></summary>
</entry>
<entry>
    <title>Error handling in Fir</title>
    <link href="http://osa1.net/posts/2025-01-18-fir-error-handling.html" />
    <id>http://osa1.net/posts/2025-01-18-fir-error-handling.html</id>
    <published>2025-01-18T00:00:00Z</published>
    <updated>2025-01-18T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>A while ago I came up with an <a href="https://gist.github.com/osa1/38fd51abe5247462eddb7d014f320cd2">“error handling expressiveness benchmark”</a>, some common error handling cases that I want to support in <a href="https://github.com/fir-lang/fir">Fir</a>.</p>
<p>After 7 months of pondering and hacking, I think I designed a system that meets all of the requirements. Error handling in Fir is safe, expressive, and convenient to use.</p>
<p>Here are some examples of what we can do in Fir today:</p>
<p>(Don’t pay too much attention to type syntax for now. Fir is still a prototype, the syntax will be improved.)</p>
<p>When we have multiple ways to fail, we don’t have to introduce a sum type with all the possible ways that we can fail, we can use variants:</p>
<pre><code>parseU32(s: Str): Result[[InvalidDigit, Overflow, EmptyInput, ..r], U32]
    if s.len() == 0:
        return Result.Err(~EmptyInput)

    let result: U32 = 0

    for c in s.chars():
        if c &lt; &#39;0&#39; || c &gt; &#39;9&#39;:
            return Result.Err(~InvalidDigit)

        let digit = c.asU32() - &#39;0&#39;.asU32()

        result = match checkedMul(result, 10):
            Option.None: return Result.Err(~Overflow)
            Option.Some(newResult): newResult

        result = match checkedAdd(result, digit):
            Option.None: return Result.Err(~Overflow)
            Option.Some(newResult): newResult

    Result.Ok(result)</code></pre>
<p>An advantage of variants is, in pattern matching, we “refine” types of binders to drop handled variants from the type. This allows handling some of the errors and returning the rest to the caller:</p>
<pre><code>defaultEmptyInput(res: Result[[EmptyInput, ..r], U32]): Result[[..r], U32]
    match res:
        Result.Err(~EmptyInput): Result.Ok(0u32)
        Result.Err(other): Result.Err(other)
        Result.Ok(val): Result.Ok(val)</code></pre>
<p>Here <code>EmptyInput</code> is removed from the error value type in the return type. The caller does not need to handle <code>EmptyInput</code>.</p>
<p>(We don’t refine types of variants nested in other types for now, so the last two branches cannot be replaced with <code>other: other</code> for now.)</p>
<p>Another advantage is that they allow composing error returning functions that return different error types:</p>
<p>(Fir supports variant constructors with fields, but to keep things simple we don’t use them in this post.)</p>
<pre><code>readFile(s: Str): Result[[IoError, ..r], Str]
    # We don&#39;t have the standard library support for file IO yet, just return
    # an error for now.
    Result.Err(~IoError)

parseU32FromFile(filePath: Str): Result[[InvalidDigit, Overflow, EmptyInput, IoError, ..r], U32]
    let fileContents = match readFile(filePath):
        Result.Err(err): return Result.Err(err)
        Result.Ok(contents): contents

    parseU32(fileContents)</code></pre>
<p>In the early return I don’t have to manually convert <code>readFile</code>s error value to <code>parseU32</code>s error value to make the types align.</p>
<p>Variants work nicely with higher-order functions as well. Here’s a function that parses a vector of strings, returning any errors to the caller:</p>
<pre><code>parseWith(vec: Vec[Str], parseFn: Fn(Str): Result[errs, a]): Result[errs, Vec[a]]
    let ret = Vec.withCapacity(vec.len())

    for s in vec.iter():
        match parseFn(s):
            Result.Err(err): return Result.Err(err)
            Result.Ok(val): ret.push(val)

    Result.Ok(ret)</code></pre>
<p>If I have a function argument that returns more errors than my callback, I can still call it without any adjustments:</p>
<pre><code>parseWith2(vec: Vec[Str], parseFn: Fn(Str): Result[[OtherError, ..r], a]): Result[[..r], Vec[a]]
    let ret = Vec.withCapacity(vec.len())

    for s in vec.iter():
        match parseFn(s):
            Result.Err(~OtherError): continue
            Result.Err(err): return Result.Err(err)
            Result.Ok(val): ret.push(val)

    Result.Ok(ret)</code></pre>
<p><code>parseWith2(vec, parseU32)</code> type checks even though <code>parseU32</code> doesn’t return <code>OtherError</code>.</p>
<p>Similarly, if I have a function that handles more cases, I can pass it as a function that handles less:</p>
<pre><code>handleSomeErrs(error: [Overflow, OtherError]): U32 = 0

parseWithErrorHandler(
        input: Str,
        handler: Fn([Overflow, ..r1]): U3
    ): Result[[InvalidDigit, EmptyInput, ..r2], U32]
    match parseU32(input):
        Result.Err(~Overflow): Result.Ok(handler(~Overflow))
        Result.Err(other): Result.Err(other)
        Result.Ok(val): Result.Ok(val)</code></pre>
<p>Here I’m able to pass <code>handleSomeErrs</code> to <code>parseWithErrorHandler</code>, even though it handles more errors than what <code>parseWithErrorHandler</code> argument needs.</p>
<h1 id="variants-as-exceptions">Variants as exceptions</h1>
<p>When we use variants as exception values, we end up with a system that is</p>
<ul>
<li>Safe: All exceptions need to be handled before <code>main</code> returns.</li>
<li>Flexible: All of the flexibility of variants shown above apply to exceptions as well.</li>
<li>Convenient:
<ul>
<li>Error values are implicitly propagated to the caller when not handled.</li>
<li>When a library uses one way of error reporting (error values or exceptions) and you need the other, conversion is just a matter of calling one function.</li>
</ul></li>
</ul>
<p>At the core of exceptions in Fir are these three functions:</p>
<ul>
<li><p><code>throw</code>, which converts a variant into an exception:</p>
<pre><code>throw(exn: exn): exn a</code></pre></li>
<li><p><code>try</code>, which converts exceptions into <code>Result.Err</code> values:</p>
<pre><code>try(cb: Fn(): exn a): Result[exn, a]</code></pre></li>
<li><p><code>untry</code>, which converts a <code>Result.Err</code> value into an exception:</p>
<pre><code>untry(res: Result[exn, a]): exn a</code></pre></li>
</ul>
<p>Here are some of the code above, using exceptions instead of error values:</p>
<pre><code>parseU32Exn(s: Str): [InvalidDigit, Overflow, EmptyInput, ..r] U32
    if s.len() == 0:
        throw(~EmptyInput)

    let result: U32 = 0

    for c in s.chars():
        if c &lt; &#39;0&#39; || c &gt; &#39;9&#39;:
            throw(~InvalidDigit)

        let digit = c.asU32() - &#39;0&#39;.asU32()

        result = match checkedMul(result, 10):
            Option.None: throw(~Overflow)
            Option.Some(newResult): newResult

        result = match checkedAdd(result, digit):
            Option.None: throw(~Overflow)
            Option.Some(newResult): newResult

    result

readFileExn(s: Str): [IoError, ..r] Str
    # We don&#39;t have the standard library support for file IO yet, just throw
    # an error for now.
    throw(~IoError)

parseU32FromFileExn(filePath: Str): [InvalidDigit, Overflow, EmptyInput, IoError, ..r] U32
    parseU32Exn(readFileExn(filePath))

parseWithExn(vec: Vec[Str], parseFn: Fn(Str): exn a): exn Vec[a]
    let ret = Vec.withCapacity(vec.len())
    for s in vec.iter():
        ret.push(parseFn(s))
    ret</code></pre>
<p>When a library provides one of these, it’s trivial to convert to the other:</p>
<pre><code>parseU32UsingExnVersion(s: Str): Result[[InvalidDigit, Overflow, EmptyInput, ..r], U32]
    try({ parseU32Exn(s) })

parseU32UsingResultVersion(s: Str): [InvalidDigit, Overflow, EmptyInput, ..r] U32
    untry(parseU32(s))</code></pre>
<p>Nice!</p>
<hr />
<p>I’m quite excited about these results. There’s still so much to do, but I think it’s clear that this way of error handling has a lot of potential.</p>
<p>I’ll be working on some of the improvements I mentioned above (and I have others planned as well), and the usual stuff that every language needs (standard library, tools etc.). Depending on interest, I may also write more about variants, error handling, or anything else related to Fir.</p>
<p>You can try Fir online <a href="https://fir-lang.github.io/?file=ErrorHandling.fir">here</a>.</p>]]></summary>
</entry>
<entry>
    <title>When is inlining useful?</title>
    <link href="http://osa1.net/posts/2024-12-07-inlining.html" />
    <id>http://osa1.net/posts/2024-12-07-inlining.html</id>
    <published>2024-12-07T00:00:00Z</published>
    <updated>2024-12-07T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>Especially in high-level languages, inlining is most useful when it leads to:</p>
<ul>
<li>Optimizing the callee’s body based on the arguments passed.</li>
<li>Optimizing the call site based on the callee’s return value.</li>
</ul>
<p>Let’s look at some examples.</p>
<h1 id="example-avoiding-redundant-bounds-checks">Example: avoiding redundant bounds checks</h1>
<p>Suppose we have a library for decoding some format of binary files with length-prefixed vectors of 32-bit integers, with all integers encoded as little-endian.</p>
<p>A simple implementation would be:</p>
<pre><code>/// Decode a length-prefixed 32-bit unsigned integer vector.
///
/// # Panics
///
/// Panics if the buffer does not have enough bytes.
pub fn decode_u32_vec(buffer: &amp;[u8]) -&gt; Vec&lt;u32&gt; {
    let len = decode_32_le(buffer, 0) as usize;
    let mut vec = vec![0; len];
    for i in 0..len {
        vec[i] = decode_32_le(buffer, (i + 1) * 4);
    }
    vec
}

/// Decode a single 32-bit unsigned integer, encoded as little-endian.
///
/// # Panics
///
/// Panics if the buffer does not have 4 bytes starting at `offset`.
#[inline(never)]  // this version isn&#39;t inlined
pub fn decode_32_le(buffer: &amp;[u8], offset: usize) -&gt; u32 {
    let b1 = buffer[offset];
    let b2 = buffer[offset + 1];
    let b3 = buffer[offset + 2];
    let b4 = buffer[offset + 3];
    u32::from_le_bytes([b1, b2, b3, b4])
}</code></pre>
<p>This version is not ideal, because <code>decode_32_le</code> checks bounds on each byte access. (<a href="https://godbolt.org/z/eTMa38nqT">compiler explorer</a>)</p>
<p>We can improve it by checking the bounds for all of the reads once:</p>
<pre><code>#[inline(never)]  // this version isn&#39;t inlined
pub fn decode_32_le(buffer: &amp;[u8], offset: usize) -&gt; u32 {
    if buffer.len() &lt; 4 || buffer.len() - 4 &lt; offset {
        panic!();
    }
    let b1 = buffer[offset];
    let b2 = buffer[offset + 1];
    let b3 = buffer[offset + 2];
    let b4 = buffer[offset + 3];
    u32::from_le_bytes([b1, b2, b3, b4])
}</code></pre>
<p>The conditional makes sure that in the rest of the function the slice indices are all within bounds, so the compiler doesn’t generate bounds checks for the accesses. The compiler is also able to optimize further now to load just one 32-bit word from the memory, instead of reading one byte at a time. (<a href="https://godbolt.org/z/MG9EaE5G3">compiler explorer</a>)</p>
<p><code>decode_32_le</code> is quite good now, but it still has to do one bounds check, and since <code>decode_u32_vec</code> calls it in each iteration, it does a bounds check in each iteration.</p>
<p>Ideally we want to be able to do one bounds check before the loop, just like we did in <code>decode_32_le</code>, and then omit bounds checks within the loop:</p>
<pre><code>pub fn decode_u32_vec(buffer: &amp;[u8]) -&gt; Vec&lt;u32&gt; {
    let len = decode_32_le(buffer, 0) as usize;
    if buffer.len() &lt; (len + 1) * 4 {
        panic!();
    }
    let mut vec = vec![0; len];
    for i in 0..len {
        vec[i] = decode_32_le(buffer, (i + 1) * 4);
    }
    vec
}</code></pre>
<p>But this cannot make the bounds checks in <code>decode_32_le</code> disappear, as it may have other call sites that may not always check the bounds before calling it.</p>
<p>Inlining <code>decode_32_le</code> in the use effectively lets us propagate the information in the call site to the callee’s code, and optimize it based on this information. If we change the <code>inline(never)</code> to <code>inline</code> in <code>decode_32_le</code>, with the extra bounds check in <code>decode_u32_vec</code>, we now check bounds once before the loop and don’t check it again in the loop. (<a href="https://godbolt.org/z/EeGbYrfP7">compiler explorer</a>)</p>
<h1 id="example-avoiding-redundant-error-checks">Example: avoiding redundant error checks</h1>
<p>Dart doesn’t have unsigned integers, and many standard library functions throw an <code>ArgumentError</code> when they are passed negative numbers.</p>
<p>One example of these functions is the <a href="https://api.dart.dev/dart-core/int/operator_triple_shift.html">unsigned right shift operator</a>. In many call sites, the shift amount is a <a href="https://github.com/dart-lang/sdk/blob/abb17bc59d8163273451b3ffb2aba784d20001b4/sdk/lib/_internal/wasm/lib/internal_patch.dart#L88-L97">constant positive number</a>. If we call the standard library function in these cases, the library function will check the sign of the arguments that we already know in the call site to be positive.</p>
<p>When we inline <a href="https://github.com/dart-lang/sdk/blob/abb17bc59d8163273451b3ffb2aba784d20001b4/sdk/lib/_internal/wasm/lib/boxed_int.dart#L94-L107">the operator</a> in a call site where the shift argument is constant, the conditional becomes <code>if (constant &lt; 0) throw ...</code>. The compiler can then simplify the condition as <code>true</code> or <code>false</code>, and then simplify this code to just <code>throw ...</code> or eliminate the conditional.</p>
<p>The <code>mix64</code> function linked above when compiled to Wasm, without inlining the right shift operator:</p>
<pre><code>(func $mix64 (param $var0 i64) (result i64)
  ...
  local.tee $var0
  i64.const 24
  call $BoxedInt.&gt;&gt;&gt;
  ...)

(func $BoxedInt.&gt;&gt;&gt; (param $var0 i64) (param $var1 i64) (result i64)
  local.get $var1
  i64.const 64
  i64.lt_u
  if
    local.get $var0
    local.get $var1
    i64.shr_u
    return
  end
  local.get $var1
  i64.const 0
  i64.lt_s
  if
    i32.const 64
    local.get $var1
    struct.new $BoxedInt
    call $ArgumentError
    call $Error._throwWithCurrentStackTrace
    unreachable
  end
  i64.const 0)</code></pre>
<p>With the shift operator inlined:</p>
<pre><code>(func $mix64 (param $var0 i64) (result i64)
  ...
  local.get $var0
  i64.const 24
  i64.shr_u
  ...)</code></pre>
<p>The call to <code>BoxedInt.&gt;&gt;&gt;</code> with error checking is optimized to a single <code>shr_u</code> (shift right, unsigned) instruction.</p>
<h1 id="example-avoiding-boxing">Example: avoiding boxing</h1>
<p>In languages where most values are passed around as boxed, inlining can eliminate boxing.</p>
<p>A common use case where this happens is FFI: pointers/references obtained from FFI calls need to be wrapped in a struct/class/etc. to make them the same “shape” as the language’s native values.</p>
<p>When you have a function that gets a reference from an FFI call, and pass it around to more FFI calls, inlining these FFI calls can avoid boxing the pointer/reference value.</p>
<p>Somewhat silly example, in Dart:</p>
<pre><code>import &#39;dart:ffi&#39;;

@Native&lt;Pointer&lt;Int64&gt; Function()&gt;()
external Pointer&lt;Int64&gt; intPtr();

@Native&lt;Int64 Function(Pointer&lt;Int64&gt;)&gt;()
external int derefIntPtr(Pointer&lt;Int64&gt; ptr);

void main() {
  Pointer&lt;Int64&gt; ptr = intPtr();
  ptr += 1;
  int i = derefIntPtr(ptr);
  print(i);
}</code></pre>
<p>Relevant parts of the generated code when compiled to Wasm:</p>
<pre><code>(func $main
  ...
  call $intPtr
  struct.get $Pointer $field2
  i32.const 8
  i32.add
  call $ffi.derefIntPtr (import)
  ...)

(func $intPtr (result (ref $Pointer))
  i32.const 71
  i32.const 0
  call $ffi.intPtr (import)
  struct.new $Pointer)</code></pre>
<p><code>intPtr</code> allocates a struct, which the call site directly unpacks (reads the field). Inlining <code>intPtr</code> eliminates this allocation:</p>
<pre><code>(func $main
  ...
  call $ffi.intPtr (import)
  i32.const 8
  i32.add
  call $ffi.derefIntPtr (import)
  ...)</code></pre>
<h1 id="example-avoiding-polymorphism">Example: avoiding polymorphism</h1>
<p>When a monomorphic type is passed to a polymorphic function, the polymorphic function can often be inlined to avoid polymorphic access to the monomorphic type.</p>
<p>An example, again in Dart, is <code>Int64List</code>, which is a monomorphic <code>List&lt;int&gt;</code>. It stores the integers unboxed, and when used directly, the integer arguments and return values do not need to be boxed.</p>
<p>When used in a polymorphic site though, the integer elements need to be boxed. Example:</p>
<pre><code>import &#39;dart:typed_data&#39;;

int sum(List&lt;int&gt; list) {
  int ret = 0;
  for (int i = 0; i &lt; list.length; i += 1) {
    ret += list[i];
  }
  return ret;
}

void main() {
  Int64List intList = Int64List.fromList([1, 2, 3, 4]);
  sum(intList);
  sum([1, 2, 3, 4]);
}</code></pre>
<p>Relevant parts of the output when compiled to Wasm:</p>
<pre><code>(func $main
  ;; Allocate `Int64List`, call `sum`:
  ...
  call $sum

  ;; Allocate the other `List&lt;int&gt;`, call `sum`:
  ...
  call $sum)

(func $sum (param $var0 (ref $Object))
  (local $var1 i64)
  (local $var2 i64)
  loop $label0
    ...
    if
      ...
      ;; Virtual call to `operator []`:
      struct.get $Object $field0
      i32.const 747
      i32.add
      call_indirect (param (ref $Object) i64) (result (ref null $#Top))

      ;; The virtual call returns a boxed integer, which we directly unbox:
      ref.cast $BoxedInt
      struct.get $BoxedInt $field1
      i64.add
      ...
    end
  end $label0)</code></pre>
<p>If we inline <code>sum</code>, the loop that iterates the <code>Int64List</code> accesses the unboxed integers directly:</p>
<pre><code>(func $main
  ...
  loop $label1
      ...
      local.get $var3
      local.get $var4
      local.get $var1
      i32.wrap_i64
      ;; Array access is now direct, no boxing.
      array.get $Array&lt;WasmI64&gt;
      i64.add
      local.set $var3
      local.get $var1
      i64.const 1
      i64.add
      local.set $var1
      br $label1
    end
  end $label1)</code></pre>
<p>A similar case is when a monomorphic type is used directly, but via a polymorphic interface. In the <code>Int64List</code> type above, <code>List64List.[]</code> is an override of <a href="https://api.dart.dev/dart-core/List/operator_get.html"><code>List&lt;E&gt;.[]</code></a>, and all overrides of a method need to have the same type.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>So when not inlined, it needs to return a boxed integer:</p>
<pre><code>(func $Int64List.[] (param $var0 (ref $Object)) (param $var1 i64) (result (ref null $#Top))
  ...
  array.get $Array&lt;WasmI64&gt;
  struct.new $BoxedInt)</code></pre>
<p>Similar to the previous example, inlining it eliminates the boxing in monomorphic use sites, as the allocated struct is immediately unboxed.</p>
<h1 id="example-eliminating-closures-and-indirect-calls">Example: eliminating closures and indirect calls</h1>
<p>In languages without monomorphisation, higher-order function arguments need to be allocated as closures.</p>
<p>When calling a closure, the caller needs to get the function pointer from the closure and call the function via the pointer.</p>
<p>These closure allocations and indirect function calls can be too slow in hot loops.</p>
<p>An example of this is in <a href="https://github.com/google/protobuf.dart">Dart protobuf library</a>. In protobufs, packed fields are encoded as a length prefix followed by the elements. In the Dart library, we decode these fields using <a href="https://github.com/google/protobuf.dart/blob/610943a3bed70c1c2079af5fca02462df10d223f/protobuf/lib/src/protobuf/coded_buffer.dart#L271-L277">this helper</a>:</p>
<pre><code>void _readPacked(CodedBufferReader input, void Function() readFunc) {
  input._withLimit(input.readInt32(), () {
    while (!input.isAtEnd()) {
      readFunc();
    }
  });
}</code></pre>
<p>Here the caller of <code>_readPacked</code> passes <code>readFunc</code> as a closure (allocation). <code>_readPacked</code> then allocates another closure, to be passed to <code>_withLimit</code>.</p>
<p>That’s two closure allocations for every packed field in the input. Calling these closures are slow too, because they’re indirect.</p>
<p>Inlining <code>_withLimit</code> in <code>_readPacked</code>, and <code>_readPacked</code> in its call sites (in <code>_mergeFromCodedBufferReader</code>) eliminates closure allocations, and calls to the closures become direct function calls. This <a href="https://github.com/google/protobuf.dart/pull/959">improves packed field decoding</a> significantly.</p>
<h1 id="a-trick-for-effective-inlining-outlining">A trick for effective inlining: outlining</h1>
<p>Consider <code>Int64List.[]</code> again. The implementation needs to check that the index is in bounds of the array, and throw an exception if not: (slightly simplified)</p>
<pre><code>class Int64List ... {
  ...

  @override
  int operator [](int index) {
    if (length.leU(index)) { // shorthand for `index &lt; 0 || index &gt;= length`
      ... // throw exception
    }
    return _data.read(index);
  }
}</code></pre>
<p>To avoid boxing when calling this function we want to always inline it, but if we’re not careful with the error throwing code path (the <code>if</code> body above), the function can get quite large, and when inlined the binary can bloat up significantly.</p>
<p>Ideally we want to inline the happy path that can lead to improvements when inlined, and leave the error path separate in a function, so that we can have the benefits of inlining without adding to the binary size too much.</p>
<p>This can be done by moving the error handling code to a separate function, and making sure that separate function is never inlined (ideally with an annotation to the compiler). In the example above, this may look like:</p>
<pre><code>class Int64List ... {
  @override
  @pragma(&#39;inline&#39;)
  int operator [](int index) {
    if (length.leU(index)) { // shorthand for `index &lt; 0 || index &gt;= length`
      fail(index, length);
    }
    return _data.read(index);
  }
}

@pragma(&#39;never-inline&#39;)
void fail(int index, int length) { ... }</code></pre>
<p>With this it doesn’t matter how large the error handling code is, because we never inline it.</p>
<p>This way of separating inlineable parts of a function from the parts we don’t want to inline is sometimes called “outlining” or “partial inlining”. We can do it manually (as in the example above), but it can also be done by a compiler based on heuristics.</p>
<p>An example transformation to this is <a href="https://www.cambridge.org/core/services/aop-cambridge-core/content/view/75629BBEDB11D8463553A09BF5DEA235/S0956796809007175a.pdf/div-class-title-the-worker-wrapper-transformation-div.pdf">GHC’s worker/wrapper transformation</a>, which splits a function into parts that (1) handle unboxing and boxing before calling the function’s body, and (2) the function’s body that work on the unboxed representations of the arguments. (1) is then inlined to avoid redundant boxing of the arguments and return values.</p>
<p>Another example is <a href="https://github.com/WebAssembly/binaryen">wasm-opt</a> which does <a href="https://github.com/WebAssembly/binaryen/blob/6fe5103ab58a4eb751998d13768a0f25795a0de6/src/passes/Inlining.cpp#L684-L754">partial inlining</a>.</p>
<h1 id="a-tip-for-effective-inlining-avoid-inlining-in-slow-paths">A tip for effective inlining: avoid inlining in slow paths</h1>
<p>In the <a href="https://github.com/google/protobuf.dart/blob/610943a3bed70c1c2079af5fca02462df10d223f/protobuf/lib/src/protobuf/coded_buffer.dart#L54-L267">protobuf decoding loop</a> that we mentioned above, we have a big type switch to determine how to decode a field. It looks like this:</p>
<pre><code>switch (fieldType) {
  case PbFieldType._OPTIONAL_BOOL:
    fs._setFieldUnchecked(meta, fi, input.readBool());
    break;
  ...
  case PbFieldType._REPEATED_UINT64:
    final list = fs._ensureRepeatedField(meta, fi);
    if (wireType == WIRETYPE_LENGTH_DELIMITED) {
      _readPacked(input, () =&gt; list.add(input.readUint64()));
    } else {
      list.add(input.readUint64());
    }
    break;
  ...
  // 37 cases in total. 
}</code></pre>
<p>Assume that there’s a function that throws an exception (e.g. <code>_ensureRepeatedField</code> above), and it’s used in only once, in one of the <code>case</code>s of the <code>switch</code>.</p>
<p>Because the function is only used once, it may look like inlining it makes sense, as that would eliminate function call overhead, shave at least a few bytes off the binary, and potentially allow optimizations in the call site.</p>
<p>However since this switch is in a hot loop, and the inlined code branch is taken very rarely (unless the application is receiving a lot of malformed input), inlining this slow path can make the instruction cache usage much worse and slow down the decoder.</p>
<p>This kind of inlining can commonly happen when optimizing for size, e.g. with <code>wasm-opt -Os</code>. Because inlining single-use functions reduce binary sizes, optimization modes that aim to make the final binaries smaller can inline slow-path error throwing functions.</p>
<p>If we really want to inline a slow-path function, a way to avoid making instruction cache usage worse is to move the slow-path code to the end of the function, away from the common code paths.</p>
<p>This is often done with branch prediction hints, such as clang’s <a href="https://llvm.org/docs/BranchWeightMetadata.html#builtin-expect"><code>__builtin_expect</code></a>. When a branch is annotated as “not likely to be taken”, the compiler can move the branch target (the basic blocks) to the end of the current function, away from the hot code. This gives us the binary size benefits of inlining single-use functions, without filling the instruction cache with instructions that will never be executed.</p>
<h1 id="remarks">Remarks</h1>
<p>The main reason why the examples above are mostly in Dart is because I’ve been spending most of my time this year optimizing Dart’s Wasm backend’s standard library implementation, so the examples are still fresh in my memory.</p>
<p>The principles apply to most languages: inlining a function makes any information about the arguments available to the function’s body, and any information on its return value to the call site.</p>
<p>A big part of optimizing high-level statically-typed languages is about avoiding polymorphism, boxing, and redundant error checks. I’m not aware of any cases where inlining a function in a high-level language, when it doesn’t result in improving one of these, is worthwhile.</p>
<p>In a lower-level language with monomorphisation and control over allocations (e.g. Rust, C++), monomorphisation eliminates polymorphism in compile time, boxing is explicit, and redundant checks can be avoided by using unchecked (unsafe) functions.</p>
<p>In these cases where programs are often much faster by default, inlining to avoid stack/register shuffling and simplify control flow can make a difference.</p>
<p>One example of simplified control flow making a big difference can be seen in <a href="https://osa1.net/posts/2024-11-29-how-to-parse-3.html">the previous post</a>, where implementing a recursive parsing function in a non-recursive way improved performance by 22%.</p>
<h1 id="updates">Updates</h1>
<ul>
<li><strong>2024-12-07:</strong> Added link to wasm-opt in partial inlining section.</li>
<li><strong>2025-02-12:</strong> Added a higher-order function example.</li>
<li><strong>2025-02-14:</strong> Added a section about inlining in slow paths.</li>
</ul>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>More accurately, a method that can be called in a polymorphic call site needs to have a type that is a subtype of the least-upper-bound of the types of all functions that can be called in the polymorphic call sites.</p>
<p>Or more briefly, all methods in an override group need to have the same type.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></summary>
</entry>
<entry>
    <title>Exploring parsing APIs: the cost of recursion</title>
    <link href="http://osa1.net/posts/2024-11-29-how-to-parse-3.html" />
    <id>http://osa1.net/posts/2024-11-29-how-to-parse-3.html</id>
    <published>2024-11-29T00:00:00Z</published>
    <updated>2024-11-29T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>In the <a href="https://osa1.net/posts/2024-11-22-how-to-parse-1.html">first post</a> of this series we looked at a few different ways of parsing a simple JSON-like language. In the <a href="https://osa1.net/posts/2024-11-28-how-to-parse-2.html">second post</a> we implemented a few lexers, and looked at the performance when the parsers from the first post are combined with the lexers in the second post.</p>
<p>One of the surprising results in these posts is that our recursive descent parser, which parses the input directly to an AST, and in some sense is the simplest possible implementation when we need an AST, actually performs the worst.</p>
<p>(The implementations that collect tokens in a vector before parsing perform worse than recursive descent parsing, but those implementations have other issues as well, and can’t be used in many cases. Maybe I should’ve omitted them entirely.)</p>
<p>We had a lot of benchmarks in the last post, but to keep things simple, let’s consider these three:</p>
<ul>
<li>Recursive descent: 127 Mb/s</li>
<li>tokenize_iter + events_iter to AST: 138 Mb/s</li>
<li>tokenize_push + events_push to AST: 151 Mb/s</li>
</ul>
<p>To recap, the “iter” variants are <code>Iterator</code>s that return one parse (or lexing) event at a time. The “push” variants take a “listener” argument with callbacks for events. See the first post for details.</p>
<p>In the “iter” benchmark, the code that generates the AST iterates an event parser:</p>
<pre><code>fn event_to_tree&lt;I: Iterator&lt;Item = Result&lt;ParseEvent, ParseError&gt;&gt;&gt;(
    parser: &amp;mut I,
    input: &amp;str,
) -&gt; Result&lt;Json, ParseError&gt; {
  ...
}</code></pre>
<p>And the event parser iterates a lexer:</p>
<pre><code>fn parse_events_iter_using_lexer_iter&lt;I: Iterator&lt;Item = Result&lt;(usize, Token), usize&gt;&gt;&gt;(
    lexer: I,
    input_size: usize,
) -&gt; EventParser&lt;I&gt; {
    ...
}</code></pre>
<p>When the AST generator asks for the next parse event, the parse event generator asks for the next token (maybe multiple times) and returns an event. AST generator consumes all of the events and builds the AST.</p>
<p>In the “push” benchmark, we have a “listener” that handles parse events and builds up an AST:</p>
<pre><code>struct AstBuilderListener&lt;&#39;a&gt; {
    input: &amp;&#39;a str,
    container_stack: Vec&lt;Container&gt;,
    current_container: Option&lt;Container&gt;,
    parsed_object: Option&lt;Json&gt;,
    error: Option&lt;ParseError&gt;,
}

impl&lt;&#39;a&gt; EventListener for AstBuilderListener&lt;&#39;a&gt; {
    ...
}</code></pre>
<p>And another listener that handles tokens:</p>
<pre><code>struct LexerEventListenerImpl&lt;&#39;a, L: EventListener&gt; {
    listener: &amp;&#39;a mut L,
    container_stack: Vec&lt;Container&gt;,
    state: ParserState,
}

impl&lt;&#39;a, L: EventListener&gt; LexerEventListener for LexerEventListenerImpl&lt;&#39;a, L&gt; {
  ...
}</code></pre>
<p>This implementation is driven by the lexer which “pushes” the tokens to the event parser, which (sometimes after handling multiple tokens) “pushes” parse events to the AST builder.</p>
<p>Both of these setups are considerably more complicated than recursive descent, yet they perform better. How?</p>
<p>When we consider what the recursive descent parser does that these don’t, it’s kind of obvious. It’s even in the name: recursion.</p>
<p>Our lexers and event parsers all optimize really well: there is no heap allocation in the parsing code (only when building the arrays and objects in the AST, which all implementations have), the code that “pushes” events are all monomorphised based on the handler type, so the handler calls are direct calls and can be (and probably is) inlined. There’s no recursion anywhere.</p>
<p>The recursive descent parser is basically one function that recursively calls itself for nested objects. It turns out this recursion has a cost. When I eliminate the recursion with some more state:</p>
<pre><code>enum ParserState {
    /// Parse any kind of object, update state based on the current container.
    TopLevel,

    /// Parsing a container, parse another element on &#39;,&#39;, or finish the
    /// container on &#39;]&#39; or &#39;}&#39;.
    ExpectComma,

    /// Parsing an object, parse a key.
    ObjectExpectKeyValue,

    /// Parsing an object, parse a key, or terminate the object.
    ObjectExpectKeyValueTerminate,

    /// Parsing an object and we&#39;ve just parsed a key, expect &#39;:&#39;.
    ObjectExpectColon,
}

fn parse_single(iter: &amp;mut Peekable&lt;CharIndices&gt;, input: &amp;str) -&gt; Result&lt;Json, ParseError&gt; {
    let mut container_stack: Vec&lt;Container&gt; = vec![];
    let mut state = ParserState::TopLevel;

    loop {
      ...
    }
}</code></pre>
<p>It performs better than the recursive descent parser and the iterator based parser, and on par with the “push” based parser: (numbers are slightly different than above as I rerun them again together)</p>
<ul>
<li>Recursive descent: 127 Mb/s</li>
<li>tokenize_iter + events_iter to AST: 136 Mb/s</li>
<li>tokenize_push + events_push to AST: 158 Mb/s</li>
<li>Direct parser without recursion: 156 Mb/s</li>
</ul>
<p>I’m not quite sure what about recursion that makes the recursive descent parser perform so much worse, but my guess is that it makes the control flow more complicated to analyze, and in runtime, you have to move things around (in registers and stack locations) based on calling conventions. When moving between registers and stack locations you do memory reads and writes. My guess is that when combined, these cost something.</p>
<h1 id="other-considerations-with-recursion">Other considerations with recursion</h1>
<p>If you checkout the git repo and run the tests with <code>cargo test</code>, you will see that a test fails with a stack overflow.</p>
<p>This is something else to keep in mind when parsing recursively. Stack overflows are a real issue with recursive parsing, and I know some libraries that are <a href="https://github.com/google/protobuf.dart/blob/ccf104dbc36929c0f8708285d5f3a8fae206343e/protobuf/lib/src/protobuf/coded_buffer_reader.dart#L29">explicit about it</a>.</p>
<p>In practice though, I’m not sure if this can be the main reason to avoid recursive parsing. Recursion can happen in other places as well, and in a server application you would probably monitor runtime, memory consumption, and maybe even other resources of a handler, and have some kind of error handler that handles everything else.</p>
<p>Some higher level languages like <a href="https://hackage.haskell.org/package/base-4.20.0.1/docs/GHC-IO-Exception.html#t:AsyncException">Haskell</a> and <a href="https://api.dart.dev/dart-core/StackOverflowError-class.html">Dart</a> make stack overflows exceptions/errors that can be caught and handled, so they can be handled as a part of “unexpected” crashes easily. In Rust, stack overflows can be handled at thread boundaries.</p>
<p>If the application is a command line tool or a compiler, where the input is provided by the user and handled on the user’s computer, it’s less of a problem and you can probably just let the application crash.</p>
<p>So I don’t think we can say that recursion should be avoided at all costs when parsing.</p>
<h1 id="references">References</h1>
<p>As usual, the code is available: <a href="https://github.com/osa1/how-to-parse/tree/main/part3">github.com/osa1/how-to-parse-3</a>.</p>
<p>To work around the stack overflow when testing, test in release mode: <code>cargo test --release</code>.</p>
<p>If you want to profile the code and understand more about why one version is faster than the other, I added 4 executables to the package, one for each benchmark listed above. You can generate a 100M input and run the parsers individually with:</p>
<pre><code>$ cargo build --release
...

$ ./target/release/test_gen 100000000 &gt; input

$ time ./target/release/parse_non_recursive input
./target/release/parse_non_recursive input  0.64s user 0.22s system 99% cpu 0.854 total</code></pre>]]></summary>
</entry>
<entry>
    <title>Exploring parsing APIs: adding a lexer</title>
    <link href="http://osa1.net/posts/2024-11-28-how-to-parse-2.html" />
    <id>http://osa1.net/posts/2024-11-28-how-to-parse-2.html</id>
    <published>2024-11-28T00:00:00Z</published>
    <updated>2024-11-28T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>In the <a href="https://osa1.net/posts/2024-11-22-how-to-parse-1.html">previous post</a> we looked at three different parsing APIs, and compared them for runtime and the use cases they support.</p>
<p>In this post we’ll add a lexer (or “tokenizer”), with two APIs, and for each lexer see how the parsers from the previous post perform when combined with the lexer.</p>
<p><strong>What is a lexer?</strong> A lexer is very similar to the event parsers we saw in the previous post, but it doesn’t try to maintain any structure. It generates “tokens”, which are parts of the program that cannot be split into smaller parts. A lexer doesn’t care about parentheses or other delimiters being balanced, or that values in an array are separated by commas, or anything else. It simply splits the input into tokens.</p>
<p><strong>Why is a lexer useful?</strong> If you already have an event parser, adding a lexer may not allow a lot of new use cases. The main use cases that I’m aware of are:</p>
<ul>
<li><p>Syntax highlighting: when higlighting syntax we don’t care about the tree structure, we care about keywords, punctuation (list separators, dots in paths etc.), delimiters (commas, bracets, brackets), and literals. A lexer gives us exactly these and nothing else.</p></li>
<li><p>Supporting incremental parsing: one way of incrementally update an AST is by starting re-lexing a few (often just one) tokens before the edited token, re-lexing until after the edit location, until generating a token identical to an existing token again. AST nodes of modified tokens are then marked as “modified” and re-parsed.</p>
<p>The details are complicated, I recommend chapter 2 of <a href="https://diekmann.uk/diekmann_phd.pdf">this PhD thesis</a> for an introduction to incremental parsing.</p>
<p>If you need to re-parse code as it gets edited, even if you don’t need or want incremental parsing, incremental lexing is easy, it makes sense to re-lex incrementally and then parse from scratch using the incrementally updated token list, because incremental lexing is so simple.</p></li>
<li><p>For separating complex parsing code into smaller parts: modern languages can have complicated literal syntax, with multiple string literals with varying delimiters (like <code>r#"..."#</code> syntax in Rust, or <code>[=[...]=]</code> in Lua), multiple variants of comments (single line and multi-line, documentation and normal), multiple number syntaxes (with different suffixes like <code>123u32</code> in Rust, underscores to separate digits for readability) and so on.</p>
<p>A lexer separates handling of these from the part of the parser that deals with the program structure.</p></li>
</ul>
<h1 id="the-apis">The APIs</h1>
<p>Similar to the previous post, we will look at three different APIs for lexing:</p>
<ul>
<li>A lexer that generates a list of tokens directly: <code>tokenize_list</code>.</li>
<li>An iterator that generates one token at a time: <code>tokenize_iter</code>.</li>
<li>A “push” API that calls “listener” methods for the tokens: <code>tokenize_push</code>.</li>
</ul>
<p>For our simplified (and enhanced, with comments) JSON, our token type is:</p>
<pre><code>pub enum Token {
    Int(u64),
    Str { size_in_bytes: usize },
    True,
    False,
    Null,
    LBracket,
    RBracket,
    LBrace,
    RBrace,
    Colon,
    Comma,
    Comment { size_in_bytes: usize },
}</code></pre>
<p>Similar to our event type from the previous post, this type needs to be cheap to generate (ideally stack allocated).</p>
<p>The tokens are generated along with byte offsets in the input, also similar to events.</p>
<p>For the push API, the listener interface also directly follows the token type:</p>
<pre><code>pub trait LexerEventListener {
    fn handle_int(&amp;mut self, byte_offset: usize, i: u64);

    fn handle_str(&amp;mut self, byte_offset: usize, size_in_bytes: usize);

    // Similar for other token types.
    ...

    fn handle_error(&amp;mut self, byte_offset: usize);
}</code></pre>
<p>To keep things simple, the event handlers don’t return a <code>bool</code> to stop parsing. It can be added in a few lines of code and it doesn’t affect performance.</p>
<p>Unlike the different types of event parsers from the previous post, implementations of these lexer APIs are almost identical. This is because the lexer has only one state, which is the current position in the input. A <code>next</code> call in the iterator implementation simply continues from the current location in the input, and updates the current location as it reads characters from the input.</p>
<p>The entry points are:</p>
<pre><code>pub fn tokenize_iter&lt;&#39;a&gt;(input: &amp;&#39;a str) -&gt; Lexer&lt;&#39;a&gt; { ... }
  // Lexer implements `Iterator`

pub fn tokenize_push&lt;L: LexerEventListener&gt;(input: &amp;str, listener: &amp;mut L) { ... }

pub fn tokenize_list(input: &amp;str) -&gt; Result&lt;Vec&lt;(usize, Token)&gt; usize&gt; { ... }</code></pre>
<h1 id="combining-with-the-event-parsers">Combining with the event parsers</h1>
<p>We have 3 lexers and 2 event parsers, so 6 combinations in total:</p>
<ol type="1">
<li>tokenize_list + parse_events_iter</li>
<li>tokenize_list + parse_events_push</li>
<li>tokenize_iter + parse_events_iter</li>
<li>tokenize_iter + parse_events_push</li>
<li>tokenize_push + parse_events_iter</li>
<li>tokenize_push + parse_events_push</li>
</ol>
<p>However (5) is not easily possible in Rust. The problem is that a push implementation cannot be converted into an iterator, as it will scan the entire input without ever returning and keep calling the listener methods. To convert a push API into an iterator, we need a language feature that allows us to stop the current thread (or maybe a “fiber”, green thread etc.) and resume it later. In Rust, this is possible with <code>async</code> or threads. Threads are expensive, and <code>async</code> requires a lot of refactoring, and all the call sites to be made <code>async</code> as well.</p>
<p>So in this post we won’t consider this combination.</p>
<h1 id="notes-on-implementations">Notes on implementations</h1>
<p>Implementing these combinations is mostly straightforward. Full code is linked below as usual. The takeaways are:</p>
<ul>
<li>The push API cannot be converted into an iterator API, without language features.</li>
<li>The push API requires state management in the consumer: the consumer will have to save the state that needs to be maintained between the calls to the listener methods.</li>
<li>The iterator API is more flexible as it can be converted into a push API.</li>
<li>The iterator API is also easier to use: the consumer can iterate through the elements in nested loops, and needs less state management. The state can also be function locals, instead of fields of a struct (or class etc.).</li>
<li>The list API (generates an entire vector of tokens) only makes sense when you need to collect all of the tokens in memory. The only use case for this that I’m aware of is incremental parsing.</li>
</ul>
<h1 id="references-and-benchmarks">References and benchmarks</h1>
<p>The code (including benchmarks) is here: <a href="https://github.com/osa1/how-to-parse/tree/main/part2">github.com/osa1/how-to-parse-2</a>.</p>
<p><strong>Token generation benchmarks:</strong> Collect all of the tokens in a <code>Vec</code>.</p>
<ul>
<li>tokenize_list: 305 MB/s</li>
<li>tokenize_push: 303 MB/s</li>
<li>tokenize_iter: 329 MB/s</li>
</ul>
<p>In the event generation benchmarks in the last post, the push implementation is about 10% faster than the iterator. But in the lexer, the iterator is faster when collecting the tokens in a vector. It looks like when the state that the parser manages between the <code>next</code> calls gets simpler, the compiler is able to optimize the code better, and iterator implementation beats the push implementation.</p>
<p>The vector generator and push implementation adding the elements to a vector via the listener perform the same, which shows that when monomorphised, the push implementation optimizes quite well for simple cases (but also in complex cases, as we will see below). In languages without monomorphisation, the push API should be slower.</p>
<p><strong>Tokens to events:</strong> Convert tokens to events.</p>
<ul>
<li>events_iter: 282 MB/s</li>
<li>events_push: 315 MB/s</li>
<li>tokenize_list + events_iter: 181 MB/s</li>
<li>tokenize_list + events_push: 187 MB/s</li>
<li>tokenize_iter + events_iter: 269 MB/s</li>
<li>tokenize_iter + events_push: 275 MB/s</li>
<li>tokenize_push + events_push: 351 MB/s</li>
</ul>
<p>The first two benchmarks are the ones from the previous post that don’t use a lexer, generate events directly. The numbers are slightly different than the numbers from the previous post as I rerun them again.</p>
<p>If you need some kind of incremental implementation, scanning the entire input and collecting the events or tokens in a vector performs bad. There’s no point in combining the list API with push or iterator APIs.</p>
<p>What’s surprising is that the push lexer implementation combined with the push event generator implementation performs better than the event generator implementation that parses the input directly without a lexer. I don’t have an explanation to why, yet.</p>
<p>Lexer iterator implementations combined with any of the event generation implementations perform slower than the event push implementation that parses the input directly, but about as fast as the event iterator implementation that parses the input directly.</p>
<p><strong>Tokens to AST:</strong> Converts tokens to events, builds AST from the events.</p>
<ul>
<li>Recursive descent: 127 MB/s</li>
<li>events_iter to AST: 140 MB/s</li>
<li>events_push to AST: 145 MB/s</li>
<li>tokenize_list + events_iter to AST: 108 MB/s</li>
<li>tokenize_list + events_push to AST: 108 MB/s</li>
<li>tokenize_iter + events_iter to AST: 138 MB/s</li>
<li>tokenize_iter + events_push to AST: 139 MB/s</li>
<li>tokenize_push + events_push to AST: 151 MB/s</li>
</ul>
<p>The first three benchmarks below are from the last post. Rerun and included here for comparison.</p>
<p>When we add an AST building step, which is more complicated compared to the rest of steps, the performance difference between the most convenient implementation (tokenize_iter + events_iter to AST) and the most performant one (tokenize_push + events_push to AST) diminishes. In the event generation benchmark, the fast one is 30% faster, but when building an AST, it’s only 9% faster.</p>
<p>The push implementation is still faster than the recursive descent parser, even with the extra lexing step. I’m planning to investigate this further in a future post.</p>]]></summary>
</entry>
<entry>
    <title>Exploring parsing APIs: what to generate, and how</title>
    <link href="http://osa1.net/posts/2024-11-22-how-to-parse-1.html" />
    <id>http://osa1.net/posts/2024-11-22-how-to-parse-1.html</id>
    <published>2024-11-22T00:00:00Z</published>
    <updated>2024-11-22T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>Consider a simplified and enhanced version of JSON, with these changes:</p>
<ul>
<li>Numbers are 64-bit unsigned integers.</li>
<li>Strings cannot have control and escape characters.</li>
<li>Single-line comments are allowed, with the usual syntax: <code>// ...</code> .</li>
</ul>
<p>When parsing a language like this, a common first step if to define an “abstract syntax tree” (AST), with only the details we want from the parser output.</p>
<p>For example, if we’re implementing a tool like <a href="https://jqlang.github.io/jq/">jq</a>, the AST may look like:</p>
<pre><code>enum Json {
    Int(u64),
    Str(String),
    Bool(bool),
    Array(Vec&lt;Json&gt;),
    Object(Vec&lt;(String, Json)&gt;),
    Null,
}</code></pre>
<p>This type is called an “abstract” syntax tree because it abstracts the unnecessary details from the parse output. In our tool we don’t need locations of nodes and comments, so the AST doesn’t contain them.</p>
<p>It’s easy to implement a parser for this AST: we iterate the input, skip whitespace and comments, then based on the next character decide what type of node (integer, string, etc.) to parse. For nested <code>Json</code> nodes in arrays and objects, we recursively call the parser.</p>
<p>This kind of parser is called a “recursive descent parser”. For our AST above, the parser looks like this:</p>
<pre><code>// The entry point: parses all of the input to JSON.
pub fn parse(input: &amp;str) -&gt; Result&lt;Json, JsonParseError&gt; {
    let mut iter = input.char_indices().peekable();
    let (_, json) = parse_single(&amp;mut iter, input)?;
    skip_trivia(&amp;mut iter)?;
    // Check that all of the input is consumed.
    ...
}

// Parse a single Json. After parsing, the input may have more characters to be parsed.
fn parse_single(
    iter: &amp;mut Peekable&lt;CharIndices&gt;,
    input: &amp;str,
) -&gt; Result&lt;(usize, Json), ParseError&gt; {
    // Skip whitespace and comments.
    skip_trivia(iter)?;

    // Get next character.
    let (byte_offset, char) = match iter.next() { ... }

    if char == &#39;[&#39; {
        // Parse an array. Call `parse_single` recursively for elements.
        ...
    }

    if char == &#39;{&#39; {
        // Parse an object. Call `parse_single` recursively for values.
        ...
    }

    if char == &#39;t&#39; {
        // Parse keyword &quot;true&quot;.
        ...
    }

    // Same for other keywords, integers, strings.
    ...
}</code></pre>
<p>While very common, this kind of parsers are inflexible, and slower than more flexible alternatives for many use cases.</p>
<p>Consider these use cases:</p>
<ul>
<li><p>A JSON formatter: a formatter needs to know about comments to be able to keep them in the formatted code. To support this use case, the AST needs to include comments too, which will make it larger, and parsing will be less efficient for the applications that don’t need comments.</p></li>
<li><p>A configuration file parser for a text editor: to be able to show error locations in configuration errors (such as an invalid value used for a setting), the AST will have to include source locations. Similar to above, this will make the AST larger and slower to parse for other applications that don’t need source locations.</p></li>
<li><p>An RPC server that looks at the command name in incoming JSON messages and relays the messages based on the command name: the server doesn’t even need a full parser, just a parser that can keep track of nesting level so that it can extract the request name field at the right level will suffice. Using a full AST parser will parse the whole message and be inefficient.</p></li>
<li><p>A log sorting tool that reads a file with one JSON log per line, sorts the lines based on top-level “timestamp” field values. Similar to the use case above, this tool only needs to read one field and parsing whole lines is wasteful.</p></li>
</ul>
<p>A well-known solution to these is to introduce a lower level parser that doesn’t generate a fully structured output like an AST, but a stream of “parse events”. These events should be general enough to allow different use cases like the ones we listed above, and should be cheap to allocate and pass around, ideally as stack allocated values, so that applications that don’t need them can skip them efficiently.</p>
<p>This type of parsing is often called “event driven parsing”. In our JSON variant, the events look like this:</p>
<pre><code>/// A parse event, with location of the event in the input.
pub struct ParseEvent {
    pub kind: ParseEventKind,
    pub byte_offset: usize,
}

/// Details of a parse event.
pub enum ParseEventKind {
    StartObject,
    EndObject,
    StartArray,
    EndArray,
    Int(u64),
    Str {
        /// Size of the string, not including the double quotes.
        size_in_bytes: usize,
    },
    Bool(bool),
    Null,
    Comment {
        /// Size of the comment, including the &quot;//&quot; a the beginning and newline at the end.
        size_in_bytes: usize,
    },
}</code></pre>
<p>Note that there’s no heap allocation required for these events. Contents of strings and comments can be obtained by slicing the input using the event location and <code>size_in_bytes</code> field.</p>
<p>When generating these event, it’s important that we don’t scan the whole input and collect all of the events in a list, as that would mean some of the users, like our RPC server and log sorted examples above, would have to do more work than necessary.</p>
<p>This means that the parser will have to be stateful: after returning an event, it needs to be able to continue from the last event location. This complicates the parser implementation quite a bit. Here’s how the parser looks like at a high level:</p>
<pre><code>// The entry point. Use via the `Iterator` interface.
pub fn parse_events(input: &amp;str) -&gt; EventParser {
    EventParser::new(input)
}

// The parser state.
pub struct EventParser&lt;&#39;a&gt; {
    input: &amp;&#39;a str,
    byte_offset: usize,
    container_stack: Vec&lt;Container&gt;,
    state: ParserState,
}

enum Container {
    Array,
    Object,
}

enum ParserState {
    /// Parse any kind of object, update state based on the current container.
    TopLevel,

    /// Finished parsing a top-level object, expect end-of-input.
    Done,

    /// Parsing an object, parse another element on &#39;,&#39;, or finish the array on &#39;}&#39;.
    ObjectExpectComma,

    /// Parsing an object, parse the first element, or finish the array on &#39;]&#39;.
    ObjectExpectKeyValue,

    /// Parsing an object and we&#39;ve just parsed a key, expect &#39;:&#39;.
    ObjectExpectColon,

    /// Parsing an array, parse another element on &#39;,&#39;, or finish the array on &#39;]&#39;.
    ArrayExpectComma,
}

impl&lt;&#39;a&gt; Iterator for EventParser&lt;&#39;a&gt; {
    type Item = Result&lt;ParseEvent, ParseError&gt;;

    fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt; {
        match self.state {
            ParserState::TopLevel =&gt; self.top_level(),
            ParserState::Done =&gt; self.done(),
            ParserState::ObjectExpectComma =&gt; self.object_expect_comma(),
            ParserState::ObjectExpectKeyValue =&gt; self.object_expect_key_value(),
            ParserState::ObjectExpectColon =&gt; self.object_expect_colon(),
            ParserState::ArrayExpectComma =&gt; self.array_expect_comma(),
        }
    }
}

...</code></pre>
<p>The main complexity of this parser comes from the fact that it cannot return an event and keep running, the caller needs to call the relevant method (<code>next</code> from the <code>Iterator</code> trait above) to keep parsing. To be able to continue from where it’s left, the parser needs to maintain some state outside of the parse functions.</p>
<p>This parser is general enough to allow implementing our original AST parser:</p>
<pre><code>pub fn event_to_tree&lt;I: Iterator&lt;Item = Result&lt;ParseEvent, ParseError&gt;&gt;&gt;(
    parser: &amp;mut I,
    input: &amp;str,
) -&gt; Result&lt;Json, ParseError&gt; {
    let mut container_stack: Vec&lt;Container&gt; = vec![];
    let mut current_container: Option&lt;Container&gt; = None;
    let mut parsed_object: Option&lt;Json&gt; = None;

    for event in parser.by_ref() {
        match event {
            ...
        }
    }

    Ok(parsed_object.unwrap())
}</code></pre>
<p>But it also allows parsing to an AST with comments (for our formatter), source locations (for our configuration parser), and our RPC server and log sorter. Here’s how the timestamp parser that stops after finding the field looks like:</p>
<pre><code>/// Parse the &quot;timestamp&quot; field at the top-level map of the JSON.
pub fn parse_timestamp(log_line: &amp;str) -&gt; Result&lt;Option&lt;u64&gt;, ParseError&gt; {
    let mut container_depth: u32 = 0;
    let mut expect_timestamp = false;

    for event in parse_events(log_line) {
        let ParseEvent { kind, byte_offset } = match event {
            Ok(event) =&gt; event,
            Err(err) =&gt; return Err(err),
        };

        let expect_timestamp_ = expect_timestamp;
        expect_timestamp = false;

        match kind {
            ParseEventKind::StartObject =&gt; {
                container_depth += 1;
            }

            ParseEventKind::EndObject =&gt; {
                container_depth -= 1;
            }

            ParseEventKind::StartArray =&gt; {
                if container_depth == 0 {
                    // Array at the top level, the line does not contain the field.
                    return Ok(None);
                }
                container_depth += 1;
            }

            ParseEventKind::EndArray =&gt; {
                container_depth -= 1;
            }

            ParseEventKind::Str { size_in_bytes } =&gt; {
                if container_depth != 1 {
                    continue;
                }
                let str = &amp;log_line[byte_offset..byte_offset + size_in_bytes];
                expect_timestamp = str == &quot;timestamp&quot;;
            }

            ParseEventKind::Int(i) =&gt; {
                if expect_timestamp_ {
                    return Ok(Some(i));
                }
            }

            ParseEventKind::Bool(_)
            | ParseEventKind::Null
            | ParseEventKind::Comment { .. } =&gt; {}
        }
    }

    Ok(None)
}</code></pre>
<p>A nice property of this parser is that it does not allocate at all. It doesn’t build an AST (so no heap-allocated vectors), and parse events are 24-byte stack allocated values. The event parser is also stack allocated by this function.</p>
<p>An alternative design to this that is slightly less flexible and more difficult to use, but easier to implement and faster is what’s sometimes called a “push parser”.</p>
<p>The idea is that, instead of returning one event at a time, the parser takes a “listener” argument, and calls the listener callbacks for each event generated. The listener type directly follows our event type above:</p>
<pre><code>// Methods return a `bool` indicating whether to continue parsing after the event.
pub trait EventListener {
    fn handle_start_object(&amp;mut self, _byte_offset: usize) -&gt; bool {
        true
    }

    fn handle_end_object(&amp;mut self, _byte_offset: usize) -&gt; bool {
        true
    }

    fn handle_start_array(&amp;mut self, _byte_offset: usize) -&gt; bool {
        true
    }

    fn handle_end_array(&amp;mut self, _byte_offset: usize) -&gt; bool {
        true
    }

    fn handle_int(&amp;mut self, _byte_offset: usize, _i: u64) -&gt; bool {
        true
    }

    fn handle_str(&amp;mut self, _byte_offset: usize, _size_in_bytes: usize) -&gt; bool {
        true
    }

    fn handle_bool(&amp;mut self, _byte_offset: usize, _b: bool) -&gt; bool {
        true
    }

    fn handle_null(&amp;mut self, _byte_offset: usize) -&gt; bool {
        true
    }

    fn handle_comment(&amp;mut self, _byte_offset: usize, _size_in_bytes: usize) -&gt; bool {
        true
    }

    fn handle_error(&amp;mut self, _error: ParseError);
}</code></pre>
<p>The parser:</p>
<pre><code>// The entry point. Parse all of the input, call `listener` with the events.
pub fn parse&lt;L: EventListener&gt;(input: &amp;str, listener: &amp;mut L) {
    let mut iter = input.char_indices().peekable();
    let input_size = input.len();

    // Parse a single JSON.
    if !parse_single(&amp;mut iter, input_size, listener) {
        return;
    }

    // Check that all of the input is consumed.
    ...
}

// Returns whether an error was reported.
fn parse_single&lt;L: EventListener&gt;(
    iter: &amp;mut Peekable&lt;CharIndices&gt;,
    input_size: usize,
    listener: &amp;mut L,
) -&gt; bool {
    // Skip whitespace and comments, generate events for comments.
    skip_trivia!(iter, listener);

    // Get next character.
    let (byte_offset, char) = match iter.next() {
        Some(next) =&gt; next,
        None =&gt; {
            listener.handle_error(ParseError {
                byte_offset: input_size,
                reason: &quot;unexpected end of input&quot;,
            });
            return false;
        }
    };

    if char == &#39;[&#39; {
        // Parse an array. Call `parse_single` recursively for elements.
        ...
    }

    if char == &#39;{&#39; {
        // Parse an object. Call `parse_single` recursively for values.
        ...
    }

    if char == &#39;t&#39; {
        // Parse keyword &quot;true&quot;.
        ...
    }

    // Same for other keywords, integers, strings.
    ...
}</code></pre>
<p>Note that the parser functions are identical (in terms of names and what they do) to our simple recursive descent parser. This is because the parser no longer needs to maintain state to be able to return and continue from where it was left, as it does all of the work in one go. Instead of building an AST or a list of events, it takes an <code>EventListener</code> argument and calls the handle methods.</p>
<p>This is a bit less convenient to use, but it’s still flexible enough to build an AST. An <code>EventListener</code> implementation that builds up a <code>Json</code> AST looks like this:</p>
<pre><code>pub struct AstBuilderListener&lt;&#39;a&gt; {
    input: &amp;&#39;a str,
    container_stack: Vec&lt;Container&gt;,
    current_container: Option&lt;Container&gt;,
    parsed_object: Option&lt;Json&gt;,
    error: Option&lt;ParseError&gt;,
}

impl&lt;&#39;a&gt; EventListener for AstBuilderListener&lt;&#39;a&gt; {
    ...
}</code></pre>
<p>However, if you need to be able to stop parsing and continue later, this parser can’t do that.</p>
<p>The main advantage of this parser is that, with the right programming language and parser design, it can be faster than the alternatives, while still being flexible enough for most use cases. See below for benchmarks.</p>
<hr />
<h1 id="aside-event-parsing-vs.-lexing">Aside: event parsing vs. lexing</h1>
<p>Our <code>ParseEvent</code> type has no nested data and looks like what we could define as the “tokens” in a parser for a programming language.</p>
<p>So it shouldn’t be surprising that we can use a lexer generator to implement a parse event generator:</p>
<pre><code>// Same `parse_events` as above, but uses a generated lexer.
pub fn parse_events(input: &amp;str) -&gt; LexgenIteratorAdapter {
    LexgenIteratorAdapter {
        lexer: Lexer::new(input),
    }
}

// An adapter is necessary to convert lexgen values to `parse_events` items.
pub struct LexgenIteratorAdapter&lt;&#39;a&gt; {
    lexer: Lexer&lt;&#39;a, std::str::Chars&lt;&#39;a&gt;&gt;,
}

impl&lt;&#39;a&gt; Iterator for LexgenIteratorAdapter&lt;&#39;a&gt; {
    type Item = Result&lt;ParseEvent, ParseError&gt;;

    fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt; {
        ...
    }
}

struct LexerState {
    container_stack: Vec&lt;Container&gt;,
}

lexgen::lexer! {
    Lexer(LexerState) -&gt; ParseEvent;

    type Error = &amp;&#39;static str;

    let comment = &quot;//&quot; (_ # &#39;\n&#39;)* &#39;\n&#39;;

    rule Init {
        $$ascii_whitespace,

        $comment =&gt; comment,

        &#39;[&#39; =&gt; ...,

        &#39;]&#39; =&gt; ...,

        &#39;{&#39; =&gt; ...,

        &quot;true&quot; =&gt; ...,

        &quot;false&quot; =&gt; ...,

        &quot;null&quot; =&gt; ...,

        [&#39;0&#39;-&#39;9&#39;]+ =&gt; ...,

        &#39;&quot;&#39; (_ # &#39;&quot;&#39;)* &#39;&quot;&#39; =&gt; ...
    }

    rule Done { ... }

    rule ArrayExpectComma { ... }

    rule ObjectExpectKeyValue { ... }

    rule ObjectExpectColon { ... }

    rule ObjectExpectComma { ... }
}</code></pre>
<p>This uses <a href="https://github.com/osa1/lexgen">lexgen</a>. lexgen generates slightly different values than what we want, so we have a <code>map</code> in the entry point to convert the lexgen values.</p>
<p>The main difference between an event parser and lexer is that an event parser maintains some of the structure of the parsed format. For example, we check that brackets are balanced, after a key in a map a colon follows, and so on.</p>
<p>A lexer generator can be used to implement an event parser, as demonstrated above.</p>
<hr />
<h1 id="references-and-benchmarks">References and benchmarks</h1>
<p>All of the code in this blog post, and more, is here: <a href="https://github.com/osa1/how-to-parse/tree/main/part1">github.com/osa1/how-to-parse</a>.</p>
<p>In the benchmark program (run with <code>cargo bench</code>), we generate a 10M large JSON, and parse it to either an AST or a vector of events.</p>
<p><strong>AST building benchmarks:</strong></p>
<ul>
<li><p>Recursive descent: the recursive descent parser that generates an AST.</p>
<p>Throughput: 128 Mb/s.</p></li>
<li><p>Event generator to AST: the iterator-style event generator, events processed by <code>event_to_tree</code> to build an AST.</p>
<p>Throughput: 138 Mb/s.</p></li>
<li><p>Lexgen event to AST: same as above, but the event parser is implemented with lexgen.</p>
<p>Throughput: 106 Mb/s.</p></li>
<li><p>Push event parser to AST: the “push” event parser, <code>AstBuilderListener</code> as the event listener.</p>
<p>Throughput: 147 Mb/s.</p></li>
</ul>
<p><strong>Event generation benchmarks:</strong> (collect events in a <code>Vec</code>)</p>
<ul>
<li><p>Parse events: the iterator-style event generator.</p>
<p>Throughput: 274 Mb/s.</p></li>
<li><p>Parse events lexgen: the lexgen-generated event generator.</p>
<p>Throughput: 179 Mb/s.</p></li>
<li><p>Parse events via push: the push event parser, events added to a <code>Vec</code> via by the listener.</p>
<p>Throughput: 304 Mb/s.</p></li>
</ul>
<p><strong>Notes:</strong></p>
<ul>
<li><p>lexgen-generated event parser is the slowest, but I think it should be possible to make it perform at least as good as the hand-written one. So far I’ve spent very little time to optimize lexgen’s code generator.</p></li>
<li><p>Push-based implementation is faster than the iterator-style implementation, both for generating events in a list, and also for building an AST.</p>
<p>The main advantage of the push-based implementation is that the control flow is as simple as the recursive descent parsing (contained within parse functions, as opposed to externally in a struct), as it does all of the parsing in one go. It looks like managing the parser state externally in a struct is not free.</p></li>
<li><p>I think the tradeoffs between the push-based and iterator implementations will be different in most high-level languages without control over allocations and monomorphisation.</p>
<ul>
<li><p>In the Rust implementation, events are stack allocated values, which will be heap-allocated objects in some of the other languages.</p></li>
<li><p>In the push-based implementation, the parser is monomorphised based on the listener type. Both the listener and parser are stack allocated. All event handler method calls are direct calls (as opposed to virtual, or via some other dynamic invocation method), which can be inlined. None of these will be the case in, e.g., Haskell and Dart.</p></li>
</ul>
<p>It would be interesting to implement the same in some other languages to see how they perform relative to each other.</p></li>
<li><p>I’m not sure why the recursive descent parser is not at least as fast as the push-based implementation, and not faster than the iterator-style one. If you have any insights into this, please let me know.</p></li>
</ul>
<h1 id="more-use-cases">More use cases</h1>
<p>The use cases described at the beginning of the post are all extracted from real-world use cases of various other formats.</p>
<p>Here are more use cases that require flexible and fast parser design:</p>
<ul>
<li><p>“Outline” views in text editors or online code browsing tools may want to process top-level definitions, and definitions nested in <code>class</code>, <code>impl</code>, and similar blocks. Parsing the whole file to an AST would be inefficient.</p></li>
<li><p>Syntax-aware code search tools like <a href="https://github.com/osa1/sg">sg</a> can implement searching only in identifiers, string literals, comments with an event-based parser. This could also be implemented with a lexer.</p></li>
<li><p>As mentioned in a <a href="https://osa1.net/posts/2024-11-04-resumable-exceptions.html">previous post</a>, ideally a formatter, language server, compiler, and refactoring tools, should reuse as much parsing code as possible. It’s difficult to do this with an AST parser, as the AST would have too much information for each of these tools. Event-based parsing makes this easier.</p></li>
</ul>
<h1 id="event-parsing-examples-from-programming-languages">Event parsing examples from programming languages</h1>
<p>I think event-driven parsing is common in some languages when parsing data formats like XML, but less common for parsing programming languages. Two examples that I’m aware of that applies the ideas to programming languages:</p>
<ul>
<li><p>rust-analyzer’s parser is <a href="https://github.com/rust-lang/rust-analyzer/blob/c0bbbb3e5d7d1d1d60308c8270bfd5b250032bb4/docs/dev/architecture.md#cratesparser">a hand written one that generates events</a>. The architecture documentation mentions that Kotlin uses a similar idea:</p>
<blockquote>
<p>It is a hand-written recursive descent parser, which produces a sequence of events like “start node X”, “finish node Y”. It works similarly to kotlin’s parser, which is a good source of inspiration for dealing with syntax errors and incomplete input</p>
</blockquote></li>
<li><p>Dart’s parser <a href="https://github.com/dart-lang/sdk/blob/19da943583e020e96026f797904dc5c6b993d4ac/pkg/_fe_analyzer_shared/lib/src/parser/listener.dart#L35-L46">uses the push-based API</a>. This parser is the only Dart language parser used by the SDK. It’s used by the analyzer, language server, compilers, and anything else that the SDK includes.</p></li>
</ul>]]></summary>
</entry>
<entry>
    <title>Resumable exceptions</title>
    <link href="http://osa1.net/posts/2024-11-04-resumable-exceptions.html" />
    <id>http://osa1.net/posts/2024-11-04-resumable-exceptions.html</id>
    <published>2024-11-04T00:00:00Z</published>
    <updated>2024-11-04T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>The main use case of resumable exceptions would be collecting a bunch of errors (instead of bailing out after the first one) to log or show to the user, or actually recovering and continuing from the point of error detection, rather than in a call site.</p>
<p><em>Why not design the code to allow error recovery, instead of using a language feature?</em> There are a few problems with this:</p>
<ul>
<li><p>Without a language feature to do this, libraries will have to implement their own ways to recover from errors, causing inconsistencies and fragmented ecosystem of error handling libraries.</p></li>
<li><p>With resumable exceptions, any code can be trivially made to transfer control to a exception handler, and back. Manually refactoring code to do the same can be a big task. This may even be infeasible.</p></li>
<li><p>With resumable exceptions as a part of the language, libraries will be designed with resumption in mind. Libraries that would normally not allow error recovery will allow error recovery, as it will be easy to do, and it will be a common thing to resume from errors.</p></li>
</ul>
<h1 id="example-use-case-parser-shared-by-a-compiler-and-a-language-server">Example use case: parser shared by a compiler and a language server</h1>
<p>Modern programming languages have complex syntax. Parsers for these languages are often thousands of lines of code.</p>
<p>Ideally, all tooling for a language would share the parser, as it’s a significant amount of work to implement, debug, maintain parsers for such large languages.</p>
<p>However not all of these tools will have the same error handling behavior. A compiler <em>cannot</em> continue in the presence of a parse error, but a language server <em>has to</em> continue.</p>
<p>With resumable exceptions, the compiler can abort after a parse error, the language server can provide placeholder AST nodes for failing parse operations and resume. This flexibility does not make the parser API any more complicated than a parser that throws an exception in any other language. A one-off refactoring script that uses the parser library doesn’t have to deal with error recovery just because the language server, which uses the same parser, needs to recover from parse errors and continue parsing.</p>
<h1 id="types-of-resumable-exceptions">Types of resumable exceptions</h1>
<p>With resumable exceptions <code>throw</code> expressions generate a value. The value depends on the exception type thrown. For example, a <code>FooDecodingException</code> can be resumed with a value of <code>Foo</code> provided by the handler.</p>
<p>This can be implemented with an abstract base class or typeclass/trait with a type parameter:</p>
<pre><code>// in a system with classes:
abstract class ResumableException&lt;Resume&gt; { 
    prim Resume throw();
    prim Never resume(Resume resumptionValue);
}

// or in a system with typeclasses/traits:
trait ResumableException&lt;Resume&gt; {
    prim fn throw(self) -&gt; Resume;
    prim fn resume(resumptionValue: Resume) -&gt; !;
}</code></pre>
<p>Here the <code>prim</code> keyword indicates that the <code>throw</code> and <code>resume</code> methods are provided by the compiler.</p>
<p><code>throw e</code> can then be type checked as <code>e.throw()</code>, and <code>resume e with value</code> can be type checked as <code>e.resume(value)</code>. Or we can use the function call syntax instead of special syntax for throwing and resuming.</p>
<p>Whether to make exceptions thrown by a function a part of its type signature or not is an orthogonal concern.</p>
<h1 id="exception-type-design-considerations">Exception type design considerations</h1>
<p>The same considerations when designing non-resumable exceptions apply to resumable exceptions:</p>
<ul>
<li>The more general an exception type gets, the less you can do with it.</li>
<li>We still want to distinguish “log and stop” kind of exceptions from recoverable ones.</li>
</ul>
<p>For example, it doesn’t make sense to resume from an <a href="https://api.dart.dev/stable/3.5.4/dart-core/ArgumentError-class.html"><code>ArgumentError</code></a>, so we don’t implement <code>ResumableException</code> for it.</p>
<p>To be able to meaningfully resume from an exception, the exception type should document when exactly it is thrown, or have a resumption value type that is specific enough to give an idea on when it is thrown.</p>
<p>For example, an exception that can be resumed with an <code>int</code> cannot be resumed without knowing what that <code>int</code> is going to be used for, so this should be documented. But an exception <code>FooDecodingError implements ResumableException&lt;Foo&gt;</code> makes it clear that it’s thrown when there’s an error during decoding a <code>Foo</code>, and the resumption value is the value to be used as the <code>Foo</code> being decoded.</p>]]></summary>
</entry>
<entry>
    <title>Idea: a more structural code editor</title>
    <link href="http://osa1.net/posts/2024-11-02-structural-editor.html" />
    <id>http://osa1.net/posts/2024-11-02-structural-editor.html</id>
    <published>2024-11-02T00:00:00Z</published>
    <updated>2024-11-02T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>Code is tree structured, but manipulated as a sequence of characters.</p>
<p>Most language tools<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> need to convert these sequence of characters to the tree form as the first thing to be able to do anything.</p>
<p>When the program is being edited, the tree structure is often broken, and often to the point where the tool cannot operate.</p>
<p>For example:</p>
<ul>
<li>An opening parenthesis, brace, or bracket, without a matching closing one</li>
<li>An unterminated string literal or multi-line comment</li>
<li>A keyword inserted at a wrong place, or without the right tokens afterwards</li>
</ul>
<p>These can make it impossible to main the tree structure of the code.</p>
<p>Since these cases are common, tools need to deal with these. A lot of time and effort is spent on error recovery so that when one of these common cases occur, the tool can still operate and do something useful.</p>
<p>For some tools handling these cases is a requirement: many of the language server functions need to work even when the code is being edited and not in a valid state. For example, “go to definition” should work, “outline” shouldn’t be reset every time the user inserts an opening brace, bracket, or parenthesis.</p>
<p><em>We can’t invent a new language</em> to solve this problem: this creates a thousand new problems, each bigger than the one we are trying to solve. Designing and implementing a new language is major undertaking on its own. We can’t design and implement a language <em>and</em> an experimental code editor at the same time, and succeed in both.</p>
<p>So we want need to support existing languages, <em>but existing languages are incredibly complex</em>, sometimes with a hundred kinds of statements, expressions, types, and so on.</p>
<p>What I’d like to propose as a solution is a “mostly structural” editor, where programs are edited in a structural way at the highest levels, but as text at the statement and expression level.</p>
<p>The details depend on the language. As an example, let’s consider Rust. In Rust, packages (called “crates”), modules, and the items in modules (function and type definitions) can be defined structurally, because there aren’t a lot of different kinds of top-level declarations. Then in the function (and method) bodies, we write the code as text, as usual.</p>
<p>The advantages of this approach are:</p>
<ul>
<li><p>We avoid inventing a new language. The idea can be applied to most languages.</p></li>
<li><p>Because we isolate invalid syntax to function bodies, no edit can cause syntax errors in the other functions in the same module, or in the other modules and packages.</p></li>
<li><p>Because we define function and method signatures separately from function/method bodies, syntax errors cannot invalidate types and cannot generate type errors outside of the function being edited.</p></li>
<li><p>For the same reason as above, “outline” view in the IDE is never broken. Functions like “go to definition” and “find references” always work.</p></li>
</ul>
<p>As for the GUI part, I imagine an editor “pane” for each function being edited. I should be able to quickly switch between functions (maybe with a fuzzy search similar to <code>ctrl-p</code> in some editors), and when working on a function I should be able to quickly open documentation or definitions of the symbols used in the function, in new panes. I imagine there will be a lot of panes open at any time. This may require a solution like a tiling window manager to quickly arrange them and switch between them.</p>
<p>This problem is not new, I do a lot of buffer/split management every day while coding, and almost never use just a single editor window. However with each pane editing just one function, there will be a lot of splits and panes. Some creativity will be needed here to make managing these panes easy for the users.</p>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>I’m not aware of any language tool that doesn’t need to parse the source. Please let me know if you know such tools.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>]]></summary>
</entry>

</feed>
