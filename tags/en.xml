<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>osa1.net - Posts tagged en</title>
    <link href="http://osa1.net/tags/en.xml" rel="self" />
    <link href="http://osa1.net" />
    <id>http://osa1.net/tags/en.xml</id>
    <author>
        <name>Ömer Sinan Ağacan</name>
        <email>omeragaca@gmail.com</email>
    </author>
    <updated>2015-08-13T00:00:00Z</updated>
    <entry>
    <title>The issue with work sharing(common subexpression elimination)</title>
    <link href="http://osa1.net/posts/2015-08-13-the-issue-with-work-sharing.html" />
    <id>http://osa1.net/posts/2015-08-13-the-issue-with-work-sharing.html</id>
    <published>2015-08-13T00:00:00Z</published>
    <updated>2015-08-13T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>I’d expect more work sharing to be always more beneficial. But apparently this is not the case, as pointed out in (Chitil, 1997)<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>.</p>
<p>Here’s an example from the paper: (slightly changed)</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell">sum [<span class="dv">1</span> <span class="fu">..</span> <span class="dv">1000</span>] <span class="fu">+</span> sum [<span class="fu">-</span><span class="dv">1000</span> <span class="fu">..</span> <span class="fu">-</span><span class="dv">1</span>] <span class="fu">+</span> prod [<span class="dv">1</span> <span class="fu">..</span> <span class="dv">1000</span>]</code></pre>
<p>We can evaluate this expression to WHNF using heap space enough for a single list(to be more specific, we only need a single cons cell at any time). After evaluating a subexpression, we can deallocate and allocate for the next list etc.</p>
<p>However, if we eliminate common subexpressions, and generate this code:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">let</span> v <span class="fu">=</span> [<span class="dv">1</span> <span class="fu">..</span> <span class="dv">1000</span>]
 <span class="kw">in</span> sum v <span class="fu">+</span> sum [<span class="fu">-</span><span class="dv">1000</span> <span class="fu">..</span> <span class="fu">-</span><span class="dv">1</span>] <span class="fu">+</span> prod v</code></pre>
<p>Now <code>v</code> has to live until the let body is evaluated to a value. We win in allocation/deallocation side, but we lose in residency side. In paper’s words: “Whereas the transformation always decreases total heap usage, it may considerably influence heap residency.”</p>
<p>In general, we can’t do this transformation, without risking increased residency:</p>
<p>\[ e’[e,e] \leadsto \texttt{let}\; x = e\; \texttt{in}\; e’[x,x] \]</p>
<p>As a solution, the paper suggests this:</p>
<ol style="list-style-type: decimal">
<li>We always do CSE if the subexpressions’ WHNF == NF(i.e. if it’s a “safe type” in paper’s terms). According to the paper, “a partially evaluated expression is certain to require only a small, fixed amount of space if it’s not a function, whose environment may refer to arbitrary large data structures, and its WHNF is already its normal form”.</li>
<li>We always do CSE when a named expression is syntactically dominating another equal expression:</li>
</ol>
<p>\[ \texttt{let}\; x = e\; \texttt{in}\; e’[e] \leadsto \texttt{let}\; x = e\; \texttt{in}\; e’[x] \]</p>
<hr />
<p>Note that (1) is not always true, assume an expression with type <code>ForeignPtr a</code> where <code>a</code> is a huge FFI object. This has WHNF == NF property, but it may increase residency significantly. Maybe GHC didn’t have FFI at the time the paper is written.</p>
<p>Also, I’m wondering how is CSE is handled in current GHC.</p>
<hr />
<p>In supercompilation, we want to avoid evaluating same expressions in a loop forever, so we keep some kind of “history”, and when we come across a term that we evaluated before, we fold the process tree and avoid evaluating same term again.</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell">(fib <span class="dv">1000</span>, fib <span class="dv">1000</span>)</code></pre>
<p>Unless we make sure to split it in a way that branches of the process tree are unaware of each other, we may end up eliminating common subexpressions. However, since there are lots of cases where we may want CSE, a splitter that always prevents it is not always desirable. We should instead allow CSE in a controlled way.</p>
<hr />
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Olaf Chitil, “Common Subexpression Elimination in a Lazy Functional Language”, section 3.5.<a href="#fnref1">↩</a></p></li>
</ol>
</div>]]></summary>
</entry>
<entry>
    <title>The issue of splitting without work duplication</title>
    <link href="http://osa1.net/posts/2015-08-13-the-issue-of-splitting-wo-duplication.html" />
    <id>http://osa1.net/posts/2015-08-13-the-issue-of-splitting-wo-duplication.html</id>
    <published>2015-08-13T00:00:00Z</published>
    <updated>2015-08-13T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>(I’m starting publishing my long list of unpublished blog posts with this post)</p>
<p>(Examples are from <a href="http://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-835.html">Bolingbroke’s PhD thesis</a>)</p>
<p><em>Example 1:</em></p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">let</span> a  <span class="fu">=</span> id y
    id <span class="fu">=</span> \x <span class="ot">-&gt;</span> x
 <span class="kw">in</span> <span class="dt">Just</span> a</code></pre>
<p><em>Problem:</em> The compiler should know about <code>id</code> while compiling <code>a</code>. This is easy to do, just tell the compiler about every binding when compiling RHSs. However, it causes some other problems:</p>
<p><em>Example 2:</em></p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">let</span> n <span class="fu">=</span> fib <span class="dv">100</span>
    b <span class="fu">=</span> n <span class="fu">+</span> <span class="dv">1</span>
    c <span class="fu">=</span> n <span class="fu">+</span> <span class="dv">2</span>
 <span class="kw">in</span> (b, c)</code></pre>
<p><em>Problem:</em> If we tell about <code>n</code> to the compiler when it’s compiling <code>b</code> and <code>c</code>, we’re taking the risk of work duplication. It may seem like <code>fib 100</code> will be evaluated in compile time and so duplication is not a huge deal, but this is not necessarily the case. First, we can’t know if it’s going to be evaluated to a value in compile time. Second, even if it’s a closed term and we somehow know it’s going to be terminated, termination checker of the evaluator may want to stop it before it’s evaluated to a value. Third, most of the time it’ll be an open term that’ll get stuck in the middle of supercompilation.</p>
<p>And when that happens we will generate a let-binding in residual code. In our case, we’ll be generating two let-bindings, one is for <code>b</code> and one is for <code>c</code>, and those let bindings will be doing same work.</p>
<hr />
<p><em>Question:</em> Can we rely on a post-processsing pass to eliminate common subexpressions? I.e. if we generate a code like this:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">let</span> b <span class="fu">=</span> <span class="kw">let</span> n_supercompiled <span class="fu">=</span> <span class="fu">&lt;</span>supercompiled fib <span class="dv">100</span><span class="fu">&gt;</span>
         <span class="kw">in</span> n_supercompiled <span class="fu">+</span> <span class="dv">1</span>
    c <span class="fu">=</span> <span class="kw">let</span> n_supercompiled <span class="fu">=</span> <span class="fu">&lt;</span>supercompiled fib <span class="dv">100</span><span class="fu">&gt;</span>
         <span class="kw">in</span> n_supercompiled <span class="fu">+</span> <span class="dv">2</span>
 <span class="kw">in</span> (b, c)</code></pre>
<p>It would transform it to obvious residual code that has single <code>n_supercompiled</code> which is in scope of <code>b</code> and <code>c</code>.</p>
<p>What are trade-offs?</p>
<hr />
<p>Finding a good heuristic is hard. Let’s say we try to estimate costs of expressions and decide whether to tell the compiler about them or not. If we decide that <code>ys</code> and <code>xs</code> are expensive in this case:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">let</span> map <span class="fu">=</span> <span class="fu">...</span>
    ys <span class="fu">=</span> map f zs
    xs <span class="fu">=</span> map g ys
 <span class="kw">in</span> <span class="dt">Just</span> xs</code></pre>
<p>We miss a deforestation opportunity, because the compiler won’t know about <code>ys</code> while compiling <code>xs</code>.</p>]]></summary>
</entry>
<entry>
    <title>On sufficiently smart compilers</title>
    <link href="http://osa1.net/posts/2015-08-09-sufficiently-smart-compiler.html" />
    <id>http://osa1.net/posts/2015-08-09-sufficiently-smart-compiler.html</id>
    <published>2015-08-09T00:00:00Z</published>
    <updated>2015-08-09T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>I’ve been thinking about optimizing functional programs recently, for a project that I’m hoping to make my research topic in the near future. You probably already know about <a href="http://c2.com/cgi/wiki?SufficientlySmartCompiler">The Myth of the Sufficiently Smart Compiler</a>, which basically says that the advanced compiler that optimizes your high-level, highly abstracted programs to efficient low-level code, is basically a myth.</p>
<p>This post is a brain dump on sufficiently smart compilation of functional programs and some compilation techniques. I’ll first make some seemingly unrelated points, and then hopefully use them to argue that the sufficiently smart compiler is not a myth, it just needs some hard-work to be realized.</p>
<h2 id="unreliable-optimizations-and-performance-critical-software">Unreliable optimizations and performance-critical software</h2>
<p>Every once in a while I see some blog posts about optimizing a JIT-compiled program by inspecting JIT trace dumps and generated code carefully, and I find this horrible, for the following reasons:</p>
<ul>
<li><p>It couples your program design with the JIT compiler’s internals. From a software engineering point of view, I think this is really one of the worst things that can happen to a software. You end up structuring your code with the compiler’s convenience in mind. But compilers can’t make sense of high-level, abstracted code(remember the myth?). So you end up code that’s low-level, hard to read, understand and maintain. And what happens when a new version of the compiler is released?</p></li>
<li><p>JIT compilers are highly complex, and as a result they’re very hard to reason about and this complex design makes them unpredictable. A seemingly unrelated change in your program can make the traces go significantly bad, and result in less optimized code, because maybe the change somehow made it to the trace and you now need to refactor your code.</p></li>
<li><p>If you need performance that bad, and you’re willing to read traces and generated assembly output for that, you could probably just write in a language that makes low-level optimizations easy/possible<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>. Or at least write performance-critical parts in a low-level language. Both of these cases eliminates the need for a JIT compiler.</p></li>
</ul>
<p>I think the last point is worth discussing further. Most JIT compilers we use nowadays are for compiling dynamic languages<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a>. By their nature dynamic languages are hard to optimize in compile-time, so they rely on runtime knowledge for optimizations. But does that make JIT compilers useless for statically-typed languages that are more amenable to compile-time optimizations? I don’t have a good answer to this, probably because I’m not a JIT expert. I think the fact that <a href="http://openjdk.java.net/groups/hotspot/">HotSpot</a> is doing good job is not an answer to this, because in JVM there’s bytecode interpretation going on, and this adds some room for runtime optimizations. Namely, you have one level of indirection that you can eliminate using JIT compilation.</p>
<p>In other statically-typed, compiled languages like C++, Haskell, OCaml etc. there’s less room for that kind of optimizations. I think applicability of JIT compilation techniques to these type of languages would make an interesting topic for a research project.</p>
<h2 id="compilers-that-can-learn-your-domain-and-manipulate-your-programs">Compilers that can learn your domain and manipulate your programs</h2>
<p>High-level languages and abstractions make efficient execution of programs harder, but there are a couple of things that they can do to help with the compilation. Namely, you can guide the compiler to optimize your domain-specific code.</p>
<p>One nice and simple example is <a href="https://downloads.haskell.org/~ghc/7.0.1/docs/html/users_guide/rewrite-rules.html">rewrite rules of GHC</a>. They’re used quite heavily in <a href="http://hackage.haskell.org/package/base">base</a>(GHC’s standard library) to eliminate intermediate lists. Other libraries use the same mechanism to tell the compiler how to optimize the code that uses their abstractions<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a>.</p>
<p>But for a compiler to support this kind of program transformations the language has to have some properties. In our case, we should be able to reason about the code in compile time, and locally, i.e. without thinking about runtime execution environment(heap, stack, variables in scope etc.) and the interaction of our code with the rest of the code. This is possible in purely functional languages because they make <a href="http://www.haskellforall.com/2013/12/equational-reasoning.html">equational reasoning</a> possible.</p>
<p>This is a very powerful property. This makes it possible to see programs as terms in an algebra, and we can freely manipulate these terms according to our rules. In the most basic sense, these rules can be the rules that define our language’s operational semantics, because by it’s very definition these rules are guaranteed to preserve semantics of programs<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a>. But we can go even further by adding rewrite rules to these rules. Rewrite rules are a way to say, “trust me, this transformation preserves semantics” and at that point a compiler is free to use the rule.</p>
<p>Furthermore, some properties of the language can give us <a href="http://ttic.uchicago.edu/~dreyer/course/papers/wadler.pdf">free theorems</a>, which in turn can help us with <a href="http://research.microsoft.com/en-us/um/people/simonpj/Papers/deforestation-short-cut.pdf">some optimizations</a><a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a>.</p>
<p>This type of “algebraic manipulation of programs” is a very powerful concept, and it can do great things. A very good example is this <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.48.7721">1997 paper about optimizing Haskell</a>. Most(maybe all?) of the transformations described in that paper are still in use.</p>
<h2 id="compilers-that-preserve-the-semantics">Compilers that preserve the semantics</h2>
<p>You probably wouldn’t want a compiler that compiles your programs to programs that do different things. We expect it to preserve the semantics. But that rule is sometimes too strict, and prevents some optimizations.</p>
<p>For example, if floating points and operations on floating points in your language are defined as they’re defined in IEEE-754, then the compiler can’t assume associativity of floating point operations and you lose some optimization opportunities. GCC’s <code>-ffast-math</code> is for relaxing this restriction by letting the compiler assume this associativity.</p>
<p>Another example is termination properties of programs. For example, would you be OK with this transformation in a purely functional language:</p>
<pre><code>(λx . 1) loop ~&gt; 1</code></pre>
<p>In a call-by-name(or call-by-need, which is an efficient implementation of call-by-name) language, this is a valid transformation. But in call-by-value language this would change the semantics. Previously this program were looping, but now it returns 1.</p>
<p>This example is actually a good demonstration of a problem that we have even in purely functional languages. Namely, there are some programs that don’t map to any values in the domain you use to model your language<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a>. The way these programs are modeled are generally by defining a special value, ⊥(read “bottom”). Non-terminating and exception/error throwing programs are said to be “bottom” and denoted with this value. Bottom values are said to be “less defined” than non-bottom values.</p>
<p>Using this definition, we can say that the transformation shown above transforms a program to a more defined one. You might want this restriction of preserving definedness of programs for different reasons, and here’s an example reason: Without this restriction, your program may terminate or loop depending on how the compiler performed. A seemingly-unrelated change in your program may cause a different termination behavior.</p>
<p>Now this is a hard problem. There are papers about transforming call-by-value functional languages while preserving termination properties(see <a href="https://www.sics.se/~pj/papers/scp/popl09-scp.pdf">this</a> as an example). In general, we can’t decide if a program is bottom or not. First of all, that would be solving the <a href="https://en.wikipedia.org/wiki/Halting_problem">halting problem</a>. But more specifically, we can’t do this transformation if <code>y</code> depends on a dynamic input here:</p>
<pre><code>(λx . 1) (1 / y) ~&gt; 1</code></pre>
<p>In most cases though, the compiler is simply not able to propagate enough information to this stage to see if <code>y</code> can be <code>0</code> or not<a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a>, even if all the necessary information is available in compile time.</p>
<h2 id="making-the-most-out-of-available-input">Making the most out of available input</h2>
<p>There’s an old yet IMHO under-appreciated technique for taking statically-known inputs into account while compiling programs. It’s called “partial evaluation” and described in details in this awesome book <a href="http://www.itu.dk/~sestoft/pebook/jonesgomardsestoft-a4.pdf">“Partial Evaluation and Automatic Program Generation”</a> by Neil D. Jones, Carsten K. Gomard and Peter Sestoft. One very interesting but somewhat esoteric application of this idea is <a href="https://cs.au.dk/~hosc/local/HOSC-12-4-pp381-391.pdf">Futamura projections</a><a href="#fn8" class="footnoteRef" id="fnref8"><sup>8</sup></a>, but to give a easier to understand example, a C partial evaluator could read your Vim config in compile time and compile Vim to an executable that doesn’t read any Vim files on startup because it’s already specialized to the Vim config it read in compile time<a href="#fn9" class="footnoteRef" id="fnref9"><sup>9</sup></a>. General tools may depend of lots of dynamic input, but in your special case you may fix some of these variables and this is where a partial evaluation comes into the play. See <a href="http://blog.regehr.org/archives/1197">this blog post</a> for another example.</p>
<p>How much further could it propagate this statically known input and specialize rest the code using it? That’s completely different story and comes with some very hard to solve problems. I’ll again come to this later.</p>
<p>The whole point is to generate specialized code for known input. We can shift the stage a little bit and apply this idea in runtime, and that gives us <a href="http://www.cs.rice.edu/~taha/MSP/">multi-stage programming</a>.</p>
<p>MSP allows us to generate code in runtime, link it to the program in a way that the generated code runs in the current execution environment(i.e. the generated code can refer to names in enclosing scope, pretty much like how closures would do).</p>
<p>Traditionally, MSP doesn’t allow code generation in compile-time, and the techniques used for code generation are completely different<a href="#fn10" class="footnoteRef" id="fnref10"><sup>10</sup></a>. But we can generate code specialized to input that is only available in runtime. For example, you can write a game that runs code specialized to the player’s options. Or run a web server that does some optimizations on request dispatch code depending on some analysis on recent requests.</p>
<p>This is again a very powerful concept, and only recently I started to appreciate it’s potential<a href="#fn11" class="footnoteRef" id="fnref11"><sup>11</sup></a>. IMO, MSP is missing a “killer language”<a href="#fn12" class="footnoteRef" id="fnref12"><sup>12</sup></a>(and also a “killer application” but I think that follows the language) and I’m hoping to make some progress on this front in the future.</p>
<h2 id="finally-a-sufficiently-smart-compiler">Finally, a sufficiently smart compiler</h2>
<p>This post may seem to be going nowhere, so let’s back up a bit and come to the point.</p>
<p>I define a sufficiently smart compiler not as a completely automated program, but as a toolchain. This toolchain has a completely automated compiler, but it also gives programmers tools for runtime code generation, and for teaching the compiler domain-specific optimizations. The compiler knows about language’s semantics, and when possible it does reductions in compile time to remove abstractions and also to leave less work to runtime.</p>
<p>While doing reductions in compile time, it takes programmers’ rules into account, and optimizes abstractions accordingly. This allows it to optimize domain-specific abstractions that normally a compiler have no way to know about.</p>
<p>By now it should be clear that such a compiler is only possible with a language that allows these optimizations. For example, without a purely functional language, rewrite rules are not easy, if not impossible.</p>
<p>The compiler gradually compiles the language into languages that are more and more close to the machine language that it has to generate in the end. Reductions and user rules are applied in a level where programs are still expressed in a purely functional language. This language should be sufficient for most optimizations that eliminate programmers’ abstractions in compile time.</p>
<p>This way, programmers don’t need to look at ridiculous bytecode traces or instructions written in a highly-complex assembly language to figure out how things are optimized, and rather they stay in the same level of abstraction that their programs are written in. When they want to know about memory allocations, for example, they should be able to look at the next level in the compilation, which should have explicit memory allocation operations and pointers etc. The main point is that they stay in a level where they can observe some particular behavior(e.g. memory allocation) of a program and they don’t have to read assembly, for example, to see if their higher-order <code>map</code> application that uses an increment function to increment integers in a list is compiled to a loop without any function calls.</p>
<p>In this compiler there’s no room for abstraction-breaking, unreliable optimizations or optimizations that cause coupling with the compiler’s internals, like in the case of JIT compilers.</p>
<p>In the beginning I said that I don’t see a this as a myth. So how I think this is possible to implement? This is already a long-enough post, and I’ll stop for now. Let me just say that almost all of these things are implemented in different projects:</p>
<ul>
<li><p>MSP does runtime code generation and <a href="http://okmij.org/ftp/ML/MetaOCaml.html">MetaOCaml</a> gives us a nice way to do that in a safe way. Another alternative is <a href="http://terralang.org/">Terra</a>, but in Terra generated code is in a different language, so that’s quite different(also, it’s a dynamically typed language that gives no guarantees about generated code).</p></li>
<li><p>Domain-specific optimizations are possible in Haskell thanks to GHC’s rewrite rules, as mentioned in the related section above.</p></li>
<li><p>GHC’s internal languages Core, STG and Cmm allow programmers to gradually go low level and see the details they’re looking for. Most of the time Core is enough to see if your abstractions are eliminated in compile time and if your rules worked as expected.</p></li>
<li><p>Compile time reduction of programs are done by supercompilers. It was a lesser known technique until recently a couple of papers(<a href="http://dl.acm.org/citation.cfm?id=1863588">1</a>, <a href="http://research.microsoft.com/en-us/um/people/simonpj/papers/supercompilation/supercomp-by-eval.pdf">2</a>) and a <a href="http://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-835.html">PhD thesis</a> explored it in the context of Haskell.</p></li>
</ul>
<p>Some of these features are orthogonal to each other, like MSP and compile-time reduction. But some others are not, for example, we expect the supercompiler to take rewrite rules into account, otherwise it may be impossible to do some optimizations.</p>
<p>The hardest part seems to be compile-time reductions of programs according to operational semantics of the language, which involves some very hard problems, and one of the reasons has to do with preserving semantics. In the next couple of posts I’m hoping to talk about that, and in the meantime you can refer to chapter 9 of the PhD thesis I linked above.</p>
<hr />
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p><a href="https://github.com/SnabbCo/snabbswitch">Snabb Switch</a> project comes to mind here. It’s a Lua project and they rely on LuaJIT to optimize their code. See this series of blog posts: <a href="https://github.com/lukego/blog/issues/5">1</a>, <a href="https://github.com/lukego/blog/issues/6">2</a>, <a href="https://github.com/lukego/blog/issues/8">3</a>.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p><a href="https://developers.google.com/v8/">V8</a> and <a href="https://wiki.mozilla.org/JavaScript:TraceMonkey">TraceMonkey</a> for JavaScript, <a href="http://luajit.org/">LuaJIT</a> for Lua, <a href="http://pypy.org/">PyPy</a> for Python. There are also research-level JIT compilers, like <a href="https://github.com/higgsjs/Higgs">Higgs</a> for JavaScript and <a href="https://github.com/samth/pycket">Pycket</a>(<a href="https://rpython.readthedocs.org/en/latest/">RPython</a> based, created by colleagues from IU) for Racket.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>One example that I like very much is the <a href="http://hackage.haskell.org/package/pipes">pipes library</a>. You can see some of it’s rewrite rules <a href="https://github.com/Gabriel439/Haskell-Pipes-Library/blob/d7b1430b1b35abfde98b32cbc4aae02a4e027dd0/src/Pipes/Core.hs#L869">here</a>.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>Because operational semantics is what defines semantics of programs.<a href="#fnref4">↩</a></p></li>
<li id="fn5"><p>One very good question to ask here is, what exactly gives us free theorems? I don’t have an answer to that question yet.<a href="#fnref5">↩</a></p></li>
<li id="fn6"><p>This type of giving semantics to languages is called “denotational semantics”. I don’t have very good reading material about this but you may want to have a look at <a href="https://en.wikibooks.org/wiki/Haskell/Denotational_semantics">this</a>.<a href="#fnref6">↩</a></p></li>
<li id="fn7"><p>We’re assuming that it somehow knows that divide-by-zero leads to bottom.<a href="#fnref7">↩</a></p></li>
<li id="fn8"><p>I wrote about this <a href="http://osa1.net/posts/2015-01-11-understanding-futamura-projections.html">previously</a> and I also have <a href="http://osa1.net/posts/2015-05-13-comp-through-interp.html">this related project</a>.<a href="#fnref8">↩</a></p></li>
<li id="fn9"><p>In practice this is probably hard to achieve, and it certainly needs some refactoring in current Vim codebase.<a href="#fnref9">↩</a></p></li>
<li id="fn10"><p>See <a href="http://osa1.net/posts/2015-05-13-comp-through-interp.html">my blog post</a> for a comparison.<a href="#fnref10">↩</a></p></li>
<li id="fn11"><p>Even though I’ve been working on MSP languages for a while know. See my previous work on this: <a href="/posts/2013-04-15-internship.report.html">1</a>, <a href="/posts/2014-03-06-proving-simply-typed-multi-staged-lc.html">2</a>, <a href="/posts/2015-05-13-comp-through-interp.html">3</a>, and here’s a <a href="/posts/2015-05-17-staging-is-not-just-codegen.html">ranty post</a>.<a href="#fnref11">↩</a></p></li>
<li id="fn12"><p><a href="http://terralang.org/">Terra</a> comes quite close, but I have some confusions about it and I’m hoping to write about those in the future.<a href="#fnref12">↩</a></p></li>
</ol>
</div>]]></summary>
</entry>
<entry>
    <title>Staging is not just code generation</title>
    <link href="http://osa1.net/posts/2015-05-17-staging-is-not-just-codegen.html" />
    <id>http://osa1.net/posts/2015-05-17-staging-is-not-just-codegen.html</id>
    <published>2015-05-17T00:00:00Z</published>
    <updated>2015-05-17T00:00:00Z</updated>
    <summary type="html"><![CDATA[<h1 id="section"></h1>
<p>It feels weird to see that even <a href="http://okmij.org/ftp/">Oleg</a> seems to <a href="http://lambda-the-ultimate.org/node/5146#comment-85570">think about it that way</a>.</p>
<p>Sure, we don’t have a definition of the term that’s supposed to be accepted by everyone, and one can use it in different meanings. My minimal definition for the term is “a technique for runtime code generation and linking”. If it’s missing “linking” part, then to me it’s just another AST definition + printer library(sometimes it’s embedded into the language to add some convenience syntactic sugar and/or quasiquotation to the language).</p>
<p>To me the whole point is “runtime specialization”. For that you should be able to use the runtime data that’s available when you’re generating the code in the generated code<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>. This is called “cross-stage persistence”. In a simple multi-stage language, this may be supported simply by serializing data as code, but this is not as flexible as one might need for runtime optimized code generation. For example, you can’t serialize a socket or file handle this way, but it’s safe and possible to use a socket or file handle that’s available while generating the code in the generated code. You can’t easily do that if the staging library/language doesn’t provide this as a feature.</p>
<p>In the case of <a href="http://okmij.org/ftp/ML/MetaOCaml.html">BER-MetaOCaml</a>, I think this is one of the major limitations without any workarounds: It only supports OCaml bytecode. Printing the code and compiling manually is not a solution for the reason I just described.</p>
<p>One more thing about printing the code: In my opinion, a multi-stage language should provide a way to print generated code <em>only for debugging purposes</em>. (e.g. To see if I’m really generating the code I want)<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a></p>
<p>I should make this clear that it’s completely fine to use it for code generation, what I’m trying to say is that if all it can do is code generation then it’s missing the point.</p>
<p>As an example, I used staging for code generation <a href="http://osa1.net/posts/2015-05-13-comp-through-interp.html">in my last project</a>, and it seems like <a href="http://scala-lms.github.io/">Scala LMS</a> people do this a lot too<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a>.</p>
<hr />
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>In a sense this is like a closure, generated code should be able to refer to names in enclosing environment of code generator.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>I’m wondering if <a href="http://terralang.org/">Terra</a> has a way to print generated code. Any ideas?<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>I didn’t read the paper very carefully, but I think one example is <a href="http://dl.acm.org/citation.cfm?id=2429128">Optimizing Data Structures in High-Level Programs</a> paper which is published in POPL ’13.<a href="#fnref3">↩</a></p></li>
</ol>
</div>]]></summary>
</entry>
<entry>
    <title>Simplest pathological program for supercompilers</title>
    <link href="http://osa1.net/posts/2015-05-16-simplest-pathological-pgm-for-supercompilers.html" />
    <id>http://osa1.net/posts/2015-05-16-simplest-pathological-pgm-for-supercompilers.html</id>
    <published>2015-05-16T00:00:00Z</published>
    <updated>2015-05-16T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>While working on one of my program transformation ideas, I’ve found a very simple program that is apparently pathological for most supercompilers:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell">loop x <span class="fu">=</span> loop (x <span class="fu">+</span> <span class="dv">1</span>)</code></pre>
<p>I tried this program on several supercompilers:</p>
<ul>
<li><p><a href="https://github.com/ilya-klyuchnikov/sc-mini">sc-mini</a><a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>: It unfolds arbitrarily and generates this program:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell">f1(v1) <span class="fu">=</span> f1(<span class="dt">S</span>(<span class="dt">S</span>(<span class="dt">S</span>(<span class="dt">S</span>(<span class="dt">S</span>(<span class="dt">S</span>(<span class="dt">S</span>(<span class="dt">S</span>(<span class="dt">S</span>(<span class="dt">S</span>(<span class="dt">S</span>(<span class="dt">S</span>(<span class="dt">S</span>(<span class="dt">S</span>(<span class="dt">S</span>(<span class="dt">S</span>(<span class="dt">S</span>(
              <span class="dt">S</span>(<span class="dt">S</span>(<span class="dt">S</span>(<span class="dt">S</span>(<span class="dt">S</span>(<span class="dt">S</span>(<span class="dt">S</span>(<span class="dt">S</span>(<span class="dt">S</span>(<span class="dt">S</span>(<span class="dt">S</span>(<span class="dt">S</span>(<span class="dt">S</span>(<span class="dt">S</span>(<span class="dt">S</span>(<span class="dt">S</span>(<span class="dt">S</span>(
                <span class="dt">S</span>(<span class="dt">S</span>(<span class="dt">S</span>(<span class="dt">S</span>(<span class="dt">S</span>(v1))))))))))))))))))))))))))))))))))))))));</code></pre></li>
<li><p><a href="https://github.com/ndmitchell/supero">supero</a>: It just loops:</p>
<pre><code>➜  supero4 git:(master) ✗ ../.cabal-sandbox/bin/supero --compile ../example/Example1.hs
files: [&quot;../example/Example1.hs&quot;]
Converting ../example/Example1.hs
{-# LANGUAGE UnboxedTuples, NoMonomorphismRestriction #-}
module ...example.Example1_gen(test) where
define: _1 = (:) (loop 1) []
peel: (:) (loop 1) []
define: _2 = loop 1
^C
➜  supero4 git:(master) ✗</code></pre></li>
<li><p><a href="https://github.com/batterseapower/supercompilation-by-evaluation">supercompilation-by-evaluation</a>: Similar to sc-mini, it unfolds for a while and generates this:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">let</span>
  h0 <span class="fu">=</span> <span class="kw">let</span>
         root_u15 <span class="fu">=</span> loop_u17 a_u51
         loop_u17 <span class="fu">=</span> h1
         a_u51 <span class="fu">=</span> h5
       <span class="kw">in</span> root_u15
  h1 <span class="fu">=</span> \i_u55 <span class="ot">-&gt;</span> h2 i_u55
  h2 <span class="fu">=</span> \i_u55 <span class="ot">-&gt;</span> <span class="kw">let</span>
                   loop_u17 <span class="fu">=</span> h1
                   a_u60 <span class="fu">=</span> h3 i_u55
                 <span class="kw">in</span> loop_u17 a_u60
  h3 <span class="fu">=</span> \i_u55 <span class="ot">-&gt;</span> <span class="kw">let</span> a_u57 <span class="fu">=</span> (<span class="fu">+</span>) i_u55 h4
                 <span class="kw">in</span> (<span class="fu">+</span>) a_u57 h4
  h4 <span class="fu">=</span> <span class="dv">1</span><span class="ot"> ::</span> <span class="dt">Int</span>
  h5 <span class="fu">=</span> <span class="dv">4</span><span class="ot"> ::</span> <span class="dt">Int</span>
<span class="kw">in</span> h0</code></pre></li>
</ul>
<p>In the ideal case a supercompiler would just generate same program, without making any changes.</p>
<p>There’s one supercompiler that I couldn’t try: <a href="https://github.com/batterseapower/chsc">chsc</a>(The Cambridge Haskell Supercompiler). I wasted a lot of time trying to make it working, but I failed. If you’re able to run it, please post the results in comments section below.</p>
<p>If you know any other supercompilers that I can test, please tell me about those too.</p>
<hr />
<p>[May 17, 2015] UPDATE: I thought about why this is a bad case for supercompilers and found some explanations.</p>
<p>First of all, this was not a fair comparison. In the case of “supercompilation-by-evaluation” and “supero” I didn’t use Peano definitions, instead used integers and primitive operation <code>(+)</code>. I fixed this and “supercompilation-by-evaluation” produced this program:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">let</span>
  h0 <span class="fu">=</span> \i_u18 <span class="ot">-&gt;</span> h1 i_u18
  h1 <span class="fu">=</span> \i_u18 <span class="ot">-&gt;</span> <span class="kw">let</span>
                   loop_u15 <span class="fu">=</span> h2
                   a_u48 <span class="fu">=</span> h3 i_u18
                 <span class="kw">in</span> loop_u15 a_u48
  h2 <span class="fu">=</span> \i_u52 <span class="ot">-&gt;</span> h1 i_u52
  h3 <span class="fu">=</span> \i_u18 <span class="ot">-&gt;</span> <span class="kw">let</span> a_u35 <span class="fu">=</span> h4 i_u18
                 <span class="kw">in</span> <span class="dt">S</span> a_u35
  h4 <span class="fu">=</span> \i_u18 <span class="ot">-&gt;</span> <span class="kw">let</span> a_u20 <span class="fu">=</span> h5 i_u18
                 <span class="kw">in</span> <span class="dt">S</span> a_u20
  h5 <span class="fu">=</span> \i_u18 <span class="ot">-&gt;</span> <span class="dt">S</span> i_u18
<span class="kw">in</span> h0</code></pre>
<p>When simplified, it turns into:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell">loop <span class="fu">=</span> loop <span class="fu">.</span> <span class="dt">S</span> <span class="fu">.</span> <span class="dt">S</span> <span class="fu">.</span> <span class="dt">S</span></code></pre>
<p>So it still has this problem of unfolding it for a while.</p>
<p>I didn’t try this on “supero”, but in any case a it shouldn’t loop forever. It’s probably a bug in the implementation. Both supercompilers use same termination criteria so I’d expect “supero” to do the same.</p>
<p>“sc-mini” is deliberately kept simple. It checks size of the term, and blows the whistle<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> if it’s larger than some fixed amount(it’s set as 40 in the source code). Indeed, look at the term produced by sc-mini, it contains 39 <code>S</code> applications and a variable. In the paper <a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a>, the author mentions “homeomorphic embedding” and refers the user to some papers that describe it.</p>
<p>I think a supercompiler that uses homeomorphic embedding would stop earlier than “supercompilation-by-evaluation”. I’d imagine something like this:</p>
<ul>
<li>Supercompilation starts with the term <code>loop Z</code>.</li>
<li>It takes a step to <code>loop (S Z)</code>. Now our process tree has an edge like this: <code>loop Z -&gt; loop (S Z)</code>.</li>
<li>Our homeomorphic embedding relation holds between <code>loop (S Z)</code> and <code>loop Z</code>, so our whistle blows and evaluation stops.</li>
<li>Now the supercompiler introduces a definition <code>fresh_v1 x = loop x</code> and replaces <code>loop (S Z)</code> with <code>fresh_v1 (S Z)</code>.</li>
<li>Optional: A simplification step would replace <code>fresh_v1</code> with <code>loop</code>.</li>
</ul>
<p>This would compile our program to <code>loop (S Z)</code>, which is not ideal maybe(still took a redundant step) but better than what’s produced by others.</p>
<p>Quoted from the paper <a href="http://research.microsoft.com/en-us/um/people/simonpj/papers/supercompilation/supercomp-by-eval.pdf">“Supercompilation by Evalution”</a>:</p>
<blockquote>
<p>Much of the supercompilation literature makes use of the homeomorphic embedding test for ensuring termination. Users of this test uniformly report that testing the termination condition makes up the majority of their supercompilers runtime<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a>. The tag-bag criteria appears to be much more efficient in practice, as our supercompiler spends only 6% of its runtime testing the criteria.</p>
</blockquote>
<p>Quoted from the paper <a href="http://community.haskell.org/~ndm/downloads/paper-rethinking_supercompilation-29_sep_2010.pdf">“Rethinking Supercompilation”</a>:</p>
<blockquote>
<p>In some cases, our rule is certainly less restrictive than the homeomorphic embedding. The example in §2.6.4 would have stopped one step earlier with a homeomorphic embedding. Under a fairly standard interpretation of variable names and let expressions, we can show that our rule is always less restrictive than the homeomorphic embedding – although other differences in our treatment of expressions mean such a comparison is not necessarily meaningful. However, we did not choose our termination criteria to permit more expressions – it was chosen for both simplicity and compilation speed.</p>
</blockquote>
<p>So it seems like tag-bag approach, when compared to homeomorphic embedding, 1) faster 2) less restrictive(meaning sometimes it allows more steps to be taken before stopping evaluation). This is probably why it evaluates the loop a couple of times where homeomorphic embedding would stop after just one evaluation.</p>
<hr />
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>This supercompiler comes with <a href="https://themonadreader.files.wordpress.com/2014/04/super-final.pdf">this paper</a>. I highly recommend the paper if you’re interested in supercompilation.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>I don’t know the original source, but apparently “whistle” is the traditional term for the heuristic that tells a supercompiler to stop.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>This supercompiler comes with <a href="https://themonadreader.files.wordpress.com/2014/04/super-final.pdf">this paper</a>. I highly recommend the paper if you’re interested in supercompilation.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>References two papers: <a href="https://www.sics.se/~pj/papers/scp/popl09-scp.pdf">“Positive supercompilation for higher order call-by-value language”</a>(apparently the author later wrote his PhD thesis on this topic) and <a href="http://community.haskell.org/~ndm/downloads/paper-a_supercompiler_for_core_haskell-01_may_2008.pdf">“A supercompiler for core Haskell”</a>.<a href="#fnref4">↩</a></p></li>
</ol>
</div>]]></summary>
</entry>
<entry>
    <title>Compilation through interpretation, a small experiment</title>
    <link href="http://osa1.net/posts/2015-05-13-comp-through-interp.html" />
    <id>http://osa1.net/posts/2015-05-13-comp-through-interp.html</id>
    <published>2015-05-13T00:00:00Z</published>
    <updated>2015-05-13T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>I’ve been studying different program transformation techniques recently, and to me <a href="http://osa1.net/posts/2015-01-11-understanding-futamura-projections.html">Futamura projections</a> are one of the most interesting applications of program transformations. Couple of days ago I finished a small project in which I implemented first Futamura projection(aka. interpreter specialization) using <a href="http://www.madore.org/~david/programs/unlambda/">Unlambda</a> as object language. You can see the project <a href="https://github.com/osa1/int-proj">here</a>. I tried to write some comments to the source when I get stuck because of a problem or realized something interesting, so I suggest reading the source if you’re interested.</p>
<p>I did two implementations and used a different meta language for each one. There are multiple ways to achieve first Futamura projections: We can use a partial evaluator, a supercompiler(which may actually subsume partial evaluation, depending on how sophisticated it is), or just a “sufficiently smart” compiler. The problem though, we don’t have a lot of(read: any) usable implementations of partial evaluators or supercompilers, so I had to use the only language with a partial evaluator that I could find: <a href="https://github.com/idris-lang/Idris-dev">Idris</a><a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>.</p>
<p>There’s one another technique that we can use. The techniques I listed above are all completely automated. If things don’t go as expected we’re on our own to figure out why is that and hack around to make the tool transform the program the way we want. Indeed this is happened even in this project, which is deliberately kept simple and small<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a>.</p>
<p>At the other end of the spectrum is multi-stage programming. In multi-stage programming the programmer specifies, using some annotations<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a>, what code to generate and how to generate it. It clearly separates code that runs in code generation time and generated code. When generated code is printed to be compiled later, multi-stage programming feels like a compiler or a partial evaluator that the programmer can guide to generate the code he/she wants.</p>
<p>My second meta language is <a href="http://okmij.org/ftp/ML/MetaOCaml.html">MetaOCaml</a>, which is basically OCaml with multi-stage programming constructs. Using these two languages as representatives of two different program generation techniques, I implemented first Futamura projections for Unlambda.</p>
<p>There’s a report file in the repository, and I refer interesting readers to that document. README file contains compilation directives and some interesting executions. One interesting thing is that I later added a simple partial evaluator to MetaOCaml implementation, and in the <code>programs/</code> directory there’s an Unlambda interpreter, written in Unlambda. Using these two programs, you can do things like partially applying(specializing) Unlambda interpreter to other programs or even itself. Before every experiment, I suggest thinking about what is the generated code you’re expecting(what does it do). What would a “sufficiently smart” partial evaluator generate? What would a simple partial evaluator generate? Similarly, try these while generating first projections.</p>
<p>Finally, if you’re interested in program transformations, stay tuned for more blog posts.</p>
<p><a href="https://github.com/osa1/int-proj">Link to the project.</a></p>
<hr />
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>There is actually an <a href="https://github.com/annenkov/unmix">implementation of well-known partial evaluator unmix</a>. I knew about the project, but didn’t remember by the time I started this project. Still, I think I’d choose Idris even if I remembered.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>Although some of those problems were implementation related, e.g. Idris was buggy. See the source code for comments and Github issues.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>In MetaOCaml case those annotations are term-level, but there are other cases where annotations happen in type level only. See <a href="http://scala-lms.github.io/">LMS</a> as an example. (I think it’s the only example for now)<a href="#fnref3">↩</a></p></li>
</ol>
</div>]]></summary>
</entry>
<entry>
    <title>Some benchmarks for meta-tracing BF JIT and traditional BF implementations</title>
    <link href="http://osa1.net/posts/2015-04-11-some-bf-benchmarks.html" />
    <id>http://osa1.net/posts/2015-04-11-some-bf-benchmarks.html</id>
    <published>2015-01-29T00:00:00Z</published>
    <updated>2015-01-29T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>I found RPython very interesting for several reasons which I may be talking about later, and I need to use it for a project, so I started running some tutorials. However, I had some concerns about the idea(I still have, and I’ll defer the discussion to some other post for now), and I wanted to experiment with different implementations of same interpreter and compare results.</p>
<p>What I wanted to see is, given a very good and mature JIT compiler(LuaJIT in this case), how hard would it be to have similar optimizations without annotating code manually to give hints to the JIT compiler.</p>
<p>So I implemented a simple BF interpreter in Lua, and started experimenting with different optimizations. As for benchmarking, I used <code>bench</code> program from this <a href="https://bitbucket.org/brownan/pypy-tutorial/">RPython tutorial repository</a>.</p>
<p>Before diving into Lua implementation, here results of running them with RPython compiled interpreter, Python and PyPy:</p>
<pre><code>./example5-rpython bench.b    0.94s user 0.00s system 99% cpu 0.947 total
pypy example5.py   bench.b   15.57s user 0.01s system 99% cpu 15.597 total
python example5.py bench.b  597.34s user 0.04s system 99% cpu 9:57.87 total</code></pre>
<p>The Lua implementation started with this:</p>
<pre class="sourceCode lua"><code class="sourceCode lua"><span class="kw">function</span> mainloop<span class="ot">(</span>program<span class="ot">,</span> bracket_map<span class="ot">,</span> dispatch_fn<span class="ot">)</span>
    <span class="kw">local</span> pc <span class="ot">=</span> <span class="dv">1</span>
    <span class="kw">local</span> tape <span class="ot">=</span> <span class="ot">{</span><span class="dv">0</span><span class="ot">}</span>
    <span class="kw">local</span> tape_pos <span class="ot">=</span> <span class="dv">1</span>

    <span class="kw">local</span> code
    <span class="kw">while</span> pc <span class="ot">&lt;=</span> <span class="ot">#</span>program <span class="kw">do</span>
        code <span class="ot">=</span> program<span class="ot">[</span>pc<span class="ot">]</span>
        <span class="kw">if</span> code <span class="ot">==</span> <span class="st">&quot;&gt;&quot;</span> <span class="kw">then</span>
            tape_pos <span class="ot">=</span> tape_pos <span class="ot">+</span> <span class="dv">1</span>
            <span class="kw">if</span> <span class="ot">#</span>tape <span class="ot">&lt;</span> tape_pos <span class="kw">then</span>
                <span class="fu">table.insert</span><span class="ot">(</span>tape<span class="ot">,</span> <span class="dv">0</span><span class="ot">)</span>
            <span class="kw">end</span>
        <span class="kw">elseif</span> code <span class="ot">==</span> <span class="st">&quot;&lt;&quot;</span> <span class="kw">then</span>
            tape_pos <span class="ot">=</span> tape_pos <span class="ot">-</span> <span class="dv">1</span>
        <span class="kw">elseif</span> code <span class="ot">==</span> <span class="st">&quot;+&quot;</span> <span class="kw">then</span>
            tape<span class="ot">[</span>tape_pos<span class="ot">]</span> <span class="ot">=</span> tape<span class="ot">[</span>tape_pos<span class="ot">]</span> <span class="ot">+</span> <span class="dv">1</span>
        <span class="kw">elseif</span> code <span class="ot">==</span> <span class="st">&quot;-&quot;</span> <span class="kw">then</span>
            tape<span class="ot">[</span>tape_pos<span class="ot">]</span> <span class="ot">=</span> tape<span class="ot">[</span>tape_pos<span class="ot">]</span> <span class="ot">-</span> <span class="dv">1</span>
        <span class="kw">elseif</span> code <span class="ot">==</span> <span class="st">&quot;.&quot;</span> <span class="kw">then</span>
            <span class="fu">io.write</span><span class="ot">(</span><span class="fu">string.char</span><span class="ot">(</span>tape<span class="ot">[</span>tape_pos<span class="ot">]))</span>
        <span class="kw">elseif</span> code <span class="ot">==</span> <span class="st">&quot;[&quot;</span> <span class="kw">and</span> tape<span class="ot">[</span>tape_pos<span class="ot">]</span> <span class="ot">==</span> <span class="dv">0</span> <span class="kw">then</span>
            pc <span class="ot">=</span> bracket_map<span class="ot">[</span>pc<span class="ot">]</span>
        <span class="kw">elseif</span> code <span class="ot">==</span> <span class="st">&quot;]&quot;</span> <span class="kw">and</span> tape<span class="ot">[</span>tape_pos<span class="ot">]</span> <span class="ot">~=</span> <span class="dv">0</span> <span class="kw">then</span>
            pc <span class="ot">=</span> bracket_map<span class="ot">[</span>pc<span class="ot">]</span>
        <span class="kw">end</span>
        pc <span class="ot">=</span> pc <span class="ot">+</span> <span class="dv">1</span>
    <span class="kw">end</span>
<span class="kw">end</span></code></pre>
<p>I’m not sure how many reasonable different implementations one can come up with, given that the language is this small. Still, there are some optimizations that we can do and I’ve tried some of them. Here are some things I tried:</p>
<ul>
<li>I tried replacing one character strings with ASCII code equivalents. Since Lua doesn’t have character constants, I thought this may give us a few instructions per branch. But results were just the same.</li>
<li>I tried replacing table getters and setters with <code>rawget</code> and <code>rawset</code>s. Nothing changed. Apparently it’s not worth the effort unless you have a metatable for your table.</li>
<li><p>I tried generating a huge “if-then-else” statement for <code>bracket_map</code>, and used it as a jump table kind of thing. Here’s the code:</p>
<pre class="sourceCode lua"><code class="sourceCode lua"><span class="kw">function</span> gen_dispatch_fn<span class="ot">(</span>bracket_map<span class="ot">,</span> fun_name<span class="ot">)</span>
    <span class="kw">local</span> first <span class="ot">=</span> <span class="kw">true</span>
    <span class="kw">local</span> acc <span class="ot">=</span> <span class="ot">{}</span>
    <span class="fu">table.insert</span><span class="ot">(</span>acc<span class="ot">,</span> <span class="st">&quot;function &quot;</span> <span class="ot">..</span> fun_name <span class="ot">..</span> <span class="st">&quot;(arg)</span><span class="ot">\n</span><span class="st">&quot;</span><span class="ot">)</span>
    <span class="kw">for</span> k<span class="ot">,</span>v <span class="kw">in</span> <span class="fu">pairs</span><span class="ot">(</span>bracket_map<span class="ot">)</span> <span class="kw">do</span>
        <span class="kw">if</span> first <span class="kw">then</span>
            <span class="fu">table.insert</span><span class="ot">(</span>acc<span class="ot">,</span> <span class="st">&quot;    if arg == &quot;</span> <span class="ot">..</span> k <span class="ot">..</span> <span class="st">&quot; then</span><span class="ot">\n</span><span class="st">&quot;</span><span class="ot">)</span>
            first <span class="ot">=</span> <span class="kw">false</span>
        <span class="kw">else</span>
            <span class="fu">table.insert</span><span class="ot">(</span>acc<span class="ot">,</span> <span class="st">&quot;    elseif arg == &quot;</span> <span class="ot">..</span> k <span class="ot">..</span> <span class="st">&quot; then</span><span class="ot">\n</span><span class="st">&quot;</span><span class="ot">)</span>
        <span class="kw">end</span>
        <span class="fu">table.insert</span><span class="ot">(</span>acc<span class="ot">,</span> <span class="st">&quot;        return &quot;</span> <span class="ot">..</span> v <span class="ot">..</span> <span class="st">&quot;</span><span class="ot">\n</span><span class="st">&quot;</span><span class="ot">)</span>
    <span class="kw">end</span>
    <span class="fu">table.insert</span><span class="ot">(</span>acc<span class="ot">,</span> <span class="st">&quot;    else</span><span class="ot">\n</span><span class="st">&quot;</span><span class="ot">)</span>
    <span class="fu">table.insert</span><span class="ot">(</span>acc<span class="ot">,</span> <span class="st">&quot;        error(</span><span class="ot">\&quot;</span><span class="st">invalid arg: </span><span class="ot">\&quot;</span><span class="st"> .. arg)</span><span class="ot">\n</span><span class="st">&quot;</span><span class="ot">)</span>
    <span class="fu">table.insert</span><span class="ot">(</span>acc<span class="ot">,</span> <span class="st">&quot;    end</span><span class="ot">\n</span><span class="st">&quot;</span><span class="ot">)</span>
    <span class="fu">table.insert</span><span class="ot">(</span>acc<span class="ot">,</span> <span class="st">&quot;end</span><span class="ot">\n</span><span class="st">&quot;</span><span class="ot">)</span>
    <span class="kw">return</span> <span class="fu">table.concat</span><span class="ot">(</span>acc<span class="ot">)</span>
<span class="kw">end</span></code></pre>
<p>I loaded this code using standard <code>load()</code> function. This also didn’t work. The reason is that, even if this is faster(which is probably not always the case), profiling showed that interpreter spents only 4% of the time for <code>bracket_map</code> lookups. So if this implementation only slightly faster, it just can’t make a big difference.</p></li>
</ul>
<p>Profiling output revealed that, 85% of the time spent on fetching the next instruction:</p>
<pre><code>@@ 69 @@
      |
      |     local code
      |     while pc &lt;= #program do
  85% |         code = program[pc]
      |         if code == 62 then
      |             tape_pos = tape_pos + 1
      |             if #tape &lt; tape_pos then
@@ 89 @@
      |             pc = bracket_map[pc]
      |         elseif code == 93 and tape[tape_pos] ~= 0 then
   4% |             pc = bracket_map[pc]
      |         end
      |         pc = pc + 1</code></pre>
<p>Which really means that you can’t optimize anything, because there’s nothing optimizable in <code>code = program[pc]</code>, since this is one of the most primitive operations that you can do in this language. (note that we don’t have metamethod assigned to this table, so <code>rawget</code> is not an optimization)</p>
<p>At this point the Lua results were like this:</p>
<pre><code>luajit example_lua.lua bench.b  34.41s user 0.00s system 99% cpu 34.442 total</code></pre>
<p>The fact that PyPy did better job than LuaJIT here is surprising and impressive. It seems like RPython and PyPy is doing a very good job here.</p>
<p>Since I already started gradually compiling things, I thought why not go further and compile everything. Here’s a simple BF to Lua compiler:</p>
<pre class="sourceCode lua"><code class="sourceCode lua"><span class="kw">function</span> compile<span class="ot">(</span>str<span class="ot">)</span>
    <span class="kw">local</span> pgm <span class="ot">=</span> <span class="ot">{}</span>
    <span class="fu">table.insert</span><span class="ot">(</span>pgm<span class="ot">,</span> <span class="st">[[</span>
<span class="st">function pgm()</span>
<span class="st">    local tape = {0}</span>
<span class="st">    local tape_pos = 1</span>
<span class="st">]]</span><span class="ot">)</span>

    <span class="kw">local</span> adv <span class="ot">=</span> <span class="st">[[</span>
<span class="st">    tape_pos = tape_pos + 1</span>
<span class="st">    if #tape &lt; tape_pos then</span>
<span class="st">        table.insert(tape, 0)</span>
<span class="st">    end</span>
<span class="st">]]</span>
    <span class="kw">local</span> <span class="kw">function</span> dev<span class="ot">(</span>i<span class="ot">)</span> <span class="kw">return</span> <span class="st">&quot;    tape_pos = tape_pos - &quot;</span> <span class="ot">..</span> i <span class="ot">..</span> <span class="st">&quot;</span><span class="ot">\n</span><span class="st">&quot;</span> <span class="kw">end</span>
    <span class="kw">local</span> <span class="kw">function</span> inc<span class="ot">(</span>i<span class="ot">)</span> <span class="kw">return</span> <span class="st">&quot;    tape[tape_pos] = tape[tape_pos] + &quot;</span> <span class="ot">..</span> i <span class="ot">..</span> <span class="st">&quot;</span><span class="ot">\n</span><span class="st">&quot;</span> <span class="kw">end</span>
    <span class="kw">local</span> <span class="kw">function</span> dec<span class="ot">(</span>i<span class="ot">)</span> <span class="kw">return</span> <span class="st">&quot;    tape[tape_pos] = tape[tape_pos] - &quot;</span> <span class="ot">..</span> i <span class="ot">..</span> <span class="st">&quot;</span><span class="ot">\n</span><span class="st">&quot;</span> <span class="kw">end</span>
    <span class="kw">local</span> out  <span class="ot">=</span> <span class="st">&quot;    io.write(string.char(tape[tape_pos]))</span><span class="ot">\n</span><span class="st">&quot;</span>
    <span class="kw">local</span> inp  <span class="ot">=</span> <span class="st">&quot;&quot;</span> <span class="co">-- no need for this</span>
    <span class="kw">local</span> jmpF <span class="ot">=</span> <span class="st">&quot;    while tape[tape_pos] ~= 0 do</span><span class="ot">\n</span><span class="st">&quot;</span>
    <span class="kw">local</span> jmpB <span class="ot">=</span> <span class="st">&quot;    if tape[tape_pos] == 0 then break end end</span><span class="ot">\n</span><span class="st">&quot;</span>

    <span class="co">-- these are used to combine consecutive same instructions</span>
    <span class="kw">local</span> devs <span class="ot">=</span> <span class="dv">0</span>
    <span class="kw">local</span> incs <span class="ot">=</span> <span class="dv">0</span>
    <span class="kw">local</span> decs <span class="ot">=</span> <span class="dv">0</span>

    <span class="kw">local</span> indent <span class="ot">=</span> <span class="dv">0</span><span class="ot">;</span>

    <span class="kw">for</span> i<span class="ot">=</span><span class="dv">1</span><span class="ot">,</span> <span class="ot">#</span>str <span class="kw">do</span>
        <span class="kw">local</span> char <span class="ot">=</span> <span class="fu">string.char</span><span class="ot">(</span><span class="fu">string.byte</span><span class="ot">(</span>str<span class="ot">,</span> i<span class="ot">))</span>

        <span class="kw">if</span> devs <span class="ot">~=</span> <span class="dv">0</span> <span class="kw">and</span> char <span class="ot">~=</span> <span class="st">&quot;&lt;&quot;</span> <span class="kw">then</span>
            <span class="fu">table.insert</span><span class="ot">(</span>pgm<span class="ot">,</span> indent_lines<span class="ot">(</span>indent<span class="ot">,</span> dev<span class="ot">(</span>devs<span class="ot">)))</span>
            devs <span class="ot">=</span> <span class="dv">0</span>
        <span class="kw">elseif</span> incs <span class="ot">~=</span> <span class="dv">0</span> <span class="kw">and</span> char <span class="ot">~=</span> <span class="st">&quot;+&quot;</span> <span class="kw">then</span>
            <span class="fu">table.insert</span><span class="ot">(</span>pgm<span class="ot">,</span> indent_lines<span class="ot">(</span>indent<span class="ot">,</span> inc<span class="ot">(</span>incs<span class="ot">)))</span>
            incs <span class="ot">=</span> <span class="dv">0</span>
        <span class="kw">elseif</span> decs <span class="ot">~=</span> <span class="dv">0</span> <span class="kw">and</span> char <span class="ot">~=</span> <span class="st">&quot;-&quot;</span> <span class="kw">then</span>
            <span class="fu">table.insert</span><span class="ot">(</span>pgm<span class="ot">,</span> indent_lines<span class="ot">(</span>indent<span class="ot">,</span> dec<span class="ot">(</span>decs<span class="ot">)))</span>
            decs <span class="ot">=</span> <span class="dv">0</span>
        <span class="kw">end</span>

        <span class="kw">if</span> char <span class="ot">==</span> <span class="st">&quot;&gt;&quot;</span> <span class="kw">then</span> <span class="co">-- 62</span>
            <span class="fu">table.insert</span><span class="ot">(</span>pgm<span class="ot">,</span> indent_lines<span class="ot">(</span>indent<span class="ot">,</span> adv<span class="ot">))</span>
        <span class="kw">elseif</span> char <span class="ot">==</span> <span class="st">&quot;&lt;&quot;</span> <span class="kw">then</span> <span class="co">-- 60</span>
            devs <span class="ot">=</span> devs <span class="ot">+</span> <span class="dv">1</span>
        <span class="kw">elseif</span> char <span class="ot">==</span> <span class="st">&quot;+&quot;</span> <span class="kw">then</span> <span class="co">-- 43</span>
            incs <span class="ot">=</span> incs <span class="ot">+</span> <span class="dv">1</span>
        <span class="kw">elseif</span> char <span class="ot">==</span> <span class="st">&quot;-&quot;</span> <span class="kw">then</span> <span class="co">-- 45</span>
            decs <span class="ot">=</span> decs <span class="ot">+</span> <span class="dv">1</span>
        <span class="kw">elseif</span> char <span class="ot">==</span> <span class="st">&quot;.&quot;</span> <span class="kw">then</span> <span class="co">-- 46</span>
            <span class="fu">table.insert</span><span class="ot">(</span>pgm<span class="ot">,</span> indent_lines<span class="ot">(</span>indent<span class="ot">,</span> out<span class="ot">))</span>
        <span class="kw">elseif</span> char <span class="ot">==</span> <span class="st">&quot;,&quot;</span> <span class="kw">then</span> <span class="co">-- 44</span>
            <span class="fu">table.insert</span><span class="ot">(</span>pgm<span class="ot">,</span> indent_lines<span class="ot">(</span>indent<span class="ot">,</span> inp<span class="ot">))</span>
        <span class="kw">elseif</span> char <span class="ot">==</span> <span class="st">&quot;[&quot;</span> <span class="kw">then</span> <span class="co">-- 91</span>
            indent <span class="ot">=</span> indent <span class="ot">+</span> <span class="dv">4</span>
            <span class="fu">table.insert</span><span class="ot">(</span>pgm<span class="ot">,</span> indent_lines<span class="ot">(</span>indent<span class="ot">,</span> jmpF<span class="ot">))</span>
        <span class="kw">elseif</span> char <span class="ot">==</span> <span class="st">&quot;]&quot;</span> <span class="kw">then</span> <span class="co">-- 93</span>
            indent <span class="ot">=</span> indent <span class="ot">-</span> <span class="dv">4</span>
            <span class="fu">table.insert</span><span class="ot">(</span>pgm<span class="ot">,</span> indent_lines<span class="ot">(</span>indent<span class="ot">,</span> jmpB<span class="ot">))</span>
        <span class="kw">end</span>
    <span class="kw">end</span>

    <span class="fu">table.insert</span><span class="ot">(</span>pgm<span class="ot">,</span> <span class="st">&quot;end&quot;</span><span class="ot">)</span>
    <span class="kw">return</span> <span class="fu">table.concat</span><span class="ot">(</span>pgm<span class="ot">)</span>
<span class="kw">end</span></code></pre>
<p>One thing to note here is that loops in BF programs correspond to loops in generated Lua. There’s another way to implement this compiler and it might turn out to be more efficient, but I didn’t try it. (see BF-to-C compiler below) Also, I’m merging some instructions together. This has significant performance impact, but it’s also necessary because if the generated code is too big, both PUC-Lua and LuaJIT is rejecting to load it. (this is documented, but the limit is not specified)</p>
<p>Results:</p>
<pre><code>luajit example_lua.lua bench.b  0.53s user 0.00s system 99% cpu 0.532 total</code></pre>
<p>Note that runtime code generation and loading is NOT included in this number, but code generation takes less than 0.01s, so I might just include that.</p>
<p>Just for completeness, I also tried a <a href="https://github.com/kgabis/brainfuck-c/blob/master/brainfuck.c">C interpreter</a>, and <a href="http://awk.info/?doc/bfc.html">BF-to-C</a> compiler:</p>
<pre><code>./c-int bench.b  2.44s user 0.00s system 99% cpu 2.443 total
./c-compiled     0.00s user 0.00s system 82% cpu 0.004 total</code></pre>
<p>A fun thing about C compiler is that compiling generated C programs takes long time:</p>
<pre><code>gcc -O3 awk_output.c  14.07s user 0.14s system 99% cpu 14.219 total</code></pre>
<p>RPython once again does an impressive job here, because it’s even faster than C interpreter. I didn’t bother profiling C code and optimizing it, because it looks like a reasonable implementation: A simple “fetch instruction and run it in a case statement” loop.</p>
<p>So I think the conclusion is that RPython and PyPy are doing really good job.</p>]]></summary>
</entry>
<entry>
    <title>Top-down expression parsing is easy</title>
    <link href="http://osa1.net/posts/2015-01-29-top-down-expr-parsing-easy.html" />
    <id>http://osa1.net/posts/2015-01-29-top-down-expr-parsing-easy.html</id>
    <published>2015-01-29T00:00:00Z</published>
    <updated>2015-01-29T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>I recently fixed <a href="http://hackage.haskell.org/package/language-lua">language-lua</a>’s 2-years-old expression parsing bug. Previously it was using <a href="http://hackage.haskell.org/package/parsec-3.1.8/docs/Text-Parsec-Expr.html">Parsec’s expression parser</a>, which is actually horrible because it can’t handle chained unary operators.</p>
<p>Two weeks ago I decided to take a look into Lua’s original implementation, and in about an hour or so the algorithm was crystal clear to me. I immediately <a href="https://github.com/osa1/language-lua/commit/b4bebe36e927dcc671dbe6dd19572b83073dc556#diff-630bbd2d118baf109da6ad79d3f168bfR257">implemented it</a> and closed the <a href="https://github.com/osa1/language-lua/issues/2">2-years-old bug report</a>.</p>
<p>This implementation is essentially a port of Lua’s expression parser. Recently I thought about the algorithm and I was wondering if this has a name – the algorithm looked pretty obvious to me once I understand and given how much we know about parsing I thought this should have a name.</p>
<p>I found <a href="http://www.engr.mun.ca/~theo/Misc/exp_parsing.htm#climbing">this algorithm named “precedence climbing”</a>. This is almost the same algorithm, only difference is that instead of using <code>lookahead</code> I’m just consuming the binary operator and returning it to the caller(which is parsing an expression with lower precedence than current parser) if precedence is lower. Associativity handling is also different(I use different left and right precedences to handle associativity) but the idea is really the same.</p>
<p>Now, there is also another algorithm called Pratt, and I can’t read the original paper(paywall), but according to <a href="http://lambda-the-ultimate.org/node/3682">this LtU discussion</a> it should also be similar. Indeed, <a href="http://journal.stuffwithstuff.com/2011/03/19/pratt-parsers-expression-parsing-made-easy/">this explanation of it</a> looks pretty similar, and <a href="http://stackoverflow.com/a/13637731/691032">this StackOverflow answer</a> says that Lua’s implementation is “Pratt style parsing”.</p>
<p>So it seems like we have two, or maybe one since they’re actually very similar, solution(s) to solve top-down expression parsing problem and Haskell implementation using Parsec is possible in only 12 lines of code.</p>
<h1 id="a-challenge">A challenge</h1>
<p>One challenge might be to modify Parsec’s expression parser so that internally it generates a Pratt/precedence climbing parser. I’m hoping to spare some time to work on this.</p>]]></summary>
</entry>
<entry>
    <title>Loading dynamic Haskell libs in Lua</title>
    <link href="http://osa1.net/posts/2015-01-16-haskell-so-lua.html" />
    <id>http://osa1.net/posts/2015-01-16-haskell-so-lua.html</id>
    <published>2015-01-16T00:00:00Z</published>
    <updated>2015-01-16T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>Last year I wrote <a href="http://osa1.net/posts/2014-04-27-calling-haskell-lua.html">a blog post</a> in which I explained how to call Lua from Haskell and Haskell from Lua using <a href="http://hackage.haskell.org/package/hslua">hslua</a> library. At the end of that blog post I mentioned that it should be possible to compile Haskell code to shared library and load that in Lua.</p>
<p>Today a friend in our research group <a href="https://github.com/iu-parfunc">parfunc</a> asked a question about compiling Haskell to shared libraries and loading generated libraries in other programs and I thought while I’m at it I can just update my blog post as well. So in this post I’m going to explain how to compile Haskell functions to shared libraries and load them in Lua.</p>
<p>Before diving into the code, a few remarks:</p>
<ul>
<li>All the code in this blog post is tried on Linux, with Lua 5.1 and latest LuaJIT.</li>
<li>To be able to load our function in Lua and register it, our functions should have C linkage and <a href="http://www.lua.org/manual/5.1/manual.html#lua_CFunction"><code>lua_CFunction</code></a> type. We can either write Haskell functions directly using this type, or write C wrapper functions around our Haskell functions to be able to use them in Lua. In this post I’m going to do first one.</li>
<li>We’ll need some intermediate C code to expose some Haskell RTS functions to Lua, like <code>hs_init</code> to start Haskell runtime and <code>hs_exit</code> to stop it.</li>
<li>To be able to <code>require</code> our shared library in Lua, we need to implement a <code>int luaopen_&lt;ourlibrary&gt;(lua_State *L)</code> function. While in theory it should be possible to implement that function in Haskell, I’ll implement it in C in this post, because I’m not sure how to write Lua wrappers for <code>hs_init</code> and <code>hs_exit</code> in Haskell.</li>
<li>To keep the code as simple as possible, our Haskell function will be a very dumb addition function.</li>
</ul>
<p>Let’s start.</p>
<h1 id="defining-lua-function-in-haskell">Defining Lua function in Haskell</h1>
<p>This is exactly the same as before: We just define a function with type: <code>LuaState -&gt; IO Int</code>. To keep the code simple, we don’t do error handling at all.</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">module</span> <span class="dt">LibArith</span> <span class="kw">where</span>

<span class="kw">import </span><span class="dt">Data.Maybe</span>
<span class="kw">import </span><span class="dt">Scripting.Lua</span> <span class="co">-- this one from hslua</span>

foreign export ccall
<span class="ot">  add ::</span> <span class="dt">LuaState</span> <span class="ot">-&gt;</span> <span class="dt">IO</span> ()

<span class="ot">add ::</span> <span class="dt">LuaState</span> <span class="ot">-&gt;</span> <span class="dt">IO</span> ()
add l <span class="fu">=</span> <span class="kw">do</span>
  i1 <span class="ot">&lt;-</span> fromJust <span class="ot">`fmap`</span> peek l <span class="dv">1</span>
  i2 <span class="ot">&lt;-</span> fromJust <span class="ot">`fmap`</span> peek l <span class="dv">2</span>
  pop l <span class="dv">2</span>
  push l (i1 <span class="fu">+</span><span class="ot"> i2 ::</span> <span class="dt">Int</span>)
  return <span class="dv">1</span></code></pre>
<h1 id="implementing-intermediate-c">Implementing intermediate C</h1>
<p>In our C glue code, we do two things:</p>
<ol style="list-style-type: decimal">
<li>Wrap <code>hs_init</code> and <code>hs_exit</code> Haskell runtime functions.</li>
<li>Implement Lua C module interface in which we register our functions to Lua. (see <a href="http://www.lua.org/manual/5.1/manual.html#pdf-package.loaders">related docs</a> for details)</li>
</ol>
<p>Here’s the code:</p>
<pre class="sourceCode c"><code class="sourceCode c"><span class="ot">#include &quot;LibArith_stub.h&quot;</span>
<span class="ot">#include &quot;lua.h&quot;</span>

<span class="dt">int</span> hs_init_lua(lua_State *L)
{
  hs_init(NULL, NULL);
  <span class="kw">return</span> <span class="dv">0</span>;
}

<span class="dt">int</span> hs_exit_lua(lua_State *L)
{
  hs_exit();
  <span class="kw">return</span> <span class="dv">0</span>;
}

<span class="dt">int</span> luaopen_lualibhelper(lua_State *L)
{
  lua_pushcfunction(L, add);
  lua_setglobal(L, <span class="st">&quot;add_in_haskell&quot;</span>);
  lua_pushcfunction(L, hs_init_lua);
  lua_setglobal(L, <span class="st">&quot;hs_init&quot;</span>);
  lua_pushcfunction(L, hs_exit_lua);
  lua_setglobal(L, <span class="st">&quot;hs_exit&quot;</span>);
  <span class="kw">return</span> <span class="dv">0</span>;
}</code></pre>
<p>Some things to note:</p>
<ul>
<li><code>LibArith_stub.h</code> is generated by GHC. I’ll explain how to compile and link next.</li>
<li>Our Haskell function actually has type <code>HsInt (*)(void *)</code>. While this is not what Lua API expected(it expects <code>int (*)(lua_State *L)</code>), in my x86_64 Linux machine this is working fine. In the worst case, you may need to wrap the Haskell function in C and convert the types using Haskell RTS C API and Lua C API.</li>
</ul>
<h1 id="compiling-and-linking">Compiling and linking</h1>
<p>This is the tricky part, I wasted a good 2 hours trying to figure how to compile to <code>.so</code> and link it with correct set of libraries.</p>
<p>First step is to compile <code>hslua</code> in a sandbox, or at least make it reachable by GHC(by installing globally, using nix environments etc.). I’ll be giving commands assuming that you’re in a sandbox that has <code>hslua</code> installed, if you’re not, then just replace <code>cabal exec ghc --</code> part with <code>ghc</code> and it should just work.</p>
<p>Step 1, compile and link the Haskell code to generate a shared library:</p>
<pre><code>$ cabal exec ghc -- LibArith.hs -shared -dynamic -fPIC -o libarith.so -lHSrts-ghc7.8.3</code></pre>
<p>Note that if you’re using a different version of GHC, you’ll need to modify the last argument to make it link it with corrent GHC RTS library.(alternatively, you can link with debug or profiling versions etc.)</p>
<p>Step 2, compile the Lua module written in C(the C code above) and link it with our shared Haskell library:</p>
<pre><code>$ cabal exec ghc -- libarithhelper.c -no-hs-main -optl -larith -o lualibhelper.so -shared -fPIC -dynamic</code></pre>
<p>Note that you may need to pass extra linker parameters if you have Lua library/headers in non-standard locations. If that’s the case, <code>-optl</code> argument of GHC is used to add linker arguments, just use standard linker arguments with that(<code>-L</code>, <code>-I</code> etc.).</p>
<p>This command should print a warning like this:</p>
<pre><code>/home/omer/opt/luajit_bin/include/luajit-2.0/lua.h:168:16:
     note: expected ‘lua_CFunction’ but argument is of type ‘HsInt (*)(void *)’
     LUA_API void  (lua_pushcclosure) (lua_State *L, lua_CFunction fn, int n);</code></pre>
<p>Like mentioned above, this doesn’t make any difference on my x86_64 Linux machine. If that’s being a problem on your system, just wrap your Haskell function in intermediate C code above using Haskell RTS API.</p>
<p>Now you should have two shared libraries, one for our Haskell code and one for the intermediate C code. One problem is that the shared library generated from C is now depending on the one generated from Haskell. So Haskell library should be in your <code>LD_LIBRARY_PATH</code>.</p>
<p>A good improvement here would be to compile Haskell code to static library, and generate one dynamic library only. (which has Haskell library statically linked to it)</p>
<h1 id="loading-the-code-in-lua">Loading the code in Lua</h1>
<p>Before loading it, make sure that the dynamic linker can really find the shared library generated from Haskell. Run this:</p>
<pre><code>$ ldd lualibhelper.so | grep &quot;not found&quot;</code></pre>
<p>Make sure it’s not printing anything.</p>
<p>Now just run Lua and enjoy the library:</p>
<pre><code>$ luajit-2.0.3
LuaJIT 2.0.3 -- Copyright (C) 2005-2014 Mike Pall. http://luajit.org/
JIT: ON CMOV SSE2 SSE3 SSE4.1 fold cse dce fwd dse narrow loop abc sink fuse
&gt; require &quot;lualibhelper&quot;
&gt; hs_init()
&gt; print(add_in_haskell(1, 2))
3
&gt; print(add_in_haskell(-10, 20))
10</code></pre>
<p>Just for the amusement, let’s crash it by running Haskell function <em>after</em> stopping the Haskell runtime:</p>
<pre><code>&gt; hs_exit()
&gt; add_in_haskell(1, 2)
newBoundTask: RTS is not initialised; call hs_init() first</code></pre>
<p>Fun :)</p>
<h1 id="conclusion">Conclusion</h1>
<p>It turns out that extending Lua using Haskell is almost as easy as the doing it using the technique I explained in my <a href="http://osa1.net/posts/2014-04-27-calling-haskell-lua.html">previous blog post on this topic</a>.</p>
<p>This post also demonstrates one other thing, namely, compiling Haskell libraries to shared libraries and dynamically loading them in different programs. I’m hoping that this post helps fellow Haskellers to extend their programs written in different languages with Haskell.</p>]]></summary>
</entry>
<entry>
    <title>Understanding Futamura Projections</title>
    <link href="http://osa1.net/posts/2015-01-11-understanding-futamura-projections.html" />
    <id>http://osa1.net/posts/2015-01-11-understanding-futamura-projections.html</id>
    <published>2015-01-11T00:00:00Z</published>
    <updated>2015-01-11T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>Here’s a way to understand Futamura projections<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>:</p>
<p>(Quick note: “Partial evaluator” == “specializer”. As far as I can see these words are also used interchangeably in the literature)</p>
<p>We call our specializer <code>specialize</code>. We specify languages using capital letters, <code>S</code>, <code>T</code>, <code>L</code> etc. We use Haskell syntax for applications. (e.g. <code>f a1 a2 a3</code> is a function application of <code>f</code> to three arguments, applications are left-associative in the case of currying)</p>
<p><code>specialize</code> takes two arguments, first argument is the program(function) to specialize, second argument is the input to specialize the program(function) on.</p>
<p>Note that our specializers are “correct”: For all specializer <code>s</code>, program <code>f</code> and program arguments <code>a</code> and <code>b</code>, <code>(s f a) b</code> is semantically same as <code>f a b</code>.</p>
<p>We show a specializer written in L which operates on programs written in T as <code>specialize_L_T</code>.</p>
<p>Now there are three interesting “Futamura projections”. Let’s say we have an interpreter for a language <code>L</code>, called <code>int</code>, which is written in <code>T</code>. We use <code>*</code> as a wildcard for languages. (e.g. it can be substituted with any language)</p>
<ol style="list-style-type: decimal">
<li><p><code>specialize_*_T int int_pgm</code>: We specialized the interpreter on a program <code>int_pgm</code>, resulting program is in <code>T</code>. We now have a program in <code>T</code> which just gets arguments of the interpreted program and produces output. This gives us a compiled version of <code>int_pgm</code> to <code>T</code>.</p></li>
<li><p><code>specialize_T_T specialize_T_T int</code>: We specialized the specializer on an interpreter. Generated program will be in <code>T</code>, and it’ll be expecting interpreter programs as input. The output will be specialized version of <code>int</code> for the given interpreter program. So we got a compiler for the interpreter <code>int</code>!</p>
<p>Note that specializers now need to be written in the language that they operate on. Alternatively, we could use two different specializers: One for specialize the interpreter, and one for specializing the interpreter specializer. (e.g. in most general form, we can have <code>specialize_A_B specialize_B_C int</code> where <code>int</code> is written in <code>C</code>)</p></li>
<li><p><code>specialize_T_T specialize_T_T specialize T_T</code>: This one is tricky. Let’s try to think what will be the generated program. We know from (2) is that <code>specialize_T_T specialize_T_T int</code> is a compiler for the language that <code>int</code> interprets. Now, we know from the note above that specializing doesn’t change meaning of the program, so our term from (2) <code>specialize_T_T specialize_T_T int</code> should be same with <code>(specialize_T_T specialize_T_T specialize_T_T) int</code>. What happens if we don’t apply the last <code>int</code>? Then we got a program that takes an interpreter and specializes it, resulting with a program in <code>T</code> that doesn’t expect interpreter argument. This is a compiler-compiler. Given an interpreter in <code>T</code>, it gives us a compiled version.</p></li>
</ol>
<p>Futamura projections are originally introduced in Futamura’s <a href="https://cs.au.dk/~hosc/local/HOSC-12-4-pp381-391.pdf">“Partial Evaluation of Computation Process – An Approach to a Compiler-Compiler”</a> and also described in <a href="http://www.itu.dk/people/sestoft/pebook/jonesgomardsestoft-letter.pdf">“Partial Evaluation and Automatic Program Generation”</a>.</p>
<p>Thinking about languages and interpreters are good way to have an intuition about how partial evaluation, specializing specializers etc. work, and “writing interpreters on problem domain” may be a good and general approach to solving problems<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a>, but I’m wondering what interesting programs and results we would get if we apply the these to different domains. Any ideas and pointers would be appreciated.</p>
<h1 id="problem-with-implementing-the-idea">Problem with implementing the idea</h1>
<p>Implementing ideas are generally a good way to learn, but in this case it’s a bit tricky. If we want to specialize specializers(like in projection (2) and (3)) we need to write one specializer in the language that it specializes, so we need a <code>specialize_T_T</code> for a <code>T</code>.</p>
<p>To be more concrete, if we want to write the specializer in Haskell, then it has to be operating on Haskell so that we could specialize it on itself. Now this is no trivial work, Haskell is a big and complicated language.</p>
<p>On the other hand, if we want to roll our own language just to try this idea, then we’ll have to write the specializer in our language. This is also not trivial, because we need implement a language that is expressive enough to write a specializer for itself.</p>
<p>Designing a minimal language that is expressive enough to implement the idea may be a good challenge.</p>
<hr />
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Futamura projections are what you get when you apply partial evaluators to interpreters and to themselves. Have a look at <a href="http://en.wikipedia.org/wiki/Partial_evaluation#Futamura_projections">related Wikipedia page</a> and see bottom of the post for more resources.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>I recently stumbled upon this <a href="http://stackoverflow.com/questions/27852709/enterprise-patterns-with-functional-programming/27860072#27860072">SO answer</a> to a question about functional design patterns. It’s interesting how forcing yourself to a particular paradigm leads to different approaches and ways to solving problems. This is one of the reasons why I’m trying to learn a new paradigm using a language that is specifically crafted for that paradigm(e.g. Haskell for functional programming instead of Lisps, Scala etc.).<a href="#fnref2">↩</a></p></li>
</ol>
</div>]]></summary>
</entry>

</feed>
