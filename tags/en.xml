<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>osa1.net - Posts tagged en</title>
    <link href="http://osa1.net/tags/en.xml" rel="self" />
    <link href="http://osa1.net" />
    <id>http://osa1.net/tags/en.xml</id>
    <author>
        <name>Ömer Sinan Ağacan</name>
        <email>omeragaca@gmail.com</email>
    </author>
    <updated>2016-07-25T00:00:00Z</updated>
    <entry>
    <title>IORef and STRef under the hood</title>
    <link href="http://osa1.net/posts/2016-07-25-IORef-STRef-exposed.html" />
    <id>http://osa1.net/posts/2016-07-25-IORef-STRef-exposed.html</id>
    <published>2016-07-25T00:00:00Z</published>
    <updated>2016-07-25T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>In this post we’ll take a look an internals of GHC’s mutable variables, and how they’re used by <a href="http://hackage.haskell.org/package/base-4.9.0.0/docs/Data-IORef.html"><code>IORef</code></a> and <a href="http://hackage.haskell.org/package/base-4.9.0.0/docs/Data-STRef.html"><code>STRef</code></a>. The code is copied from GHC, with minor changes for clarity.</p>
<hr />
<pre><code>λ&gt; :m + Data.IORef
λ&gt; :info IORef
newtype IORef a
  = GHC.IORef.IORef (GHC.STRef.STRef GHC.Prim.RealWorld a)
        -- Defined in ‘GHC.IORef’
instance Eq (IORef a) -- Defined in ‘GHC.IORef’</code></pre>
<p><code>GHC.IORef</code> is defined in <code>libraries/base/GHC/IORef.hs</code>:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="co">-- | A mutable variable in the &#39;IO&#39; monad</span>
<span class="kw">newtype</span> <span class="dt">IORef</span> a <span class="fu">=</span> <span class="dt">IORef</span> (<span class="dt">STRef</span> <span class="dt">RealWorld</span> a)</code></pre>
<p>We’ll look at 3 operations: read, write, and atomic modify.</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="co">-- | Read the value of an &#39;IORef&#39;</span>
<span class="ot">readIORef   ::</span> <span class="dt">IORef</span> a <span class="ot">-&gt;</span> <span class="dt">IO</span> a
readIORef  (<span class="dt">IORef</span> var) <span class="fu">=</span> stToIO (readSTRef var)

<span class="co">-- | Write a new value into an &#39;IORef&#39;</span>
<span class="ot">writeIORef  ::</span> <span class="dt">IORef</span> a <span class="ot">-&gt;</span> a <span class="ot">-&gt;</span> <span class="dt">IO</span> ()
writeIORef (<span class="dt">IORef</span> var) v <span class="fu">=</span> stToIO (writeSTRef var v)

<span class="ot">atomicModifyIORef ::</span> <span class="dt">IORef</span> a <span class="ot">-&gt;</span> (a <span class="ot">-&gt;</span> (a,b)) <span class="ot">-&gt;</span> <span class="dt">IO</span> b
atomicModifyIORef (<span class="dt">IORef</span> (<span class="dt">STRef</span> r<span class="st">#)) f = IO $ \s -&gt; atomicModifyMutVar# r# f s</span></code></pre>
<p><code>STRef</code> is defined in <code>libraries/base/GHC/STRef.hs</code>:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="co">-- | A value of type `STRef s a` is a mutable variable in state thread `s`,</span>
<span class="co">-- containing a value of type `a`</span>
<span class="kw">data</span> <span class="dt">STRef</span> s a <span class="fu">=</span> <span class="dt">STRef</span> (<span class="dt">MutVar</span><span class="st"># s a)</span>

<span class="co">-- | Read the value of an &#39;STRef&#39;</span>
<span class="ot">readSTRef ::</span> <span class="dt">STRef</span> s a <span class="ot">-&gt;</span> <span class="dt">ST</span> s a
readSTRef (<span class="dt">STRef</span> var<span class="st">#) = ST $ \s1# -&gt; readMutVar# var# s1#</span>

<span class="co">-- | Write a new value into an &#39;STRef&#39;</span>
<span class="ot">writeSTRef ::</span> <span class="dt">STRef</span> s a <span class="ot">-&gt;</span> a <span class="ot">-&gt;</span> <span class="dt">ST</span> s ()
writeSTRef (<span class="dt">STRef</span> var<span class="st">#) val = ST $ \s1# -&gt;</span>
    <span class="kw">case</span> writeMutVar<span class="st"># var# val s1# of</span>
      s2<span class="st"># -&gt; (# s2#, () #)</span></code></pre>
<p>Note that there’s no <code>atomicModifySTRef</code>, because that only makes sense in IO context. So <code>atomicModifyIORef</code> directly calls the primop.</p>
<p>In summary:</p>
<ul>
<li>IORef: <code>MutVar#</code>, wrapped with <code>STRef</code>. When we unpack an <code>IORef</code> in data constructor fields, internally we store a <code>MutVar#</code>.</li>
<li>writeIORef, writeSTRef: <code>writeMutVar#</code></li>
<li>readIORef, readSTRef: <code>readMutVar#</code></li>
<li>atomicModifyIORef: <code>atomicModifyMutVar#</code></li>
</ul>
<h1 id="readmutvar">readMutVar#</h1>
<p>Primop definition:</p>
<pre><code>primop  ReadMutVarOp &quot;readMutVar#&quot; GenPrimOp
   MutVar# s a -&gt; State# s -&gt; (# State# s, a #)
   {Read contents of {\tt MutVar\#}. Result is not yet evaluated.}
   with
   has_side_effects = True
   can_fail         = True</code></pre>
<p>Code generation is handled by <code>StgCmmPrim.emitPrimOp</code>:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">emitPrimOp ::</span> <span class="dt">DynFlags</span>
           <span class="ot">-&gt;</span> [<span class="dt">LocalReg</span>]        <span class="co">-- where to put the results</span>
           <span class="ot">-&gt;</span> <span class="dt">PrimOp</span>            <span class="co">-- the op</span>
           <span class="ot">-&gt;</span> [<span class="dt">CmmExpr</span>]         <span class="co">-- arguments</span>
           <span class="ot">-&gt;</span> <span class="dt">FCode</span> ()

<span class="fu">...</span>

emitPrimOp dflags [res] <span class="dt">ReadMutVarOp</span> [mutv]
   <span class="fu">=</span> emitAssign (<span class="dt">CmmLocal</span> res) (cmmLoadIndexW dflags mutv (fixedHdrSizeW dflags) (gcWord dflags))</code></pre>
<p>This is just relative addressing, base is the <code>MutVar#</code> we’re reading, and we skip the closure header to read the contents.</p>
<h1 id="writemutvar">writeMutVar#</h1>
<p>Primop definition:</p>
<pre><code>primop  WriteMutVarOp &quot;writeMutVar#&quot;  GenPrimOp
   MutVar# s a -&gt; a -&gt; State# s -&gt; State# s
   {Write contents of {\tt MutVar\#}.}
   with
   has_side_effects = True
   code_size        = { primOpCodeSizeForeignCall }
                         -- for the write barrier
   can_fail         = True</code></pre>
<p>Code generation is again implemented in <code>emitPrimOp</code>:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell">emitPrimOp dflags [] <span class="dt">WriteMutVarOp</span> [mutv,var]
   <span class="fu">=</span> <span class="kw">do</span> emitStore (cmmOffsetW dflags mutv (fixedHdrSizeW dflags)) var
        emitCCall [<span class="co">{-no results-}</span>]
                  (<span class="dt">CmmLit</span> (<span class="dt">CmmLabel</span> mkDirty_MUT_VAR_Label))
                  [(<span class="dt">CmmReg</span> (<span class="dt">CmmGlobal</span> <span class="dt">BaseReg</span>), <span class="dt">AddrHint</span>), (mutv,<span class="dt">AddrHint</span>)]</code></pre>
<p>This is more involved than <code>readMutVar#</code>. First, we write the variable to the <code>MutVar#</code> in the first line (<code>emitStore</code>). Then, we call a function, specified by the variable <code>mkDirty_MUT_VAR_Label</code>, passing two arguments: a global called <code>BaseReg</code>, and the <code>MutVar#</code>. <code>mkDirty_MUT_VAR_Label</code> just holds the name of this function: (defined in <code>rts/sm/Storage.c</code>)</p>
<pre class="sourceCode c"><code class="sourceCode c"><span class="co">/*</span>
<span class="co">   This is the write barrier for MUT_VARs, a.k.a. IORefs.  A</span>
<span class="co">   MUT_VAR_CLEAN object is not on the mutable list; a MUT_VAR_DIRTY</span>
<span class="co">   is.  When written to, a MUT_VAR_CLEAN turns into a MUT_VAR_DIRTY</span>
<span class="co">   and is put on the mutable list.</span>
<span class="co">*/</span>
<span class="dt">void</span> dirty_MUT_VAR(StgRegTable *reg, StgClosure *p)
{
    Capability *cap = regTableToCapability(reg);
    <span class="kw">if</span> (p-&gt;header.info == &amp;stg_MUT_VAR_CLEAN_info) {
        p-&gt;header.info = &amp;stg_MUT_VAR_DIRTY_info;
        recordClosureMutated(cap,p);
    }
}</code></pre>
<p>Remember that for the first argument we passed something called <code>BaseReg</code>, and for the second argument we passed the <code>MutVar#</code>.</p>
<p>This function gets a <code>Capability</code> from the register table, and if the <code>MutVar#</code> is “clean”, it marks it as “dirty” and records in the capability that it’s now mutated.</p>
<p><code>Capability</code> lacks documentation, but it’s not too important, so we just skip that and look at <code>recordClsoureMutated</code>.</p>
<pre class="sourceCode c"><code class="sourceCode c"><span class="dt">void</span> recordClosureMutated(Capability *cap, StgClosure *p)
{
    bdescr *bd = Bdescr((StgPtr)p);
    <span class="kw">if</span> (bd-&gt;gen_no != <span class="dv">0</span>) recordMutableCap(p,cap,bd-&gt;gen_no);
}</code></pre>
<p><code>p</code> is our <code>MutVar#</code> here. <code>bdescr</code> stands for “block descriptor”. GHC RTS allocates memory in blocks, and every block belongs to a generation. First generation is special in that if a <code>MutVar#</code> is in the first generation, it can’t point to a younger generation, as the first generation is already the youngest generation. This is from <code>includes/rts/storage/GC.h</code>:</p>
<pre><code>- generation 0 is the allocation area.  It is given a fixed set of blocks
  during initialisation, and these blocks normally stay in G0S0.  In parallel
  execution, each Capability has its own nursery.</code></pre>
<p>This code basically checks if the <code>MutVar#</code> belongs to first generation (generation 0). If that’s not the case, we record the <code>MutVar#</code> in the generation’s “mut list”:</p>
<pre class="sourceCode c"><code class="sourceCode c"><span class="dt">void</span> recordMutableCap(<span class="dt">const</span> StgClosure *p, Capability *cap, <span class="dt">uint32_t</span> gen)
{
    bdescr* bd = cap-&gt;mut_lists[gen];
    <span class="kw">if</span> (bd-&gt;free &gt;= bd-&gt;start + BLOCK_SIZE_W) {
        bdescr *new_bd = allocBlockOnNode_lock(cap-&gt;node);
        new_bd-&gt;link = bd;
        bd = new_bd;
        cap-&gt;mut_lists[gen] = bd;
    }
    *bd-&gt;free++ = (StgWord)p;
}</code></pre>
<p>Garbage collector then checks that list when collecting younger generations, to avoid collecting young objects kept alive by older generations (i.e. pointers from older generations to younger generations, see <code>scavenge_capability_mut_lists</code> in <code>rts/sm/Scav.c</code>).</p>
<p>We saw in <code>dirty_MUT_VAR</code> that the <code>MutVar#</code> is marked as “dirty” when it’s mutated. When is it marked as “clean” again?</p>
<p>When a <code>MutVar#</code> is copied during GC, the object pointed by it is also copied to the same generation, and then the <code>MutVar#</code> becomes clean again, because it no longer points to a younger generation. This is the related code:</p>
<pre class="sourceCode c"><code class="sourceCode c"><span class="dt">static</span> <span class="dt">void</span>
scavenge_block(bdescr *bd)
{
    ...
    <span class="kw">case</span> MUT_VAR_DIRTY:
        gct-&gt;eager_promotion = rtsFalse;
        evacuate(&amp;((StgMutVar *)p)-&gt;var);
        gct-&gt;eager_promotion = saved_eager_promotion;
        <span class="kw">if</span> (gct-&gt;failed_to_evac) {
            ((StgClosure *)q)-&gt;header.info = &amp;stg_MUT_VAR_DIRTY_info;
        } <span class="kw">else</span> {
            ((StgClosure *)q)-&gt;header.info = &amp;stg_MUT_VAR_CLEAN_info;
        }
        p += sizeofW(StgMutVar);
        <span class="kw">break</span>;
    ...
}</code></pre>
<h1 id="atomicmodifymutvar">atomicModifyMutVar#</h1>
<p>Primop definition:</p>
<pre><code>primop  AtomicModifyMutVarOp &quot;atomicModifyMutVar#&quot; GenPrimOp
   MutVar# s a -&gt; (a -&gt; b) -&gt; State# s -&gt; (# State# s, c #)
   with
   out_of_line      = True
   has_side_effects = True
   can_fail         = True</code></pre>
<p><code>out_of_line = True</code> basically tells code generator that this primop is implemented as a function. Code generator then just generates a function call:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">cgOpApp ::</span> <span class="dt">StgOp</span>        <span class="co">-- The op</span>
        <span class="ot">-&gt;</span> [<span class="dt">StgArg</span>]     <span class="co">-- Arguments</span>
        <span class="ot">-&gt;</span> <span class="dt">Type</span>         <span class="co">-- Result type (always an unboxed tuple)</span>
        <span class="ot">-&gt;</span> <span class="dt">FCode</span> <span class="dt">ReturnKind</span>

<span class="fu">...</span>

cgOpApp (<span class="dt">StgPrimOp</span> primop) args res_ty <span class="fu">=</span> <span class="kw">do</span>
    dflags <span class="ot">&lt;-</span> getDynFlags
    cmm_args <span class="ot">&lt;-</span> getNonVoidArgAmodes args
    <span class="kw">case</span> shouldInlinePrimOp dflags primop cmm_args <span class="kw">of</span>
        <span class="dt">Nothing</span> <span class="ot">-&gt;</span> <span class="kw">do</span>  <span class="co">-- out-of-line</span>
          <span class="kw">let</span> fun <span class="fu">=</span> <span class="dt">CmmLit</span> (<span class="dt">CmmLabel</span> (mkRtsPrimOpLabel primop))
          emitCall (<span class="dt">NativeNodeCall</span>, <span class="dt">NativeReturn</span>) fun cmm_args
        <span class="fu">...</span></code></pre>
<p>The primop is implemented in Cmm, in <code>rts/PrimOps.cmm</code>. The code is a mess, but here’s the important part:</p>
<pre class="sourceCode c"><code class="sourceCode c">stg_atomicModifyMutVarzh ( gcptr mv, gcptr f )
{
  ...
  retry:
    x = StgMutVar_var(mv);
    StgThunk_payload(z,<span class="dv">1</span>) = x;
<span class="ot">#ifdef THREADED_RTS</span>
    (h) = ccall cas(mv + SIZEOF_StgHeader + OFFSET_StgMutVar_var, x, y);
    <span class="kw">if</span> (h != x) { <span class="kw">goto</span> retry; }
<span class="ot">#else</span>
    StgMutVar_var(mv) = y;
<span class="ot">#endif</span>

    <span class="kw">if</span> (GET_INFO(mv) == stg_MUT_VAR_CLEAN_info) {
        ccall dirty_MUT_VAR(BaseReg <span class="st">&quot;ptr&quot;</span>, mv <span class="st">&quot;ptr&quot;</span>);
    }

    <span class="kw">return</span> (r);
}</code></pre>
<p>It’s basically a compare-and-swap loop, and in the end it marks the <code>MutVar#</code> as “dirty”, using the same <code>dirty_MUT_VAR</code> function used by the code generated for <code>writeMutVar#</code>.</p>
<h1 id="the-mutvar-struct">The <code>MutVar#</code> struct</h1>
<p>As the last thing, we look at the definition of <code>MutVar#</code>: (in <code>includes/rts/storage/Closures.h</code>)</p>
<pre><code>typedef struct {
    StgHeader   header;
    StgClosure *var;
} StgMutVar;</code></pre>
<p>Nothing interesting here. See <a href="https://ghc.haskell.org/trac/ghc/wiki/Commentary/Rts/Storage/HeapObjects?redirectedfrom=Commentary/Rts/HeapObjects">this Wiki page</a> for GHC’s heap object layout. In our case, payload contains a single closure.</p>
<hr />
<p>This concludes our <code>MutVar#</code> (which is used under the hood for <code>IORef</code> and <code>STRef</code>) tour. I guess lessons here are:</p>
<ol style="list-style-type: decimal">
<li><p><code>readIORef</code> is fast, but <code>writeIORef</code> is one function call in the best case. In the worst case, it does an expensive allocation (this is not just a heap pointer bump). If you have a tight loop with some state variables, prefer parameter passing instead.</p></li>
<li><p>Unpacking an <code>IORef</code> in a data constructor field does not really make the constructor mutable. Instead, it inlines the <code>MutVar#</code>, which has a mutable pointer field.</p></li>
</ol>
<p>If you think about it a little bit, you may realize that optimizing (2) is actually quite tricky. Imagine having something like this:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">data</span> <span class="dt">D</span> <span class="fu">=</span> <span class="dt">D</span> {<span class="ot"> f1 ::</span> <span class="ot">{-# UNPACK #-}</span> <span class="fu">!</span>(<span class="dt">IORef</span> <span class="dt">Int</span>)
           ,<span class="ot"> f2 ::</span> <span class="dt">Int</span>
           }

<span class="ot">bumpf1 ::</span> <span class="dt">D</span> <span class="ot">-&gt;</span> <span class="dt">IO</span> ()
bumpf1 (<span class="dt">D</span> f1 _) <span class="fu">=</span> modifyIORef f1 (<span class="fu">+</span> <span class="dv">1</span>)

<span class="ot">bumpf2 ::</span> <span class="dt">D</span> <span class="ot">-&gt;</span> <span class="dt">D</span>
bumpf2 (<span class="dt">D</span> f1 f2) <span class="fu">=</span> <span class="dt">D</span> f1 (f2 <span class="fu">+</span> <span class="dv">1</span>)</code></pre>
<p>You’d expect this to print <code>True</code>:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">do</span> ref <span class="ot">&lt;-</span> newIORef <span class="dv">0</span>
   <span class="kw">let</span> d1 <span class="fu">=</span> <span class="dt">D</span> ref <span class="dv">0</span>
       d2 <span class="fu">=</span> bumpD2 d1
   bumpf1 d1
   rv1 <span class="ot">&lt;-</span> readIORef (f1 d1)
   rv2 <span class="ot">&lt;-</span> readIORef (f1 d2)
   print (rv1 <span class="fu">==</span> rv2)</code></pre>
<p>If <code>D</code> becomes mutable after the <code>UNPACK</code>, this code doesn’t work anymore, because we lose sharing after the functional update in line <code>bumpD2 d1</code>.</p>
<p>See also <a href="https://ghc.haskell.org/trac/ghc/ticket/7662#comment:3">this discussion</a> for how other compilers improve this.</p>]]></summary>
</entry>
<entry>
    <title>Unboxed sums FAQ</title>
    <link href="http://osa1.net/posts/2016-07-22-unboxed-sums-faq.html" />
    <id>http://osa1.net/posts/2016-07-22-unboxed-sums-faq.html</id>
    <published>2016-07-22T00:00:00Z</published>
    <updated>2016-07-22T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>The unboxed sums patch that implements unlifted, unboxed sum types (as described in <a href="https://ghc.haskell.org/trac/ghc/wiki/UnpackedSumTypes">this Wiki page</a>) was merged yesterday, and a <a href="https://www.reddit.com/r/haskell/comments/4txuo7/unboxed_sum_types_with_unpack_support_will_be_in/">/r/haskell discussion</a> emerged shortly after. As the implementor, I tried to answer questions there, but to keep answers more organized I wanted to write a blog post about it.</p>
<p>The reason I’m not writing this to the Wiki page is because this is about current plans and status of the feature. The wiki page may be updated in the future as the feature evolves and/or may be edited by others. This page reflects the current status as of today, future plans, and my own ideas.</p>
<hr />
<h2 id="syntax-is-awful-why">Syntax is awful, why?</h2>
<p>This feature is designed to complement the similar feature for product types (tuples), called <a href="https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#unboxed-tuples">“unboxed tuples”</a>. The syntax is thus chosen to reflect this idea. Instead of commas in the unboxed tuple syntax, we used bars (similar to how bars used in sum type declarations). The syntax looks bad for several reasons:</p>
<ul>
<li><p>Type argument of an alternative have to be a single type. If we want multiple types in an alternative, we have to use an unboxed tuple. For example, unboxed sum version of the type <code>data T = T1 Int | T2 String Bool</code> is <code>(# Int | (# String, Bool #) #)</code>. That’s a lot of parens and hashes.</p></li>
<li><p>Similarly, for nullary alternatives (alternatives/constructors with no arguments) we have to use empty unboxed tuples. So a bool-like type looks like <code>(# (# #) | (# #) #)</code>.</p></li>
<li><p>Data constructors use the same syntax, except we have to put spaces between bars. For example, if you have a type with 10 alternatives, you do something like <code>(# | | | | value | | | | | #)</code>. Space between bars is optional in the type syntax, but not optional in the term syntax. The reason is because otherwise we’d have to steal some existing syntax. For example, <code>(# ||| a #)</code> can be parsed as singleton unboxed tuple of <code>Control.Arrow.|||</code> applied to an argument, or an unboxed sum with 4 alternatives.</p></li>
</ul>
<p>Note that the original Wiki page for unboxed sums included a “design questions” section that discussed some alterantive syntax (see <a href="https://ghc.haskell.org/trac/ghc/wiki/UnpackedSumTypes?version=32">this version</a>). Nobody made any progress to flesh out the ideas, and I updated the Wiki page to reflect the implementation. So it was known that the syntax is not good, but it just wasn’t a major concern.</p>
<p>Answer to the second question is also an answer to this question.</p>
<h2 id="how-is-this-supposed-to-be-used-by-users">How is this supposed to be used by users?</h2>
<p>We’re not expecting users to use this type extensively. It’ll mostly be used by the compiler, for optimizations. In fact, we could have skipped the front-end syntax entirely, and it’d be OK for the most part. If you haven’t used unboxed tuples before, you probably won’t be using unboxed sums.</p>
<p>The only place you may want to use this syntax is when you’re writing a high-performance library or program, and you have a sum type that’s used strictly and can take advantage of removing a level of indirection.</p>
<h2 id="how-is-this-used-by-the-compiler">How is this used by the compiler?</h2>
<p>A detailed answer would take too long, but here’s a summary:</p>
<ul>
<li><p><a href="research.microsoft.com/en-us/um/people/simonpj/Papers/cpr/cpr.ps.gz">Constructed product analysis</a> can now be used for returning sums efficiently. Note that this feature was left as “future work” in the paper (which is from 2004. See section 3.2). The high-level idea is that if a function returns a value that <em>it constructs</em>, then instead of boxing the components of the value and returning a boxed object, it can just return the components instead. In the case where the function result is directly scrutinized (i.e. case expressions), this usually reduces allocations. In other cases, it moves the allocation from the callee to the call site, which in turn leads to stack allocation is some cases (when the object doesn’t escape from the scope).</p>
<p>For product types, unboxed tuples are used for returning the value without heap allocation. For sum types, we use unboxed sums.</p></li>
<li><p>Result of strictness (or “demand”) analysis can now be used to pass sums efficiently. As a result worker/wrapper transformations can now be done for functions that take sum arguments. See <a href="https://ghc.haskell.org/trac/ghc/wiki/Commentary/Compiler/Demand">this Wiki page for demand analysis</a> and <a href="http://research.microsoft.com/en-us/um/people/simonpj/papers/usage-types/cardinality-popl14.pdf">this 2014 paper</a>.</p></li>
<li><p><code>{-# UNPACK #-}</code> pragmas now work on sum types, using unboxed sums under the hood.</p></li>
</ul>
<p>Note that none of these need a concrete syntax for unboxed sums.</p>
<hr />
<p>Hopefully this clarifies some questions and concerns, especially about the syntax. We have plenty of time until the first RC for 8.2 (<a href="https://ghc.haskell.org/trac/ghc/wiki/Status/GHC-8.2.1">mid-February 2017</a>), so it’s certainly possible to improve the syntax, and I’ll be working on that part once I’m done with the optimizations.</p>]]></summary>
</entry>
<entry>
    <title>On matching bang patterns</title>
    <link href="http://osa1.net/posts/2016-06-27-matching-bang-pattern.html" />
    <id>http://osa1.net/posts/2016-06-27-matching-bang-pattern.html</id>
    <published>2016-06-27T00:00:00Z</published>
    <updated>2016-06-27T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>I thought a bang pattern is all about <code>seq</code>. That may actually be true, but when that <code>seq</code> is happening may not be obvious. Even after ~5 years of Haskell I was initially very confused by this, and in fact at first I thought it was a bug:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">data</span> <span class="dt">T</span> <span class="fu">=</span> <span class="dt">A</span> <span class="fu">|</span> <span class="dt">B</span> <span class="fu">|</span> <span class="dt">C</span>

<span class="ot">fn5 ::</span> <span class="dt">Int</span> <span class="ot">-&gt;</span> [<span class="dt">T</span>] <span class="ot">-&gt;</span> <span class="dt">Int</span>
fn5  i []       <span class="fu">=</span> i
fn5  i (<span class="dt">A</span> <span class="fu">:</span> ts) <span class="fu">=</span> fn5 (i <span class="fu">+</span> <span class="dv">1</span>) ts
fn5 <span class="fu">!</span>i (<span class="dt">B</span> <span class="fu">:</span> ts) <span class="fu">=</span> fn5 (i <span class="fu">+</span> <span class="dv">2</span>) ts
fn5  i (<span class="dt">C</span> <span class="fu">:</span> ts) <span class="fu">=</span> fn5 <span class="dv">0</span> ts</code></pre>
<p>The question is, given these definitions, what does this evaluate to, and why:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell">fn5 undefined [<span class="dt">C</span>]</code></pre>
<p>This question is basically a <code>BangPatterns</code> question. The key point is that a bang pattern <em>first evaluates the value</em> to match, then looks at the pattern. This is from GHC 8.0.1 user manual, <a href="https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#bang-patterns-informal">section 9.28.1</a>:</p>
<blockquote>
<p>Matching an expression e against a pattern !p is done by first evaluating e (to WHNF) and then matching the result against p.</p>
</blockquote>
<p>My initial thought was that this example would not crash because the pattern <code>i</code> always matches, and since second argument is only matched by last case of this definition, which doesn’t evaluate <code>i</code>, <code>i</code> would not get evaluated.</p>
<p>Or in other words, I thought all this bang pattern does is to add a <code>seq</code>, <em>to the RHS</em>:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell">fn5 i (<span class="dt">B</span> <span class="fu">:</span> ts) <span class="fu">=</span> i <span class="ot">`seq`</span> fn5 (i <span class="fu">+</span> <span class="dv">2</span>) ts</code></pre>
<p>which is not what it really does!</p>
<hr />
<p>Before bang patterns, I think this pattern was considered as the standard way of forcing a function argument (mostly used for accumulator arguments):</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell">f acc _ <span class="fu">|</span> acc <span class="ot">`seq`</span> <span class="dt">False</span> <span class="fu">=</span> undefined
f acc arg <span class="fu">=</span> <span class="fu">...</span></code></pre>
<p>The guard in first equation always fails, but it forces the <code>acc</code> by the time it fails. While this looks bad<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>, it compiles to nice code after a case-of-case transformation, and it evaluates <code>acc</code> as first thing to do whenever it’s applied to two arguments.</p>
<p>Now, with <code>BangPatterns</code>, we get to do this:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell">f <span class="fu">!</span>acc arg <span class="fu">=</span> <span class="fu">...</span></code></pre>
<p>Which is good when we have one equation only, but when we have multiple equations like in <code>fn5</code> above, we need add bang patterns to every equation, or we risk having bugs<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a>.</p>
<p>So in short, we don’t have a good solution for saying a function is strict on some arguments, without risking bugs (by trying to add minimum number of bangs) or adding a lot of them.</p>
<hr />
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>I don’t like seeing <code>undefined</code>s like this, because I reserve them for code that’s not yet implemented but needs to be implemented. Using <code>undefined</code> as a proxy is also not OK these days, as we have <a href="https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/glasgow_exts.html#visible-type-application">visible type applications in GHC 8.0.1</a> and <a href="http://hackage.haskell.org/package/base-4.9.0.0/docs/Data-Proxy.html"><code>Proxy</code> in base</a> since <code>base-4.7</code>.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>I don’t mean semantic bugs, rather, unexpected memory usage etc. caused by not forcing thunks on time.<a href="#fnref2">↩</a></p></li>
</ol>
</div>]]></summary>
</entry>
<entry>
    <title>How I solved the Synacor Challenge</title>
    <link href="http://osa1.net/posts/2016-06-19-solving-synacor-challenge.html" />
    <id>http://osa1.net/posts/2016-06-19-solving-synacor-challenge.html</id>
    <published>2016-06-19T00:00:00Z</published>
    <updated>2016-06-19T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>It took 4 attempts at various coffee shops in Cambridge/UK (where I’m spending this summer – more on this later) but I finally solved the <a href="https://challenge.synacor.com/">Synacor Challenge</a>. Here’s how I did it.</p>
<p>WARNING: This post spoils everything! Do not read further if you’re interested in solving it yourself. It’s a lot of fun, so I highly recommend that.</p>
<h1 id="first-attempt">First attempt</h1>
<p>Initial VM implementation took about an hour. It worked well until I’ve been eaten by a damn grue. At that point I realized that I need a save/load system.</p>
<p>The game looks like a classic text-based adventure where you just interact by typing commands. So I thought saving the commands should be enough for “save game”. To load, VM would just load the input from the save file to its input buffer (which is used when <code>in</code> instruction is executed). Indeed it works nicely.</p>
<p>The game is very easy until you make it to the “office”. IIRC, you collect 6 out of 8 codes until that point. At the office you learn about the teleporter, which is the first real challenge in the game…</p>
<h1 id="second-attempt">Second attempt</h1>
<p>At this point I implemented about 20 debugger commands, for things like stepping, breaking at an instruction, breaking at an address, breaking on a specific register read, setting the instruction pointer etc. Just by using these commands I was able to teleport to the right place. However, the code I found there did not really work. It turns out the code is generated dynamically, based on value in R8. Which is kind of expected, otherwise we could find all the codes just by looking at strings in the binary.</p>
<p>So at this point I realize I need a disassembler…</p>
<h1 id="third-attempt">Third attempt</h1>
<p>The disassembler was trivial to implement. I implement it as a part of VM because 1) in theory the program can generate code in runtime (although I don’t think this is the case in practice) 2) code and data is in the same address space and instruction boundaries are not clearly known. I guess I could do something like: Start with address 0, at each jump disassemble the jump target etc. but I’m not sure if that works as some jump targets are generated dynamically.</p>
<p>Anyway, the debugger has a disassembler now, and I start disassembling functions.</p>
<p>When I step instruction by instruction after using the teleporter, I see that it’s checking R8, if it’s not 0, then calling a function which is the “confimation process” that’s supposed to take 1 billion years. The function has very complex control flow so I try to avoid actually debugging it.</p>
<p>I look at the code that this function returns. It’s checking if R1 (which has the return value) is 6. So I think, why not just set R1 6 and return the function? Indeed, it works, but not really how I expected. I already knew that the code I’m searching is generated dynamically, but it’s actually generated using R8, and only when R1 is 6. So as it turns out, I need to guess a value of R8 that makes the validation function return 6. Just making the function return 6 doesn’t really work.</p>
<p>However, I said “it kinda worked”. Because the teleporter actually teleports me to the right place. It’s just that the generated code is not valid.</p>
<h1 id="fourth-attempt">Fourth attempt</h1>
<p>Before disassembling the verification function, I decide to solve the rest of the challenge. I realize that we’re now in a maze, 4x4. Each tile in the maze has either a number, or an operation (<code>+</code>, <code>-</code>, <code>*</code>). There’s an orb in the south-west corner, and there’s a door in the north-east corner. “30” is written on the door, and “22” is printed on the Orb. It’s easy to see what’s going on. Two things to realize that every time we return to the first tile things get reset. Also, the goal tile can be visited once (Orb disappears on visit).</p>
<p>I implement a program that does breadth-first search on this state space, see <code>maze.rs</code>. It then prints directions to move. When we follow those directions we find the final code and the challenge is completed.</p>
<p>However, since I by-passed the previous challenge, I need to solve that now.</p>
<p>This is the most fun part, it involves lots of debugging, and some programming. Here’s the disassembly of the verification function:</p>
<pre><code>                                         -- fn(reg0 = 4, reg1 = 1, reg7 = our value) {
[6027] Jt [Reg(0), Num(6035)]            --   if reg0 == 0 {
[6030] Add [Reg(0), Reg(1), Num(1)]      --     reg0 = reg1 + 1;
[6034] Ret []                            --     return; }
[6035] Jt [Reg(1), Num(6048)]            --   if reg1 == 0 {
[6038] Add [Reg(0), Reg(0), Num(32767)]  --     reg0 -= 1;
[6042] Set [Reg(1), Reg(7)]              --     reg1 = reg7;
[6045] Call [Num(6027)] ------ loop      --     fn();
[6047] Ret []                            --     return; }
[6048] Push [Reg(0)]                     --   push(reg0);
[6050] Add [Reg(1), Reg(1), Num(32767)]  --   reg1 -= 1;
[6054] Call [Num(6027)] ------ loop      --   fn();
[6056] Set [Reg(1), Reg(0)]              --   reg1 = reg0;
[6059] Pop [Reg(0)]                      --   reg0 = pop();
[6061] Add [Reg(0), Reg(0), Num(32767)]  --   reg0 -= 1;
[6065] Call [Num(6027)] ------ loop      --   fn();
[6067] Ret [] -- end of basic block 6027 --   return;
                                         -- }</code></pre>
<p>I added next to each instruction how they would look like in a C-like language. It’s hard to understand what’s going on. So to experiment with it I implement it in Rust. Since registers are shared in each call, I use shared state in my initial implementation:</p>
<pre class="sourceCode rust"><code class="sourceCode rust"><span class="ot">#[</span>inline<span class="ot">]</span>
<span class="kw">fn</span> add(i1 : <span class="kw">u16</span>, i2 : <span class="kw">u16</span>) -&gt; <span class="kw">u16</span> {
    (i1 + i2) % <span class="dv">32768</span>
}

<span class="ot">#[</span>derive<span class="ot">(</span>Debug<span class="ot">)]</span>
<span class="kw">struct</span> FnState {
    reg0  : <span class="kw">u16</span>,
    reg1  : <span class="kw">u16</span>,
    reg7  : <span class="kw">u16</span>,
    stack : Vec&lt;<span class="kw">u16</span>&gt;,
}

<span class="kw">impl</span> FnState {
    <span class="kw">fn</span> init(reg0 : <span class="kw">u16</span>, reg1 : <span class="kw">u16</span>, reg7 : <span class="kw">u16</span>) -&gt; FnState {
        FnState {
            reg0: reg0,
            reg1: reg1,
            reg7: reg7,
            stack: Vec::new(),
        }
    }

    <span class="kw">fn</span> f(&amp;<span class="kw">mut</span> <span class="kw">self</span>) {
        <span class="kw">if</span> <span class="kw">self</span>.reg0 == <span class="dv">0</span> {
            <span class="kw">self</span>.reg0 = add(<span class="kw">self</span>.reg1, <span class="dv">1</span>);
            <span class="kw">return</span>;
        }
        <span class="kw">if</span> <span class="kw">self</span>.reg1 == <span class="dv">0</span> {
            <span class="co">// self.reg0 = add(self.reg0, 32767);</span>
            <span class="kw">self</span>.reg0 -= <span class="dv">1</span>;
            <span class="kw">self</span>.reg1 = <span class="kw">self</span>.reg7;
            <span class="kw">self</span>.f();
            <span class="kw">return</span>;
        }
        <span class="kw">self</span>.stack.push(<span class="kw">self</span>.reg0);
        <span class="co">// self.reg1 = add(self.reg1, 32767);</span>
        <span class="kw">self</span>.reg1 -= <span class="dv">1</span>;
        <span class="kw">self</span>.f();
        <span class="kw">self</span>.reg1 = <span class="kw">self</span>.reg0;
        <span class="kw">self</span>.reg0 = <span class="kw">self</span>.stack.pop().unwrap();
        <span class="co">// self.reg0 = add(self.reg0, 32767);</span>
        <span class="kw">self</span>.reg0 -= <span class="dv">1</span>;
        <span class="kw">self</span>.f();
        <span class="kw">return</span>;
    }
}</code></pre>
<p>Now, this function grows really fast. Even for very small inputs it takes minutes to compute. I try to think some well-known functions that grow very fast. Ackermann comes to my mind and I check the Wiki page. Indeed, this looks quite similar, but the third argument makes it different than Ackermann. In any case, it doesn’t really matter for the solution.</p>
<p>So the problem is coming up with a <code>reg7</code> in this code so that <code>f(4, 1, reg7)</code> returns <code>6</code>. For that I need to implement a search but this is basically impossible with this slow function. I start simplifying the function a little bit. My first attempt:</p>
<pre class="sourceCode rust"><code class="sourceCode rust"><span class="kw">impl</span> FnState {
    <span class="kw">fn</span> f(&amp;<span class="kw">mut</span> <span class="kw">self</span>) {
        <span class="co">// When reg0 hits zero, restart it from reg1 + 1</span>
        <span class="kw">if</span> <span class="kw">self</span>.reg0 == <span class="dv">0</span> {
            <span class="kw">self</span>.reg0 = add(<span class="kw">self</span>.reg1, <span class="dv">1</span>);
            <span class="kw">return</span>;
        }

        <span class="co">// When reg1 hits zero, decrement reg0, restart reg1 from reg7</span>
        <span class="kw">if</span> <span class="kw">self</span>.reg1 == <span class="dv">0</span> {
            <span class="kw">self</span>.reg0 -= <span class="dv">1</span>;
            <span class="kw">self</span>.reg1 = <span class="kw">self</span>.reg7;
            <span class="kw">self</span>.f();
            <span class="kw">return</span>;
        }

        <span class="kw">let</span> save_reg0 = <span class="kw">self</span>.reg0;

        <span class="kw">self</span>.reg1 -= <span class="dv">1</span>;
        <span class="kw">self</span>.f();
        <span class="kw">self</span>.reg1 = <span class="kw">self</span>.reg0;

        <span class="kw">self</span>.reg0 = save_reg0;
        <span class="kw">self</span>.reg0 -= <span class="dv">1</span>;

        <span class="kw">self</span>.f();
        <span class="kw">return</span>;
    }
}</code></pre>
<p>This version doesn’t use an explicit stack, instead uses a temporary in the call frame. This works because in the original version each push corresponds to a pop done in the same call frame.</p>
<p>It’s still too complicated. I keep simplifying.</p>
<pre class="sourceCode rust"><code class="sourceCode rust"><span class="kw">fn</span> f(<span class="kw">mut</span> reg0 : <span class="kw">u16</span>, <span class="kw">mut</span> reg1 : <span class="kw">u16</span>, <span class="kw">mut</span> reg7 : <span class="kw">u16</span>) -&gt; (<span class="kw">u16</span>, <span class="kw">u16</span>) {
    <span class="kw">if</span> reg0 == <span class="dv">0</span> {
        reg0 = add(reg1, <span class="dv">1</span>);
        <span class="kw">return</span> (reg0, reg1);
    }

    <span class="kw">if</span> reg1 == <span class="dv">0</span> {
        reg0 -= <span class="dv">1</span>;
        reg1 = reg7;
        <span class="kw">return</span> f(reg0, reg1, reg7);
    }

    <span class="kw">let</span> save_reg0 = reg0;
    reg1 -= <span class="dv">1</span>;

    <span class="kw">let</span> (reg0_, reg1_) = f(reg0, reg1, reg7);
    reg0 = reg0_;
    reg1 = reg1_;

    reg1 = reg0;

    reg0 = save_reg0;
    reg0 -= <span class="dv">1</span>;

    <span class="kw">return</span> f(reg0, reg1, reg7);
}</code></pre>
<p>This version doesn’t have any shared mutable state. At this point I realize that it may be possible to remove internal mutable state too:</p>
<pre class="sourceCode rust"><code class="sourceCode rust"><span class="kw">fn</span> f(reg0 : <span class="kw">u16</span>, reg1 : <span class="kw">u16</span>, reg7 : <span class="kw">u16</span>) -&gt; (<span class="kw">u16</span>, <span class="kw">u16</span>) {
    <span class="kw">if</span> reg0 == <span class="dv">0</span> {
        <span class="kw">return</span> (add(reg1, <span class="dv">1</span>), reg1);
    }

    <span class="kw">if</span> reg1 == <span class="dv">0</span> {
        <span class="kw">return</span> f(reg0 - <span class="dv">1</span>, reg7, reg7);
    }

    <span class="kw">let</span> (reg1, _) = f(reg0, reg1 - <span class="dv">1</span>, reg7);

    <span class="kw">return</span> f(reg0 - <span class="dv">1</span>, reg1, reg7);
}</code></pre>
<p>Now, this is a function I can read and understand. One advantage of this version is that this could be easily memoized:</p>
<pre class="sourceCode rust"><code class="sourceCode rust"><span class="kw">fn</span> f_memo(reg0 : <span class="kw">u16</span>, reg1 : <span class="kw">u16</span>, reg7 : <span class="kw">u16</span>, memo : &amp;<span class="kw">mut</span> HashMap&lt;(<span class="kw">u16</span>, <span class="kw">u16</span>), (<span class="kw">u16</span>, <span class="kw">u16</span>)&gt;) -&gt; (<span class="kw">u16</span>, <span class="kw">u16</span>) {
    <span class="kw">if</span> <span class="kw">let</span> <span class="kw">Some</span>(ret) = memo.get(&amp;(reg0, reg1)) {
        <span class="kw">return</span> *ret;
    }

    <span class="kw">if</span> reg0 == <span class="dv">0</span> {
        <span class="kw">let</span> ret = (add(reg1, <span class="dv">1</span>), reg1);
        memo.insert((reg0, reg1), ret);
        <span class="kw">return</span> ret;
    }

    <span class="kw">if</span> reg1 == <span class="dv">0</span> {
        <span class="kw">let</span> ret = f_memo(reg0 - <span class="dv">1</span>, reg7, reg7, memo);
        memo.insert((reg0, reg1), ret);
        <span class="kw">return</span> ret;
    }

    <span class="co">// careful there</span>
    <span class="kw">let</span> (reg1_new, _) = f_memo(reg0, reg1 - <span class="dv">1</span>, reg7, memo);

    <span class="kw">let</span> ret = f_memo(reg0 - <span class="dv">1</span>, reg1_new, reg7, memo);
    memo.insert((reg0, reg1), ret);
    <span class="kw">return</span> ret;
}</code></pre>
<p>This version is super fast when compared to the original version. I feel like I can just search the whole space in a few hours. I start the search and as it searches through the search space I start wondering about how to further improve it. I think, why not split search space into pieces and search in paralell?</p>
<pre class="sourceCode rust"><code class="sourceCode rust"><span class="kw">fn</span> search(start_range : <span class="kw">u16</span>, end_range : <span class="kw">u16</span>) -&gt; <span class="kw">Option</span>&lt;<span class="kw">u16</span>&gt; {
    <span class="kw">for</span> i in start_range .. end_range {
        <span class="co">// println!(&quot;i = {}&quot;, i);</span>
        <span class="kw">let</span> <span class="kw">mut</span> tbl = HashMap::new();
        <span class="kw">let</span> ret = f_memo(<span class="dv">4</span>, <span class="dv">1</span>, i, &amp;<span class="kw">mut</span> tbl);
        <span class="kw">if</span> ret.<span class="dv">0</span> == <span class="dv">6</span> {
            <span class="ot">println!</span>(<span class="st">&quot;Found an answer: {:?} {}&quot;</span>, ret, i);
            <span class="kw">return</span> <span class="kw">Some</span>(i);
        }
    }

    <span class="kw">None</span>
}

<span class="kw">fn</span> search_in_parallel(n : <span class="kw">i32</span>) {
    <span class="kw">let</span> increment = <span class="kw">u16</span>::MAX / (n <span class="kw">as</span> <span class="kw">u16</span>);
    <span class="kw">let</span> <span class="kw">mut</span> threads = Vec::with_capacity(n <span class="kw">as</span> usize);

    <span class="kw">for</span> i in <span class="dv">0</span> .. n {
        <span class="kw">let</span> range_start = increment * (i <span class="kw">as</span> <span class="kw">u16</span>);
        <span class="kw">let</span> range_end   = increment * ((i <span class="kw">as</span> <span class="kw">u16</span>) + <span class="dv">1</span>) + <span class="dv">1</span>;
        threads.push(thread::Builder::new().stack_size(<span class="dv">1000000000</span>).spawn(move || {
            search(range_start, range_end)
        }).unwrap());
    }

    <span class="kw">for</span> thread in threads {
        thread.join();
    }
}

<span class="kw">fn</span> main() {
    search_in_parallel(<span class="dv">8</span>);
}</code></pre>
<p>This parallel search takes a couple of seconds until it prints:</p>
<pre><code>Found an answer: (6, 5) 25734</code></pre>
<p>So <code>25734</code> returns 6! This is the R8 value we were looking for.</p>
<p>I modify my VM to return when this function is called (I know it lives in address <code>6027</code> so I just check instruction pointer in the main loop) and drop to debugger prompt. In the debugger, I set R1 (return value register) 6, and set R8 25734, and continue running the program.</p>
<p>It works perfectly, with a working code printed to the screen!</p>
<h1 id="code-and-conclusion">Code and conclusion</h1>
<p>Overall I enjoyed this a lot. My favorite part was definitely debugging the verification function. I don’t really enjoy text adventures but that’s really a very small part of the challenge, so it wasn’t a big deal.</p>
<p><a href="https://github.com/osa1/synacor-challenge">My code is on Github</a>. I didn’t organize the code at all, so you can see my inline notes, logs, and commented out code with their improved/changed versions, and have a feeling of how I developed my solutions. In the repo you’ll also see the original binary and challenge spec. I pushed those in case the original challenge page goes down in the future.</p>
<p>The Rust compiler I used was <code>rustc 1.11.0-nightly (0554abac6 2016-06-10)</code>.</p>
<p>If you know similar challenges let me know in the comment section below. One challenge that looks similar is ICFP’06 programming contest <a href="http://www.boundvariable.org/">“The Cult of the Bound Variable”</a>, which I always wanted to solve but never really got a chance. Maybe I should try it next.</p>]]></summary>
</entry>
<entry>
    <title>Rust borrow checker woes</title>
    <link href="http://osa1.net/posts/2016-03-28-rust-brwchk-woes.html" />
    <id>http://osa1.net/posts/2016-03-28-rust-brwchk-woes.html</id>
    <published>2016-03-28T00:00:00Z</published>
    <updated>2016-03-28T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>I’ve been doing some Rust hacking in my free time, and unfortunately while it’s way, way better than how it was when I first tried it (around 0.4 or 0.6 IIRC), it still has some problems that encourage redundant runtime operations or bad programming practices. In this post I’ll give three examples that are all caused by dumb borrow checker. As you’ll see, in all cases the cost is either bad programming practices or runtime costs (which is really ironic, given that one of the design goals of Rust is performance).</p>
<h1 id="no-local-reasoning-about-borrowing-rules-of-constructors">1. No local reasoning about borrowing rules of constructors</h1>
<p>It’s types that borrow, not values, and that makes sense. If you have an <code>Option&lt;&amp;'a T&gt;</code> where <code>'a</code> is coming from some other variable <code>x</code>, 1) <code>x</code> needs to live longer than this <code>Option</code> value 2) you can’t borrow <code>x</code> mutably while keeping the <code>Option</code> in scope (or the other way around, you can’t borrow <code>Option</code> mutably while keeping <code>x</code> in scope).</p>
<p>This makes sense because in compile time, given an <code>Option&lt;&amp;'a T&gt;</code>, you can’t make any assumptions on <code>Option</code>’s actual value. Since <code>Some</code> constructor of the <code>Option</code> type will borrow the <code>T</code> here, you just have to assume that values of this type always borrow <code>T</code> (and that’s why we have <code>&amp;'a</code> in the type).</p>
<p>The problem is that it’s sometimes possible to do some local reasoning, and not doing that is too restrictive. Suppose you have this:</p>
<pre class="sourceCode rust"><code class="sourceCode rust"><span class="kw">struct</span> ListOfThings {
    list : Vec&lt;Thing&gt;,
}

<span class="kw">struct</span> Thing {
    name : String,
    <span class="co">// other fields here -- this is expensive to copy!</span>
}

<span class="kw">impl</span> Thing {
    <span class="kw">fn</span> do_something(&amp;<span class="kw">mut</span> <span class="kw">self</span>) {}
}

<span class="kw">impl</span> ListOfThings {
    <span class="kw">fn</span> do_something(&amp;<span class="kw">mut</span> <span class="kw">self</span>, name : &amp;<span class="kw">str</span>) {
        <span class="kw">match</span> <span class="kw">self</span>.find_thing_mut(name) {
            <span class="kw">None</span> =&gt; {
                <span class="kw">self</span>.init_thing(name.to_owned());
                <span class="kw">self</span>.do_something(name)
            },
            <span class="kw">Some</span>(t) =&gt; {
                t.do_something()
            },
        }
    }

    <span class="kw">fn</span> find_thing_mut&lt;<span class="ot">&#39;a</span>&gt;(&amp;<span class="ot">&#39;a</span> <span class="kw">mut</span> <span class="kw">self</span>, name : &amp;<span class="kw">str</span>) -&gt; <span class="kw">Option</span>&lt;&amp;<span class="ot">&#39;a</span> <span class="kw">mut</span> Thing&gt; {
        <span class="kw">self</span>.list.iter_mut().find(|t| t.name.as_str() == name)
    }

    <span class="kw">fn</span> init_thing(&amp;<span class="kw">mut</span> <span class="kw">self</span>, name : String) {
        <span class="kw">self</span>.list.push(Thing { name: name })
    }
}</code></pre>
<p>The important part is this expression:</p>
<pre class="sourceCode rust"><code class="sourceCode rust"><span class="kw">match</span> <span class="kw">self</span>.find_thing_mut(name) {
    <span class="kw">None</span> =&gt; {
        <span class="kw">self</span>.init_thing(name.to_owned());
        <span class="kw">self</span>.do_something(name)
    },
    <span class="kw">Some</span>(t) =&gt; {
        t.do_something()
    },
}</code></pre>
<p>The error we get is:</p>
<pre><code>&lt;anon&gt;:18:17: 18:21 error: cannot borrow `*self` as mutable more than once at a time [E0499]
&lt;anon&gt;:18                 self.init_thing(name.to_owned());
                          ^~~~
&lt;anon&gt;:18:17: 18:21 help: see the detailed explanation for E0499
&lt;anon&gt;:16:15: 16:19 note: previous borrow of `*self` occurs here; the mutable
                          borrow prevents subsequent moves, borrows, or
                          modification of `*self` until the borrow ends
&lt;anon&gt;:16         match self.find_thing_mut(name) {
                        ^~~~
&lt;anon&gt;:24:10: 24:10 note: previous borrow ends here
&lt;anon&gt;:16         match self.find_thing_mut(name) {
...
&lt;anon&gt;:24         }
                  ^
&lt;anon&gt;:19:17: 19:21 error: cannot borrow `*self` as mutable more than once at a time [E0499]
&lt;anon&gt;:19                 self.do_something(name)
                          ^~~~
&lt;anon&gt;:19:17: 19:21 help: see the detailed explanation for E0499
&lt;anon&gt;:16:15: 16:19 note: previous borrow of `*self` occurs here; the mutable
                          borrow prevents subsequent moves, borrows, or
                          modification of `*self` until the borrow ends
&lt;anon&gt;:16         match self.find_thing_mut(name) {
                        ^~~~
&lt;anon&gt;:24:10: 24:10 note: previous borrow ends here
&lt;anon&gt;:16         match self.find_thing_mut(name) {
...
&lt;anon&gt;:24         }
                  ^</code></pre>
<p><code>find_thing_mut()</code> really needs to return a ref, because <code>Thing</code> is expensive to copy. The problem is since <code>None</code> has type <code>Option&lt;&amp;'a mut Thing&gt;</code> where <code>a</code> is the lifetime of <code>self</code>, we can’t call a <code>&amp;mut self</code> when that <code>None</code> is in scope. This is annoying and could be improved by doing some local reasoning. In our case, since we know that <code>None</code> can’t borrow anything (it doesn’t have any references), we could refine our information about currently borrwed values, and let this compile.</p>
<p>There are a couple of solutions. One half-solution is to use something like standard <a href="http://doc.rust-lang.org/nightly/std/collections/struct.HashMap.html#method.entry"><code>HashMap</code>’s <code>entry()</code></a>. Imagine doing something like:</p>
<pre class="sourceCode rust"><code class="sourceCode rust"><span class="kw">use</span> std::collections::hash_map::HashMap;

<span class="kw">struct</span> ListOfThings {
    list : HashMap&lt;String, Thing&gt;,
}

<span class="kw">struct</span> Thing {
    field1 : <span class="kw">i32</span>,
    <span class="co">// other fields here -- this is expensive to copy!</span>
}

<span class="kw">impl</span> Thing {
    <span class="kw">fn</span> do_something(&amp;<span class="kw">mut</span> <span class="kw">self</span>) {}
}

<span class="kw">impl</span> ListOfThings {
    <span class="kw">fn</span> do_something(&amp;<span class="kw">mut</span> <span class="kw">self</span>, name : &amp;<span class="kw">str</span>) {
        <span class="kw">match</span> <span class="kw">self</span>.list.get_mut(name) {
            <span class="kw">None</span> =&gt; {
                <span class="kw">self</span>.list.insert(name.to_owned(), Thing { field1: <span class="dv">123</span> });
            },
            <span class="kw">Some</span>(t) =&gt; {
                t.do_something();
            }
        }
    }
}</code></pre>
<p>The error we get:</p>
<pre><code>&lt;anon&gt;:20:17: 20:26 error: cannot borrow `self.list` as mutable more than once at a time [E0499]
&lt;anon&gt;:20                 self.list.insert(name.to_owned(), Thing { field1: 123 });
                          ^~~~~~~~~
&lt;anon&gt;:20:17: 20:26 help: see the detailed explanation for E0499
&lt;anon&gt;:18:15: 18:24 note: previous borrow of `self.list` occurs here; the
                          mutable borrow prevents subsequent moves, borrows, or
                          modification of `self.list` until the borrow ends
&lt;anon&gt;:18         match self.list.get_mut(name) {
                        ^~~~~~~~~
&lt;anon&gt;:25:10: 25:10 note: previous borrow ends here
&lt;anon&gt;:18         match self.list.get_mut(name) {
...
&lt;anon&gt;:25         }
                  ^</code></pre>
<p>This is exactly the same error we got before. The solution is to use <a href="http://doc.rust-lang.org/nightly/std/collections/struct.HashMap.html#method.entry">the <code>Entry</code> API</a>:</p>
<pre class="sourceCode rust"><code class="sourceCode rust"><span class="kw">use</span> std::collections::hash_map::{HashMap, Entry};

<span class="kw">struct</span> ListOfThings {
    list : HashMap&lt;String, Thing&gt;,
}

<span class="kw">struct</span> Thing {
    field1 : <span class="kw">i32</span>,
    <span class="co">// other fields here -- this is expensive to copy!</span>
}

<span class="kw">impl</span> Thing {
    <span class="kw">fn</span> do_something(&amp;<span class="kw">mut</span> <span class="kw">self</span>) {}
}

<span class="kw">impl</span> ListOfThings {
    <span class="kw">fn</span> do_something(&amp;<span class="kw">mut</span> <span class="kw">self</span>, name : &amp;<span class="kw">str</span>) {
        <span class="kw">match</span> <span class="kw">self</span>.list.entry(name.to_owned()) {
            Entry::Vacant(ve) =&gt; {
                ve.insert(Thing { field1: <span class="dv">123</span> });
            },
            Entry::Occupied(<span class="kw">mut</span> oe) =&gt; {
                oe.get_mut().do_something();
            }
        }
    }
}</code></pre>
<p>Some things to note here:</p>
<ol style="list-style-type: decimal">
<li><p>We had to give ownership of the key to the lookup function (<code>HashMap::entry()</code>). This potentially means copying a value just to lookup. Ideally we’d only need to do this when inserting to the map. <code>HashMap::get()</code> doesn’t have this problem.</p></li>
<li><p>I said “half-solution” because this doesn’t really make the original code working. See how I removed a line in the first case in my <code>HashMap</code>-based implementation. If I change the first case to this:</p>
<pre class="sourceCode rust"><code class="sourceCode rust">        <span class="kw">match</span> <span class="kw">self</span>.list.entry(name.to_owned()) {
            Entry::Vacant(ve) =&gt; {
                ve.insert(Thing { field1: <span class="dv">123</span> });
                <span class="kw">self</span>.do_something(name)
            },</code></pre>
<p>It’d still fail as <code>Entry</code> keeps a reference to <code>self</code>. Of course you could always do things like:</p>
<pre class="sourceCode rust"><code class="sourceCode rust">        <span class="kw">match</span> <span class="kw">self</span>.list.entry(name.to_owned()) {
            Entry::Vacant(ve) =&gt; {
                <span class="kw">let</span> <span class="kw">mut</span> thing = Thing { field1 : <span class="dv">123</span> };
                thing.do_something();
                ve.insert(thing);
            },</code></pre>
<p>Which works, but that’s quite different from our original program. Note that if we still had a method like <code>init_thing()</code> and has to pass <code>Entry</code> to that, it’d still fail with same error message. So yeah, not quite a solution.</p></li>
</ol>
<p>The solution I use is this:</p>
<pre class="sourceCode rust"><code class="sourceCode rust"><span class="kw">impl</span> ListOfThings {
    <span class="kw">fn</span> do_something(&amp;<span class="kw">mut</span> <span class="kw">self</span>, name : &amp;<span class="kw">str</span>) {
        <span class="kw">let</span> thing : <span class="kw">Option</span>&lt;*<span class="kw">mut</span> Thing&gt; = <span class="kw">self</span>.find_thing_mut(name).map(|t| (t <span class="kw">as</span> *<span class="kw">mut</span> _));
        <span class="kw">match</span> thing {
            <span class="kw">None</span> =&gt; {
                <span class="kw">self</span>.init_thing(name.to_owned());
                <span class="kw">self</span>.do_something(name)
            },
            <span class="kw">Some</span>(t) =&gt; {
                <span class="kw">unsafe</span> { (*t).do_something() }
            },
        }
    }
}</code></pre>
<p>(missing parts are the same as the original code),</p>
<p>I basically work around the borrow checker by using a raw pointer and an <code>unsafe</code> block, and hope that my <code>.map()</code> will be compiled as a no-op.</p>
<h1 id="references-to-self-in-method-values">2. References to self in method values</h1>
<p>A code like this fails if the method is mutable in self: <code>self.f(self.x)</code>. As a running example:</p>
<pre class="sourceCode rust"><code class="sourceCode rust"><span class="kw">struct</span> Widget {
    pos_x : <span class="kw">i32</span>,
    pos_y : <span class="kw">i32</span>,
}

<span class="kw">impl</span> Widget {
    <span class="kw">pub</span> <span class="kw">fn</span> draw(&amp;<span class="kw">mut</span> <span class="kw">self</span>) {
        <span class="kw">self</span>.draw_at(<span class="kw">self</span>.pos_x, <span class="kw">self</span>.pos_y);
    }

    <span class="kw">pub</span> <span class="kw">fn</span> draw_at(&amp;<span class="kw">mut</span> <span class="kw">self</span>, pos_x : <span class="kw">i32</span>, pos_y : <span class="kw">i32</span>) {}
}</code></pre>
<p>These are the errors:</p>
<pre><code>&lt;anon&gt;:8:22: 8:32 error: cannot use `self.pos_x` because it was mutably borrowed [E0503]
&lt;anon&gt;:8         self.draw_at(self.pos_x, self.pos_y);
                              ^~~~~~~~~~
&lt;anon&gt;:8:9: 8:13 note: borrow of `*self` occurs here
&lt;anon&gt;:8         self.draw_at(self.pos_x, self.pos_y);
                 ^~~~
&lt;anon&gt;:8:34: 8:44 error: cannot use `self.pos_y` because it was mutably borrowed [E0503]
&lt;anon&gt;:8         self.draw_at(self.pos_x, self.pos_y);
                                          ^~~~~~~~~~
&lt;anon&gt;:8:9: 8:13 note: borrow of `*self` occurs here
&lt;anon&gt;:8         self.draw_at(self.pos_x, self.pos_y);
                 ^~~~</code></pre>
<p>Basically the method itself (<code>self.draw_at</code>) borrows <code>self</code> mutably, and since arguments are evaluated <em>after</em> the function in a function application, we get this borrow checker error. The solution is simple in this case:</p>
<pre class="sourceCode rust"><code class="sourceCode rust"><span class="kw">let</span> pos_x = <span class="kw">self</span>.pos_x;
<span class="kw">let</span> pos_y = <span class="kw">self</span>.pos_y;
<span class="kw">self</span>.draw_at(pos_x, pos_y);</code></pre>
<h1 id="variables-that-live-across-loops">3. Variables that live across loops</h1>
<p>Suppose you have a loop that internally calls some <code>&amp;mut self</code> methods, and when it returns, it returns something with a reference to <code>&amp;self</code>. Something like:</p>
<pre class="sourceCode rust"><code class="sourceCode rust"><span class="kw">pub</span> <span class="kw">struct</span> TUI {
    field1: <span class="kw">i32</span>,
}

<span class="kw">pub</span> <span class="kw">enum</span> TUIRet&lt;<span class="ot">&#39;a</span>&gt; {
    Abort,
    KeyHandled,
    Input(&amp;<span class="ot">&#39;a</span> <span class="kw">str</span>),
}

<span class="kw">impl</span> TUI {
    <span class="kw">pub</span> <span class="kw">fn</span> idle_loop&lt;<span class="ot">&#39;a</span>&gt;(&amp;<span class="ot">&#39;a</span> <span class="kw">mut</span> <span class="kw">self</span>) -&gt; TUIRet&lt;<span class="ot">&#39;a</span>&gt; {
        <span class="kw">loop</span> {
            <span class="kw">self</span>.draw();

            <span class="kw">match</span> <span class="kw">self</span>.keypressed() {
                ret @ TUIRet::Abort =&gt; { <span class="kw">return</span> ret; },
                ret @ TUIRet::Input(_) =&gt; { <span class="kw">return</span> ret; },
                _ =&gt; {},
            }
        }
    }

    <span class="kw">pub</span> <span class="kw">fn</span> keypressed(&amp;<span class="kw">mut</span> <span class="kw">self</span>) -&gt; TUIRet {
        <span class="ot">panic!</span>()
    }

    <span class="kw">pub</span> <span class="kw">fn</span> draw(&amp;<span class="kw">self</span>) {}
}</code></pre>
<p>Can you see what could go wrong here? Here’s the error:</p>
<pre><code>&lt;anon&gt;:18:13: 18:17 error: cannot borrow `*self` as immutable because it is also borrowed as mutable [E0502]
&lt;anon&gt;:18             self.draw();
                      ^~~~
&lt;anon&gt;:20:19: 20:23 note: previous borrow of `*self` occurs here; the mutable
                          borrow prevents subsequent moves, borrows, or
                          modification of `*self` until the borrow ends
&lt;anon&gt;:20             match self.keypressed() {
                            ^~~~
&lt;anon&gt;:26:6: 26:6 note: previous borrow ends here
&lt;anon&gt;:16     pub fn idle_loop&lt;&#39;a&gt;(&amp;&#39;a mut self) -&gt; TUIRet&lt;&#39;a&gt; {
...
&lt;anon&gt;:26     }
              ^
&lt;anon&gt;:20:19: 20:23 error: cannot borrow `*self` as mutable more than once at a time [E0499]
&lt;anon&gt;:20             match self.keypressed() {
                            ^~~~
&lt;anon&gt;:20:19: 20:23 help: see the detailed explanation for E0499
&lt;anon&gt;:20:19: 20:23 note: previous borrow of `*self` occurs here; the mutable
                          borrow prevents subsequent moves, borrows, or
                          modification of `*self` until the borrow ends
&lt;anon&gt;:20             match self.keypressed() {
                            ^~~~
&lt;anon&gt;:26:6: 26:6 note: previous borrow ends here
&lt;anon&gt;:16     pub fn idle_loop&lt;&#39;a&gt;(&amp;&#39;a mut self) -&gt; TUIRet&lt;&#39;a&gt; {
...
&lt;anon&gt;:26     }
              ^</code></pre>
<p>This is probably the worst of all. The weird part is that this works:</p>
<pre class="sourceCode rust"><code class="sourceCode rust">    <span class="kw">pub</span> <span class="kw">fn</span> idle_loop&lt;<span class="ot">&#39;a</span>&gt;(&amp;<span class="ot">&#39;a</span> <span class="kw">mut</span> <span class="kw">self</span>) -&gt; TUIRet&lt;<span class="ot">&#39;a</span>&gt; {
        <span class="kw">loop</span> {
            <span class="kw">self</span>.draw();
            <span class="kw">return</span> <span class="kw">self</span>.keypressed();
        }
    }</code></pre>
<p>Only solution I could find here was to remove the references to <code>self</code>, by just copying the value to <code>Input</code>. This unfortunately means more redundant copying.</p>]]></summary>
</entry>
<entry>
    <title>On -XStrict</title>
    <link href="http://osa1.net/posts/2015-11-16-XStrict-faq.html" />
    <id>http://osa1.net/posts/2015-11-16-XStrict-faq.html</id>
    <published>2015-11-16T00:00:00Z</published>
    <updated>2015-11-16T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p><code>-XStrict</code> has <a href="https://phabricator.haskell.org/D1142">landed in HEAD</a> a couple of days ago, and judged from the <a href="https://www.reddit.com/r/haskell/comments/3sts2t/strict_haskell_xstrict_has_landed/">upvotes</a> it seems like <a href="https://www.reddit.com/r/haskell">/r/haskell</a> was quite excited about it.</p>
<p>In the thread I tried to answer questions about <code>-XStrict</code>s effects on programs’ semantics. Does it make the language effectively call-by-value? Do I still have bottoms in my values? Do I lose infinite lists(streams)? In this post I’ll try to give a more organized answer, with some answers to the questions asked in the Reddit thread.</p>
<hr />
<p>Let’s think about how to create a thunk in Haskell:</p>
<ul>
<li><p>Create a let-binding. The RHS of let-binding is a thunk until actually use it.</p></li>
<li><p>Create a where-binding. This is just a syntactic sugar for a let-binding, so I won’t consider this as a different case.</p></li>
<li><p>Pass an argument to a function or a data constructor. The argument will only be evaluated when it’s actually “used”.</p></li>
</ul>
<p>Here I deliberately don’t define what I mean by “used”, because it’ll complicate the discussion a lot.</p>
<p>Now, with <code>-XStrict</code>, we have a bang pattern in every binder. This means that:</p>
<ol style="list-style-type: decimal">
<li><p>Let-bindings are now strict. E.g. if we have something like:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">let</span> a <span class="fu">=</span> <span class="fu">...</span> <span class="kw">in</span> <span class="fu">&lt;</span>body<span class="fu">&gt;</span></code></pre>
<p><code>a</code> is now evaluated before <code>&lt;body&gt;</code>, and so we can be sure that it won’t be bottom in <code>&lt;body&gt;</code>. Note however that this isn’t to say that <code>a</code> can’t <em>contain</em> bottoms. Here I’m just saying that <code>a</code> can’t be bottom in <code>&lt;body&gt;</code>.</p></li>
<li><p>Function arguments are now strict. In a function like this:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell">f a b <span class="fu">=</span> <span class="fu">&lt;</span>body<span class="fu">&gt;</span></code></pre>
<p><code>a</code> and <code>b</code> can’t be bottom in <code>&lt;body&gt;</code>.</p></li>
<li><p>Data constructor arguments(fields) are now strict. If we have this:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">data</span> <span class="dt">List</span> a <span class="fu">=</span> <span class="dt">Nil</span> <span class="fu">|</span> <span class="dt">Cons</span> a (<span class="dt">List</span> a)</code></pre>
<p>The two fields of <code>Cons</code> are now strict. Now, this case may look a bit tricky at first. What we really say here is that once a <code>List</code> is constructed, it can’t contain bottoms. We can still do something like <code>undefined :: List Int</code>, but the program immediately fails, instead of running until you try to pattern match on that <code>undefined</code> value like in the lazy case. This follows from the first two rules. Keep reading for more details.</p></li>
</ol>
<p>When all these combined, it means that our programs are evaluated just like how they would be in a call-by-value language. For example, if we have:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">let</span> a <span class="fu">=</span> <span class="fu">...</span>  <span class="kw">in</span> <span class="fu">&lt;</span>body<span class="fu">&gt;</span></code></pre>
<p>We can be sure that <code>a</code> won’t be bottom in <code>&lt;body&gt;</code>, and it can’t contain bottoms too! This follows from all the rules above. The first rule only says that <code>a</code> can’t be bottom itself. It doesn’t say anything about the fields(subexpressions) of <code>a</code>. Third rule says that it can’t contain bottom fields.</p>
<p>When we make all the binders and fields strict, including all the modules in all the dependencies(<code>base</code> etc.), we guarantee that we our programs evaluate like in a call-by-value language.</p>
<p>Now, call-by-value, call-by-name(and it’s efficient implementation call-by-need) etc. are really about how to evaluate a function application. In our case since we have a strictness annotation in all the function arguments, arguments will be evaluated before being passed to the function. Which really means evaluating the function application in call-by-value semantics.</p>
<p>Below are some questions and answers that I answered in the Reddit thread and in some follow-up threads.</p>
<hr />
<h2 id="what-about-standard-list-tuple-etc.-types">What about standard list, tuple etc. types?</h2>
<p>Unless we compile modules that define those using <code>-XStrict</code>, they’ll stay non-strict. For the standard types, we need <code>base</code> compiled with <code>-XStrict</code>. In practice this will probably never happen. But I think we can have another base, say, <code>base-strict</code>, which is the same <code>base</code>, except compiled with <code>-XStrict</code>. In this case depending on which one we’re using our lists, tuples etc. become strict or lazy.</p>
<h2 id="what-about-monadic-code">What about monadic code?</h2>
<p>Monadic code is really not special in any sense. When talking about the semantics we should see through the syntactic sugar. Say we have this:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">do</span> a <span class="ot">&lt;-</span> m1
   b <span class="ot">&lt;-</span> m2 a
   return (a <span class="fu">+</span> b)</code></pre>
<p>This is really just a syntactic sugar for:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell">m1 <span class="fu">&gt;&gt;=</span> (\a <span class="ot">-&gt;</span> m2 a <span class="fu">&gt;&gt;=</span> (\b <span class="ot">-&gt;</span> return (a <span class="fu">+</span> b)))</code></pre>
<p>With <code>-XStrict</code>, function arguments <code>a</code> and <code>b</code> and all the arguments of <code>&gt;&gt;=</code> and <code>return</code> will be strict. This makes the whole code strict. In this code this means that <code>m1</code> will be evaluated before <code>m2 a</code> is evaluated, and <code>return</code>s return value will be strict etc.</p>
<h2 id="what-about-list-comprehensions-and-infinite-listsstreams">What about list comprehensions and infinite lists(streams)?</h2>
<p>A list comprehension like <code>[1..]</code> is again a syntactic sugar. It’s expanded form is <code>enumFrom 1</code>. <code>enumFrom</code>’s type is <code>Enum a =&gt; a -&gt; [a]</code>. Let’s say we’re using <code>Enum Int</code> here. Since the instance is defined in <code>base</code>, and lists are also defined in <code>base</code>, this code will still work. However, if we compile <code>base</code> with <code>-XStrict</code>, this code loops because the standard list type will become strict.</p>
<p>In practice we would probably define strict and lazy lists separately to have the laziness when we need.</p>
<h2 id="what-about-higher-order-functions">What about higher-order functions?</h2>
<p>Since we don’t distinguish strict and lazy functions in type level, when we have a higher-order functions it may seem like we’d loose the guarantees. But this is not the case, at least not in general. Suppose we have this:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell">map<span class="ot"> ::</span> (a <span class="ot">-&gt;</span> b) <span class="ot">-&gt;</span> [a] <span class="ot">-&gt;</span> [b]
map _ []       <span class="fu">=</span> []
map f (x <span class="fu">:</span> xs) <span class="fu">=</span> f x <span class="fu">:</span> map f xs</code></pre>
<p>This is compiled with <code>-XStrict</code>. Now suppose we pass a function <code>f</code> which is lazy in it’s argument. Since <code>(:)</code> is strict in this code, we’ll still evaluate the <code>f x</code> before returning. Our guarantee that the list won’t be bottom and won’t have bottom still holds.</p>
<p>See also the paper <a href="http://research.microsoft.com/en-us/um/people/simonpj/papers/strict-core/tacc-hs09.pdf">“Types are calling conventions”</a>.</p>
<h2 id="haskells-denotational-semantics-says-that-lifted-types-have-bottoms">Haskell’s denotational semantics says that lifted types have bottoms</h2>
<p>This is true. Even if we have this strict type:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">data</span> <span class="dt">List</span> a <span class="fu">=</span> <span class="dt">Nil</span> <span class="fu">|</span> <span class="dt">Cons</span> <span class="fu">!</span>a <span class="fu">!</span>(<span class="dt">List</span> a)</code></pre>
<p>We can construct bottom values of this type, using, for example, <code>undefined :: List Int</code> or <code>(let x = x in x) :: Blah</code>.</p>
<p>However, if you think about how this value will be evaluated you’ll realize that this is exactly like how it would be evaluated in a call-by-value language. For example, if we try to bind it to a value:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">let</span> a <span class="fu">=</span><span class="ot"> undefined ::</span> <span class="dt">List</span> <span class="dt">Int</span> <span class="kw">in</span> <span class="fu">&lt;</span>body<span class="fu">&gt;</span></code></pre>
<p>This code will fail before <code>&lt;body&gt;</code> is run. In <code>&lt;body&gt;</code> <code>a</code> can’t be bottom and can’t contain bottoms.</p>
<p>(See also <a href="https://mail.haskell.org/pipermail/ghc-devs/2015-September/009799.html">this ghc-devs thread</a> about adding user defined unlifted data types to GHC. With this we could eliminate all the bottoms in some user-defined types.)</p>
<h2 id="but-haskell-will-still-generate-thunks-in-the-rts-level">But Haskell will still generate thunks in the RTS level?</h2>
<p>This is exactly right. <code>-XStrict</code> is really a very simple compiler pass that adds strictness annotations to every binder and field. We don’t have any changes in the GHC RTS to take advantage of additional strictness.</p>
<p>In other words, operational semantics of the language and implementation of this operational semantics in GHC RTS is still the same. We just do a program transformation to generate a program that evaluates like it would in a call-by-value language.</p>
<p>This means that if we have this program:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">let</span> a <span class="fu">=</span><span class="ot"> undefined ::</span> <span class="dt">List</span> <span class="dt">Int</span> <span class="kw">in</span> <span class="fu">&lt;</span>body<span class="fu">&gt;</span></code></pre>
<p>A thunk is still constructed in the runtime, but it’s evaluated <em>before</em> the <code>&lt;body&gt;</code> is evaluated. So this code fails before <code>&lt;body&gt;</code> is evaluated, and if we evaluate <code>&lt;body&gt;</code> it means that <code>a</code> is not bottom and doesn’t have bottoms.</p>
<p>This is another potential improvement over the <code>-XStrict</code>. For more details and some optimizations see <a href="https://mail.haskell.org/pipermail/ghc-devs/2015-October/010175.html">this ghc-devs thread</a>.</p>]]></summary>
</entry>
<entry>
    <title>On data representation in GHC Haskell</title>
    <link href="http://osa1.net/posts/2015-11-13-data-repr-1.html" />
    <id>http://osa1.net/posts/2015-11-13-data-repr-1.html</id>
    <published>2015-11-13T00:00:00Z</published>
    <updated>2015-11-13T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>It’s been a while since last time I wrote a blog post. This is not because I don’t have anything to write, rather, I have too much to write, and I was afraid that if I start writing it’d take too long.</p>
<p>But now that I started writing stuff for different applications(fellowships, internships etc.) I thought maybe this is a good time to write some blog posts too.</p>
<hr />
<p>At ICFP this year<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> we initiated a discussion about data representation in GHC Haskell. <a href="http://www.cs.indiana.edu/~rrnewton/homepage.html">My advisor</a> gave <a href="https://youtu.be/TT4poCUSf3A?t=59s">lightning talk at HIW</a>(Haskell Implementors Workshop). It’s a 5-minute talk and I recommend everyone reading this blog post to watch it. In the presentation, he showed this plot: (click on it to maximize)</p>
<p><a href="/images/data_repr_1/plot_old.png"><img src="/images/data_repr_1/plot_old_small.png" /></a></p>
<p>I put that here as a reference, but I’ll actually use some more detailed plots and correct a mistake in that plot. You can generate all the plots I show here using my benchmark programs <a href="https://github.com/osa1/spec_bench">here</a>.</p>
<p>In this post I want to make a point that is similar to Ryan’s point in the lightning talk: In Haskell, we’re not doing good job in data layouts. Our data is lazy by default, and laziness implies indirections(pointers). Updating a lazy record field means first generating a thunk and pointing to that thunk from the record. When the thunk is evaluated, we get one more indirection: A new pointer pointing to the actual data<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a>. This means that we need two pointer dereferencing just to read a single <code>Int</code> from a record.</p>
<p>At this point garbage collector helps to eliminate one level of indirection, and updates the field to point to the data directly. But this waits until the next garbage collection.</p>
<p>GHC has some support for “unpacking” fields of ADTs and records. When a field is “unpacked”, it means that 1) the field is strict 2) the value is not allocated separately and pointed to by a pointer, it’s part of the data constructor/record<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a>.</p>
<p>To illustrate how important unpacking is, I implemented two benchmarks. This is the data types I use in the benchmarks:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">data</span> <span class="dt">GL</span> a <span class="fu">=</span> <span class="dt">GLNil</span> <span class="fu">|</span> <span class="dt">GLCons</span> a (<span class="dt">GL</span> a)

<span class="kw">data</span> <span class="dt">SGL</span> a <span class="fu">=</span> <span class="dt">SGLNil</span> <span class="fu">|</span> <span class="dt">SGLCons</span> <span class="fu">!</span>a (<span class="dt">SGL</span> a)

<span class="kw">data</span> <span class="dt">IntList</span> <span class="fu">=</span> <span class="dt">ILNil</span> <span class="fu">|</span> <span class="dt">ILCons</span> <span class="ot">{-# UNPACK #-}</span> <span class="fu">!</span><span class="dt">Int</span> <span class="dt">IntList</span></code></pre>
<p>The first type, <code>GL</code>, is the exactly the same as GHC’s standard list type. Second one is mostly the same, only difference is I have a strictness annotation(a <a href="https://downloads.haskell.org/~ghc/7.10.2/docs/html/users_guide/bang-patterns.html"><code>BangPattern</code></a>) before in the head(or <code>car</code>) part of the list. Third one is similar to second one, except I also unpack the <code>car</code>.</p>
<p>Note that if you look at the types, first two of these types are parametric on the list elements, while the third one is specialized to integers. This is essentially monomorphization, and some compilers can do that automatically in some cases(<a href="http://mlton.org/Monomorphise">1</a>, <a href="http://www.impredicative.com/ur/">2</a>, if you know other compilers that do this, please write a comment), but in the presence of higher-order functions(and probably some other features), it’s in general not possible to monomorphise everything<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a>. GHC has some limited support for this with the <a href="https://downloads.haskell.org/~ghc/7.10.2/docs/html/users_guide/pragmas.html#specialize-pragma"><code>SPECIALIZE</code> pragma</a>, but it only works on functions and it doesn’t specialize data types(and maybe it can’t do this even in theory).</p>
<p>Now, I’m going to implement two functions on these data types. First function is for summing up all the elements:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">glSum ::</span> <span class="dt">GL</span> <span class="dt">Int</span> <span class="ot">-&gt;</span> <span class="dt">Int</span>
glSum <span class="fu">=</span> glSumAcc <span class="dv">0</span>

<span class="ot">glSumAcc ::</span> <span class="dt">Int</span> <span class="ot">-&gt;</span> <span class="dt">GL</span> <span class="dt">Int</span> <span class="ot">-&gt;</span> <span class="dt">Int</span>
glSumAcc <span class="fu">!</span>acc <span class="dt">GLNil</span>        <span class="fu">=</span> acc
glSumAcc <span class="fu">!</span>acc (<span class="dt">GLCons</span> h t) <span class="fu">=</span> glSumAcc (acc <span class="fu">+</span> h) t</code></pre>
<p>I implement exactly the same function for other types too.</p>
<p>Second function is the length function:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="ot">glLength ::</span> <span class="dt">GL</span> a <span class="ot">-&gt;</span> <span class="dt">Int</span>
glLength <span class="fu">=</span> glLengthAcc <span class="dv">0</span>

<span class="ot">glLengthAcc ::</span> <span class="dt">Int</span> <span class="ot">-&gt;</span> <span class="dt">GL</span> a <span class="ot">-&gt;</span> <span class="dt">Int</span>
glLengthAcc <span class="fu">!</span>acc <span class="dt">GLNil</span>        <span class="fu">=</span> acc
glLengthAcc <span class="fu">!</span>acc (<span class="dt">GLCons</span> _ l) <span class="fu">=</span> glLengthAcc (acc <span class="fu">+</span> <span class="dv">1</span>) l</code></pre>
<p>(Similarly for other two types..)</p>
<p>Now I’m going to benchmark these two function for all three variants, but let’s just speculate about what would be the expected results.</p>
<p>Clearly the third one should be faster for <code>sum</code>, because we don’t need to follow pointers for reading the integer. But should the second type(parametric but strict) be any faster? I’d say yes. The reason is because the field is strict, and so when we do pattern matching on <code>Int</code> to add integers, we don’t need to enter any thunks, we know that <code>Int</code> is already in WHNF. We should be able to just read the field.</p>
<p>Here’s the result: (click on it to maximize)</p>
<p><a href="/images/data_repr_1/plot_sum.png"><img src="/images/data_repr_1/plot_sum_small.png" /></a></p>
<p>As you can see I have an extra line in this plot: I added GHC’s standard lists and used <a href="http://hackage.haskell.org/package/base-4.8.1.0/docs/Data-List.html#v:sum">standard <code>sum</code> function</a>. It’s ridiculously slow, it’s almost two orders of magnitude slower on a list with length 10^7. Before moving on and interpreting rest of the lines let’s just talk a bit about why this is slower. We only need to look at the type of <code>sum</code> function:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell">sum<span class="ot"> ::</span> (<span class="dt">Foldable</span> t, <span class="dt">Num</span> a) <span class="ot">=&gt;</span> t a <span class="ot">-&gt;</span> a</code></pre>
<p>This function is very, very general. There’s no way for this function to run fast, unless GHC is smart enough to generate a specialized version of this for <code>[Int]</code>. It turns out GHC is not smart enough, and in our case this is causing trouble. Note that we need two specializations here. First is to eliminate <code>Foldable t</code> part(we statically know that <code>t</code> is <code>[]</code>), and second is to eliminate <code>Num a</code> part(we statically know that <code>a</code> is <code>Int</code>).</p>
<p>Now that we have this out of the way, let’s look at the other 3 lines. We can see that unboxed version is really faster(0.039 seconds other lists vs. 0.018 seconds unboxed list), which was expected. The interesting part is strict version is exactly the same as lazy version. Now, I don’t have a good explanation for this. The generated Core and STG for these two variants of <code>sum</code> are exactly the same. The Cmm or assembly code should be different. The problem is, I really can’t make any sense of generated Cmm code. I should study Cmm more for this.</p>
<p>But I have an idea: Pattern matching means entering the thunk to reveal the WHNF structure. Since our integers are boxed, we need to pattern match on them to read the primitive <code>Int#</code>s. This means entering the thunks, even if the field is strict.</p>
<p>In my benchmarks, I only used completely normalized lists. This means that in the lazy case we enter thunks, only to return immediately, because the <code>Int</code> is already in WHNF. There’s no difference in lazy and strict variants in pattern matching.</p>
<p>Only difference is when we update the field, in which case generated code should be different if the field is strict.</p>
<p>To validate the second part of this claim, I wrote another benchmark. In this benchmark I again create a list of length 10^7, but every cell has an integer calculated using a function. I then measure list generation and consumption(sum) times. The idea is that in the case of strict list, list generation should be slower, but consumption should be faster, and the opposite in the lazy list case. Indeed we can observe this in the output:</p>
<pre><code>Performing major GC
Generating generic list
Took 0.737606563 seconds.
Performing major GC
Summing generic list
Took 0.490998969 seconds.
Performing major GC
Generating strict list
Took 0.870686580 seconds.
Performing major GC
Summing strict list
Took 0.035886157 seconds.</code></pre>
<p>The program is <a href="https://github.com/osa1/spec_bench/commit/b63322eb1edd32792837b58853c00ba0effad0a6">here</a>. We can see that summing strict list is 10x faster, but producing is slower, because instead of generating thunks we’re actually doing the work while producing cons cells.</p>
<p>(One thing to note here is that if the computation in thunks is not expensive enough, strict lists are faster in both production and consumption. I think the reason is because thunking overhead is bigger than actually doing the work in some cases)</p>
<p>OK, I hope this explains the story with <code>sum</code>. The second part of the benchmark is even more interesting. It runs <code>length</code>. Here’s the plot:</p>
<p><a href="/images/data_repr_1/plot_len.png"><img src="/images/data_repr_1/plot_len_small.png" /></a></p>
<p>We again have a line for standard Haskell list here. Good news is that even though standard <a href="http://hackage.haskell.org/package/base-4.8.1.0/docs/Data-List.html#v:length">length</a>’s type is again very general, this time GHC was able to optimize:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell">length<span class="ot"> ::</span> <span class="dt">Foldable</span> t <span class="ot">=&gt;</span> t a <span class="ot">-&gt;</span> <span class="dt">Int</span></code></pre>
<p>Interesting part is that unboxed list is again faster! But why? We’re not using head fields, whether it’s a pointer or not should not matter, right?</p>
<p>Here I again have an idea: Even though we never use head parts, the garbage collector has to traverse all the data, and copy them from one generation to other.</p>
<p>(We also allocate more, but I’m not measuring that in the benchmarks)</p>
<p>To again back my claims, I have another set of benchmark programs that generate some GC stats. Here’s the output for “generic” case:</p>
<pre><code>   800,051,568 bytes allocated in the heap
 1,138,797,344 bytes copied during GC
   310,234,360 bytes maximum residency (12 sample(s))
    68,472,584 bytes maximum slop
           705 MB total memory in use (0 MB lost due to fragmentation)

                                   Tot time (elapsed)  Avg pause  Max pause
Gen  0      1520 colls,     0 par    0.207s   0.215s     0.0001s    0.0009s
Gen  1        12 colls,     0 par    0.348s   0.528s     0.0440s    0.1931s

INIT    time    0.000s  (  0.000s elapsed)
MUT     time    0.126s  (  0.082s elapsed)
GC      time    0.555s  (  0.743s elapsed)
EXIT    time    0.004s  (  0.050s elapsed)
Total   time    0.685s  (  0.875s elapsed)

%GC     time      81.0%  (84.9% elapsed)

Alloc rate    6,349,615,619 bytes per MUT second

Productivity  19.0% of total user, 14.9% of total elapsed</code></pre>
<p>“Generic strict” case:</p>
<pre><code>   800,051,568 bytes allocated in the heap
 1,138,797,344 bytes copied during GC
   310,234,360 bytes maximum residency (12 sample(s))
    68,472,584 bytes maximum slop
           705 MB total memory in use (0 MB lost due to fragmentation)

                                   Tot time (elapsed)  Avg pause  Max pause
Gen  0      1520 colls,     0 par    0.211s   0.213s     0.0001s    0.0008s
Gen  1        12 colls,     0 par    0.367s   0.531s     0.0442s    0.1990s

INIT    time    0.000s  (  0.000s elapsed)
MUT     time    0.114s  (  0.080s elapsed)
GC      time    0.578s  (  0.744s elapsed)
EXIT    time    0.003s  (  0.045s elapsed)
Total   time    0.698s  (  0.869s elapsed)

%GC     time      82.8%  (85.6% elapsed)

Alloc rate    7,017,996,210 bytes per MUT second

Productivity  17.2% of total user, 13.8% of total elapsed</code></pre>
<p>“Unboxed” case:</p>
<pre><code>   560,051,552 bytes allocated in the heap
   486,232,928 bytes copied during GC
   123,589,752 bytes maximum residency (9 sample(s))
     3,815,304 bytes maximum slop
           244 MB total memory in use (0 MB lost due to fragmentation)

                                   Tot time (elapsed)  Avg pause  Max pause
Gen  0      1062 colls,     0 par    0.117s   0.123s     0.0001s    0.0018s
Gen  1         9 colls,     0 par    0.116s   0.179s     0.0199s    0.0771s

INIT    time    0.000s  (  0.001s elapsed)
MUT     time    0.070s  (  0.054s elapsed)
GC      time    0.233s  (  0.302s elapsed)
EXIT    time    0.002s  (  0.019s elapsed)
Total   time    0.306s  (  0.376s elapsed)

%GC     time      76.1%  (80.4% elapsed)

Alloc rate    8,000,736,457 bytes per MUT second

Productivity  23.9% of total user, 19.4% of total elapsed</code></pre>
<p>Interesting parts are productivity rates and total bytes allocated. We can see that unboxed version is a lot better in both.</p>
<p>The reason why productivities are too bad in all cases is, I think, that because this is purely an allocation benchmark, all we do is to allocate and then we do one pass on the whole thing. In this type of programs it makes sense to increase the initial heap a little bit to increase the productivity. For example, if I use <code>-H1G</code> productivity increases to 64% in generic list case and to 97% in unboxed list case.</p>
<p>So what’s the lesson here?</p>
<p>The data layout matters a lot. Even if the data is not used in some hot code path, GC needs to traverse all the live data and in the case of GHC Haskell it needs to copy them in each GC cycle. Also, lazy-by-default is bad for performance.</p>
<h1 id="an-improvement">An improvement</h1>
<p>I recently finished implementing hard parts of a project. I don’t want to give too much detail here for now but let’s just say we’re improving the unboxing story in GHC. With our new patch, you will be able to <code>{-# UNPACK #-}</code> some of the types that you can’t right now. When/if the patch lands I’m going to announce it here.</p>
<p>With <a href="http://downloads.haskell.org/~ghc/master/users-guide/glasgow_exts.html?highlight=strictdata#strict-haskell"><code>-XStrictData</code></a>, <a href="https://phabricator.haskell.org/D1142"><code>-XStrict</code></a> and flags like <code>-funbox-strict-fields</code>(search for it <a href="https://downloads.haskell.org/~ghc/7.10.2/docs/html/users_guide/flag-reference.html">here</a>) we’ll have a lot better story about data layout than we have today. But there are still some missing pieces. Our patch will hopefully implement one more missing piece and we’ll have even better data layout story.</p>
<p>I think the next step then will be <a href="https://mail.haskell.org/pipermail/ghc-devs/2015-October/010175.html">better calling conventions for strict functions</a> and some other tweaks in the runtime system for better strict code support overall. Then maybe we can officially declare Haskell as a language with lazy and strict evaluation ;-) .</p>
<hr />
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>I realized that I forgot to announce it here, but <a href="/papers/cnf.pdf">we had a paper at ICFP</a>, and I gave a <a href="https://www.youtube.com/watch?v=gkx-D-7Y1EU">talk at Haskell Implementors Workshop</a> this year.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>That data is still not totally normalized, it’s in weak head normal form. The new heap object that we get when we evaluate a thunk is called an “indirection”. See more details <a href="https://ghc.haskell.org/trac/ghc/wiki/Commentary/Rts/Storage/HeapObjects#Indirections">here</a>.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>See the <a href="https://downloads.haskell.org/~ghc/7.10.2/docs/html/users_guide/pragmas.html#unpack-pragma">GHC user manual entry</a> for more info.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>I’m hoping to make this a subject to another blog post. One thing to note here is that even when monomorphization is possible, it may cause a code explosion.<a href="#fnref4">↩</a></p></li>
</ol>
</div>]]></summary>
</entry>
<entry>
    <title>A learning system that is as fast as a static one</title>
    <link href="http://osa1.net/posts/2015-08-16-a-learning-system-that-is-fast.html" />
    <id>http://osa1.net/posts/2015-08-16-a-learning-system-that-is-fast.html</id>
    <published>2015-08-16T00:00:00Z</published>
    <updated>2015-08-16T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>I was reading the 1986 paper <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.128.6414">The Concept of a Supercompiler</a> one more time, and I found this amazing part that I either missed or didn’t find interesting when I read it for the first time:</p>
<p>(emphasis mine)</p>
<blockquote>
<p>Supercompilation can also be performed throughout all levels of the system. Fix k = K and supercompile E(K, q) with a variable q. You have an expert system that cannot learn, but answers questions very quickly. You can restore the ability of the system to learn by endowing it with two memories: short-term and long-term. To answer a question, both memories must be scanned. The long-term memory has those procedures that are obtained by using a supercompiler with the knowledge present at the moment of the last supercompilation session. The short-term memory includes universal interpretive procedures operating on the incremental knowledge received after the last supercompilation session. Such a system can be both fast and able to learn. <em>When it has no questions to answer(the periods of “sleep”), it will execute supercompilation procedures, converting its short-range memory into the long-range one. This would be a step towards a computer individual.</em></p>
</blockquote>
<p>This is interesting for two reasons. First, we discussed similar ideas with my advisor couple of weeks ago, but in a different context. I’m hoping to write more about this in future posts. Second, I think the idea is easily implementable with a multi-stage language. I think this is one of the ideas that are very good yet even after years still waiting to be implemented and explored further.</p>]]></summary>
</entry>
<entry>
    <title>The issue with work sharing (common subexpression elimination)</title>
    <link href="http://osa1.net/posts/2015-08-13-the-issue-with-work-sharing.html" />
    <id>http://osa1.net/posts/2015-08-13-the-issue-with-work-sharing.html</id>
    <published>2015-08-13T00:00:00Z</published>
    <updated>2015-08-13T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>I’d expect more work sharing to be always more beneficial. But apparently this is not the case, as pointed out in (Chitil, 1997)<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>.</p>
<p>Here’s an example from the paper: (slightly changed)</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell">sum [<span class="dv">1</span> <span class="fu">..</span> <span class="dv">1000</span>] <span class="fu">+</span> sum [<span class="fu">-</span><span class="dv">1000</span> <span class="fu">..</span> <span class="fu">-</span><span class="dv">1</span>] <span class="fu">+</span> prod [<span class="dv">1</span> <span class="fu">..</span> <span class="dv">1000</span>]</code></pre>
<p>We can evaluate this expression to WHNF using heap space enough for a single list(to be more specific, we only need a single cons cell at any time). After evaluating a subexpression, we can deallocate and allocate for the next list etc.</p>
<p>However, if we eliminate common subexpressions, and generate this code:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">let</span> v <span class="fu">=</span> [<span class="dv">1</span> <span class="fu">..</span> <span class="dv">1000</span>]
 <span class="kw">in</span> sum v <span class="fu">+</span> sum [<span class="fu">-</span><span class="dv">1000</span> <span class="fu">..</span> <span class="fu">-</span><span class="dv">1</span>] <span class="fu">+</span> prod v</code></pre>
<p>Now <code>v</code> has to live until the let body is evaluated to a value. We win in allocation/deallocation side, but we lose in residency side. In paper’s words: “Whereas the transformation always decreases total heap usage, it may considerably influence heap residency.”</p>
<p>In general, we can’t do this transformation, without risking increased residency:</p>
<p>\[ e’[e,e] \leadsto \texttt{let}\; x = e\; \texttt{in}\; e’[x,x] \]</p>
<p>As a solution, the paper suggests this:</p>
<ol style="list-style-type: decimal">
<li>We always do CSE if the subexpressions’ WHNF == NF(i.e. if it’s a “safe type” in paper’s terms). According to the paper, “a partially evaluated expression is certain to require only a small, fixed amount of space if it’s not a function, whose environment may refer to arbitrary large data structures, and its WHNF is already its normal form”.</li>
<li>We always do CSE when a named expression is syntactically dominating another equal expression:</li>
</ol>
<p>\[ \texttt{let}\; x = e\; \texttt{in}\; e’[e] \leadsto \texttt{let}\; x = e\; \texttt{in}\; e’[x] \]</p>
<hr />
<p>Note that (1) is not always true, assume an expression with type <code>ForeignPtr a</code> where <code>a</code> is a huge FFI object. This has WHNF == NF property, but it may increase residency significantly. Maybe GHC didn’t have FFI at the time the paper is written.</p>
<p>Also, I’m wondering how is CSE is handled in current GHC.</p>
<hr />
<p>In supercompilation, we want to avoid evaluating same expressions in a loop forever, so we keep some kind of “history”, and when we come across a term that we evaluated before, we fold the process tree and avoid evaluating same term again.</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell">(fib <span class="dv">1000</span>, fib <span class="dv">1000</span>)</code></pre>
<p>Unless we make sure to split it in a way that branches of the process tree are unaware of each other, we may end up eliminating common subexpressions. However, since there are lots of cases where we may want CSE, a splitter that always prevents it is not always desirable. We should instead allow CSE in a controlled way.</p>
<hr />
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Olaf Chitil, “Common Subexpression Elimination in a Lazy Functional Language”, section 3.5.<a href="#fnref1">↩</a></p></li>
</ol>
</div>]]></summary>
</entry>
<entry>
    <title>The issue of splitting without work duplication</title>
    <link href="http://osa1.net/posts/2015-08-13-the-issue-of-splitting-wo-duplication.html" />
    <id>http://osa1.net/posts/2015-08-13-the-issue-of-splitting-wo-duplication.html</id>
    <published>2015-08-13T00:00:00Z</published>
    <updated>2015-08-13T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>(I’m starting publishing my long list of unpublished blog posts with this post)</p>
<p>(Examples are from <a href="http://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-835.html">Bolingbroke’s PhD thesis</a>)</p>
<p><em>Example 1:</em></p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">let</span> a  <span class="fu">=</span> id y
    id <span class="fu">=</span> \x <span class="ot">-&gt;</span> x
 <span class="kw">in</span> <span class="dt">Just</span> a</code></pre>
<p><em>Problem:</em> The compiler should know about <code>id</code> while compiling <code>a</code>. This is easy to do, just tell the compiler about every binding when compiling RHSs. However, it causes some other problems:</p>
<p><em>Example 2:</em></p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">let</span> n <span class="fu">=</span> fib <span class="dv">100</span>
    b <span class="fu">=</span> n <span class="fu">+</span> <span class="dv">1</span>
    c <span class="fu">=</span> n <span class="fu">+</span> <span class="dv">2</span>
 <span class="kw">in</span> (b, c)</code></pre>
<p><em>Problem:</em> If we tell about <code>n</code> to the compiler when it’s compiling <code>b</code> and <code>c</code>, we’re taking the risk of work duplication. It may seem like <code>fib 100</code> will be evaluated in compile time and so duplication is not a huge deal, but this is not necessarily the case. First, we can’t know if it’s going to be evaluated to a value in compile time. Second, even if it’s a closed term and we somehow know it’s going to be terminated, termination checker of the evaluator may want to stop it before it’s evaluated to a value. Third, most of the time it’ll be an open term that’ll get stuck in the middle of supercompilation.</p>
<p>And when that happens we will generate a let-binding in residual code. In our case, we’ll be generating two let-bindings, one is for <code>b</code> and one is for <code>c</code>, and those let bindings will be doing same work.</p>
<hr />
<p><em>Question:</em> Can we rely on a post-processsing pass to eliminate common subexpressions? I.e. if we generate a code like this:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">let</span> b <span class="fu">=</span> <span class="kw">let</span> n_supercompiled <span class="fu">=</span> <span class="fu">&lt;</span>supercompiled fib <span class="dv">100</span><span class="fu">&gt;</span>
         <span class="kw">in</span> n_supercompiled <span class="fu">+</span> <span class="dv">1</span>
    c <span class="fu">=</span> <span class="kw">let</span> n_supercompiled <span class="fu">=</span> <span class="fu">&lt;</span>supercompiled fib <span class="dv">100</span><span class="fu">&gt;</span>
         <span class="kw">in</span> n_supercompiled <span class="fu">+</span> <span class="dv">2</span>
 <span class="kw">in</span> (b, c)</code></pre>
<p>It would transform it to obvious residual code that has single <code>n_supercompiled</code> which is in scope of <code>b</code> and <code>c</code>.</p>
<p>What are trade-offs?</p>
<hr />
<p>Finding a good heuristic is hard. Let’s say we try to estimate costs of expressions and decide whether to tell the compiler about them or not. If we decide that <code>ys</code> and <code>xs</code> are expensive in this case:</p>
<pre class="sourceCode haskell"><code class="sourceCode haskell"><span class="kw">let</span> map <span class="fu">=</span> <span class="fu">...</span>
    ys <span class="fu">=</span> map f zs
    xs <span class="fu">=</span> map g ys
 <span class="kw">in</span> <span class="dt">Just</span> xs</code></pre>
<p>We miss a deforestation opportunity, because the compiler won’t know about <code>ys</code> while compiling <code>xs</code>.</p>]]></summary>
</entry>

</feed>
